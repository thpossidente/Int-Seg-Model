
n.inputs <- 16
n.hidden <- 100
n.outputs <- 26
learning.rate <- 0.2

alphabet.data <- read.table("C:/Users/research/Documents/GitHub/Int-Seg-Model/Int-Seg-Model/letter-recognition-data.txt", header = F)

weights <- matrix(runif(n.inputs*n.outputs, min=0, max=1), nrow=n.inputs, ncol=n.outputs) #initialiize weights at random values between 1 and 0

sigmoid.activation <- function(x){
  return(1 / (1+exp(-x)))
}


forward.pass <- function(input){ #calculate output activations with "winner-takes-all" method
  outputs <- numeric(n.outputs)
  for(i in 1:n.outputs){
    outputs[i] <- sigmoid.activation(sum(input * weights[,i]))
  }
  outputs[which.max(outputs)] <- 1
  outputs[outputs != max(outputs)] <- 0
  return(outputs)
}


heb.update <- function(input){
  outputs <- forward.pass(input)
  for(i in 1:n.inputs){
    for(j in 1:n.outputs){
      change.weight <- learning.rate * outputs[j] * input[i]
      weights[i,j] <<- weights[i,j] + change.weight
    }
  }
}
  

batch <- function(){ 
  pb <- txtProgressBar(min=0, max=n.epochs, style=3)
  for(i in 1:nrow(alphabet.data)){
    vector <- alphabet.data[i,]  
    heb.update(vector)
  }
    setTxtProgressBar(pb, i)
}

batch()  #run training batches


  