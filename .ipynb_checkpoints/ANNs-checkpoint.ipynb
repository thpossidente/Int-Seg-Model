{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing and Running ANNs for RF Mutual Information Investigation\n",
    "## Thomas Possidente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Value Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ANN Building Imports\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Value Inits - Specify as needed\n",
    "num_inputs = int(1000)  # number of dummy images in set\n",
    "size = int(16)          # Dimension of each dummy image should be size*size\n",
    "RF_size = int(4)        # Dimensions of the RF to be analyzed should be RF_size*RF_size\n",
    "\n",
    "# Value Inits - Leave these alone\n",
    "num_of_RFs = int((size*size) / (RF_size*RF_size))\n",
    "inputs_by_RF = np.empty([16000, 16])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.read_csv('test.csv')\n",
    "inputs = inputs.drop('X1', axis = 0) # Taking out col names\n",
    "inputs = inputs.apply(pd.to_numeric)  # converting to floats\n",
    "\n",
    "inputs = inputs.values # convert to np ndarray\n",
    "inputs = inputs.reshape(1000,16,16) # reshape to desired dims (1000 examples, 16*16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through num_inputs and extract each RF input, flatten, and store in ndarray\n",
    "\n",
    "origin_x = 0\n",
    "origin_y = 0\n",
    "count = 0\n",
    "\n",
    "for n in range(0, num_of_RFs): # For each RF field in dummy image\n",
    "    for i in range(0, num_inputs): # For each dummy image in set\n",
    "        single_RF_input = inputs[i, origin_y:(origin_y + RF_size), origin_x:(origin_x + RF_size)] # Extract 1 RF \n",
    "        single_RF_input_flat = single_RF_input.reshape((RF_size*RF_size)) \n",
    "        inputs_by_RF[count,] = single_RF_input_flat \n",
    "        count += 1\n",
    "    if(origin_x == (size - RF_size)):  # Changes RF field over image\n",
    "        origin_x = 0\n",
    "        origin_y += RF_size\n",
    "    else:\n",
    "        origin_x += RF_size\n",
    "        \n",
    "inputs_by_RF = inputs_by_RF.reshape(num_of_RFs, num_inputs, (RF_size*RF_size)) \n",
    "# ^Reshaping to (RF Location, Image number, flattened RF) - this makes it easier to select inputs for each separate ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "class Hebbian(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, output_dim, lmbda=1.0, eta=0.0005, connectivity='random', connectivity_prob=0.25, **kwargs):\n",
    "        '''\n",
    "        Constructor for the Hebbian learning layer.\n",
    "\n",
    "        args:\n",
    "            output_dim - The shape of the output / activations computed by the layer.\n",
    "            lambda - A floating-point valued parameter governing the strength of the Hebbian learning activation.\n",
    "            eta - A floating-point valued parameter governing the Hebbian learning rate.\n",
    "            connectivity - A string which determines the way in which the neurons in this layer are connected to\n",
    "                the neurons in the previous layer.\n",
    "        '''\n",
    "        self.output_dim = output_dim\n",
    "        self.lmbda = lmbda\n",
    "        self.eta = eta\n",
    "        self.connectivity = connectivity\n",
    "        self.connectivity_prob = connectivity_prob\n",
    "\n",
    "        super(Hebbian, self).__init__(**kwargs)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def random_conn_init(self, shape, dtype=None):\n",
    "        A = np.random.normal(0, 1, shape)\n",
    "        A[self.B] = 0\n",
    "        return tf.constant(A, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def zero_init(self, shape, dtype=None):\n",
    "        return np.zeros(shape)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # create weight variable for this layer according to user-specified initialization\n",
    "        if self.connectivity == 'random':\n",
    "            self.B = np.random.random(input_shape[0]) < self.connectivity_prob\n",
    "        elif self.connectivity == 'zero':\n",
    "            self.B = np.zeros(self.output_dim)\n",
    "            \n",
    "        if self.connectivity == 'all':\n",
    "            self.kernel = self.add_weight(name='kernel', shape=(np.prod(input_shape[1:]), \\\n",
    "                        np.prod(self.output_dim)), initializer='uniform', trainable=False)\n",
    "        elif self.connectivity == 'random':\n",
    "            self.kernel = self.add_weight(name='kernel', shape=(np.prod(input_shape[1:]), \\\n",
    "                        np.prod(self.output_dim)), initializer=self.random_conn_init, trainable=False)\n",
    "        elif self.connectivity == 'zero':\n",
    "            self.kernel = self.add_weight(name='kernel', shape=(np.prod(input_shape[1:]), \\\n",
    "                        np.prod(self.output_dim)), initializer=self.zero_init, trainable=False)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "        # call superclass \"build\" function\n",
    "        super(Hebbian, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        x_shape = tf.shape(x)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # reshape to (batch_size, product of other dimensions) shape\n",
    "        x = tf.reshape(x, (tf.reduce_prod(x_shape[1:]), batch_size))\n",
    "\n",
    "        # compute activations using Hebbian-like update rule\n",
    "        activations = x + self.lmbda * tf.matmul(self.kernel, x)  # why \"x +\", should this be removed?\n",
    "\n",
    "        # compute outer product of activations matrix with itself\n",
    "        outer_product = tf.matmul(tf.expand_dims(x, 1), tf.expand_dims(x, 0))  # Why outer product? Should this line be removed?\n",
    "\n",
    "        # update the weight matrix of this layer\n",
    "        self.kernel = self.kernel + tf.multiply(self.eta, tf.reduce_mean(outer_product, axis=2)) # Still why outer prod?\n",
    "        self.kernel = tf.multiply(self.kernel, self.B) # zeroing node connections that are meant to be zeros\n",
    "        return K.reshape(activations, x_shape)\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Hebbian(input_shape = (RF_size,1), output_dim = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom\\Anaconda3\\lib\\site-packages\\keras\\models.py:863: UserWarning: Output \"hebbian_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"hebbian_1\" during training.\n",
      "  **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model cannot be compiled because it has no loss to optimize.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b9e99cb37ff1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdummyOpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdummy_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdummyOpt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdummyLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                            \u001b[0mweighted_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweighted_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                            \u001b[0mtarget_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                            **kwargs)\n\u001b[0m\u001b[0;32m    864\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    838\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m                     raise ValueError('The model cannot be compiled '\n\u001b[0m\u001b[0;32m    841\u001b[0m                                      'because it has no loss to optimize.')\n\u001b[0;32m    842\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The model cannot be compiled because it has no loss to optimize."
     ]
    }
   ],
   "source": [
    "                                                # TODO how to compile model without a real optimizer/loss function?\n",
    "                                                # then how to get values out and make sure weights are changing (probably\n",
    "                                                # just model.predict)\n",
    "            \n",
    "            # dummy_opt seems to be working (maybe), need to figure out how to dummy loss correctly. Maybe need to\n",
    "            # pass fake y_true values?\n",
    "            \n",
    "def dummy_loss(): pass  \n",
    "#def dummy_loss(y_true, y_pred): return y_pred             \n",
    "\n",
    "class dummy_opt(keras.optimizers.Optimizer): pass\n",
    "    \n",
    "dummyLoss = dummy_loss()\n",
    "dummyOpt = dummy_opt()\n",
    "\n",
    "model.compile(optimizer= dummyOpt,loss = dummyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The model needs to be compiled before being used.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6637406750f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_by_RF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m             raise RuntimeError('The model needs to be compiled '\n\u001b[0m\u001b[0;32m    989\u001b[0m                                'before being used.')\n\u001b[0;32m    990\u001b[0m         return self.model.fit(x, y,\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The model needs to be compiled before being used."
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(inputs_by_RF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
