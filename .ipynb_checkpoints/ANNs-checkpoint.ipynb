{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing and Running ANNs for RF Mutual Information Investigation\n",
    "## Thomas Possidente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Value Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ANN Building Imports\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "\n",
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Value Inits - Specify as needed\n",
    "num_inputs_per_batch = int(1000)  # number of dummy images in set\n",
    "size = int(16)          # Dimension of each dummy image should be size*size\n",
    "RF_size = int(4)        # Dimensions of the RF to be analyzed should be RF_size*RF_size\n",
    "\n",
    "# Value Inits - Leave these alone\n",
    "num_of_RFs = int((size*size) / (RF_size*RF_size))\n",
    "inputs_by_RF = np.empty([(num_inputs_per_batch * size), size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.read_csv('test.csv')\n",
    "inputs = inputs.drop('X1', axis = 0) # Taking out col names\n",
    "inputs = inputs.apply(pd.to_numeric)  # converting to floats\n",
    "\n",
    "inputs = inputs.values # convert to np ndarray\n",
    "inputs = inputs.reshape(1000,16,16) # reshape to desired dims (1000 examples, 16*16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through num_inputs and extract each RF input, flatten, and store in ndarray\n",
    "\n",
    "origin_x = 0\n",
    "origin_y = 0\n",
    "count = 0\n",
    "\n",
    "for n in range(0, num_of_RFs): # For each RF field in dummy image\n",
    "    for i in range(0, num_inputs_per_batch): # For each dummy image in set\n",
    "        single_RF_input = inputs[i, origin_y:(origin_y + RF_size), origin_x:(origin_x + RF_size)] # Extract 1 RF \n",
    "        single_RF_input_flat = single_RF_input.reshape((RF_size*RF_size)) \n",
    "        inputs_by_RF[count,] = single_RF_input_flat \n",
    "        count += 1\n",
    "    if(origin_x == (size - RF_size)):  # Changes RF field over image\n",
    "        origin_x = 0\n",
    "        origin_y += RF_size\n",
    "    else:\n",
    "        origin_x += RF_size\n",
    "        \n",
    "inputs_by_RF = inputs_by_RF.reshape(num_of_RFs, num_inputs_per_batch, (RF_size*RF_size)) \n",
    "# ^Reshaping to (RF Location, Image number, flattened RF) - this makes it easier to select inputs for each separate ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "class Hebbian(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, output_dim, lmbda=1.0, eta=0.005, connectivity='random', connectivity_prob=0.25, **kwargs):\n",
    "        '''\n",
    "        Constructor for the Hebbian learning layer.\n",
    "\n",
    "        args:\n",
    "            output_dim - The shape of the output / activations computed by the layer.\n",
    "            lambda - A floating-point valued parameter governing the strength of the Hebbian learning activation.\n",
    "            eta - A floating-point valued parameter governing the Hebbian learning rate.\n",
    "            connectivity - A string which determines the way in which the neurons in this layer are connected to\n",
    "                the neurons in the previous layer.\n",
    "        '''\n",
    "        self.output_dim = output_dim\n",
    "        self.lmbda = lmbda\n",
    "        self.eta = eta\n",
    "        self.connectivity = connectivity\n",
    "        self.connectivity_prob = connectivity_prob\n",
    "\n",
    "        super(Hebbian, self).__init__(**kwargs)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def random_conn_init(self, shape, dtype=None):\n",
    "        A = np.random.normal(0, 1, shape)\n",
    "        A[self.B] = 0\n",
    "        return tf.constant(A, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def zero_init(self, shape, dtype=None):\n",
    "        return np.zeros(shape)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # create weight variable for this layer according to user-specified initialization\n",
    "        if self.connectivity == 'random':\n",
    "            self.B = np.random.random(input_shape[0]) < self.connectivity_prob\n",
    "        elif self.connectivity == 'zero':\n",
    "            self.B = np.zeros(self.output_dim)\n",
    "            \n",
    "        if self.connectivity == 'all':\n",
    "            self.kernel = self.add_weight(name='kernel', shape=(np.prod(input_shape[1:]), \\\n",
    "                        np.prod(self.output_dim)), initializer='uniform', trainable=False)\n",
    "        elif self.connectivity == 'random':\n",
    "            self.kernel = self.add_weight(name='kernel', shape=(np.prod(input_shape[1:]), \\\n",
    "                        np.prod(self.output_dim)), initializer=self.random_conn_init, trainable=False)\n",
    "        elif self.connectivity == 'zero':\n",
    "            self.kernel = self.add_weight(name='kernel', shape=(np.prod(input_shape[1:]), \\\n",
    "                        np.prod(self.output_dim)), initializer=self.zero_init, trainable=False)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "        # call superclass \"build\" function\n",
    "        super(Hebbian, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, x):  # x is the input to the network\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x_shape = tf.shape(x)\n",
    "\n",
    "        # reshape to (batch_size, product of other dimensions) shape\n",
    "        x = tf.reshape(x, (batch_size, tf.reduce_prod(x_shape[1:])))\n",
    "\n",
    "        # compute activations using Hebbian-like update rule\n",
    "        activations = self.lmbda * tf.matmul(x, self.kernel) \n",
    "\n",
    "        # Should implement winner-takes-all on activations here\n",
    "        \n",
    "        #for n in range(0, num_inputs_per_batch):\n",
    "        #    pre_winner = activations[[n]] * 0\n",
    "        #    index_max = tf.argmax(pre_winner)\n",
    "        #    activations = activations[index_max].assign(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #activations = x + self.lmbda * tf.matmul(self.kernel, x)  # why \"x +\", should this be removed? \n",
    "                                                                  # Matrix Multiplication will give us the activations\n",
    "                                                                  # so why add the input to the activation? This shouldn't\n",
    "                                                                  # even work bc the dims of x are different than\n",
    "                                                                  # the dims of tf.matmul(kernal, x)\n",
    "\n",
    "        # compute outer product of activations matrix with itself\n",
    "            # outer_product = tf.matmul(tf.expand_dims(x, 1), tf.expand_dims(x, 0))  # No idea why you would compute outer prod \n",
    "                                                                               # of inputs w/ itself. This method of weight \n",
    "                                                                               # updating seems wrong for our purposes\n",
    "                    \n",
    "         # update the weight matrix of this layer\n",
    "            # self.kernel = self.kernel + tf.multiply(self.eta, tf.reduce_mean(outer_product, axis=2)) # Still why outer prod?\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                    \n",
    "        # Loop through each input and update weights via hebbian update\n",
    "        for i in range(0, num_inputs_per_batch):  \n",
    "            input_set_1 = x[[i]] # select one input\n",
    "            input_set = tf.tile(input_set_1, [tf.shape(self.kernel)[1]]) # repeat input\n",
    "            input_set = tf.reshape(input_set, [tf.shape(self.kernel)[1], tf.shape(self.kernel)[0]]) # reshape to transpose of kernal shape\n",
    "            input_set = tf.transpose(input_set) # transpose\n",
    "            self.kernel = self.kernel + (self.eta * (input_set - self.kernel) * activations[[i]]) # weight update based on Hebb's Rule\n",
    "       \n",
    "        #self.kernel = tf.multiply(self.kernel, self.B) # zeroing node connections that started at 0\n",
    "        return K.reshape(activations, (batch_size, self.output_dim))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrap Page ###\n",
    "\n",
    "\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "#    print(sess.run(weights))\n",
    "#    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hebbian_11 (Hebbian)         (None, 4)                 64        \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 0\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Hebbian(input_shape = (1, (RF_size*RF_size)), output_dim = 4, eta = 0.5))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loss(y_true, y_pred): return y_pred             \n",
    "\n",
    "class dummy_opt(keras.optimizers.Optimizer): \n",
    "    def __init__(self): return(None)\n",
    "    def get_updates(self, loss, params): return(np.array(0))\n",
    "    def get_configs(self): return(0)\n",
    "    \n",
    "dummyOpt = dummy_opt()\n",
    "\n",
    "model.compile(optimizer= dummyOpt,loss = dummy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 31s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n",
      "Epoch 2/10\n",
      " - 0s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n",
      "Epoch 3/10\n",
      " - 0s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n",
      "Epoch 4/10\n",
      " - 0s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n",
      "Epoch 5/10\n",
      " - 0s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n",
      "Epoch 6/10\n",
      " - 0s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n",
      "Epoch 7/10\n",
      " - 0s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n",
      "Epoch 8/10\n",
      " - 0s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n",
      "Epoch 10/10\n",
      " - 0s - loss: -1.7273e-01\n",
      "[array([[ 2.0921323 ,  0.70519495,  0.07430287,  1.1239237 ],\n",
      "       [-2.1278749 ,  0.4983978 ,  1.0668519 , -0.31257948],\n",
      "       [ 0.8718128 , -0.22247857,  1.8992623 ,  0.06970201],\n",
      "       [ 1.2230616 , -1.833812  , -0.13533255,  1.0189292 ],\n",
      "       [-0.6199136 ,  0.34011433,  0.01743944, -0.43034574],\n",
      "       [-1.0211784 , -1.0274101 ,  0.8562588 ,  0.7975055 ],\n",
      "       [ 1.5491563 , -1.0262009 , -0.8120423 ,  1.3308686 ],\n",
      "       [ 0.16862124, -0.62754256,  2.0752852 ,  1.5136431 ],\n",
      "       [-1.0832523 ,  0.4234916 ,  1.002946  , -0.54286295],\n",
      "       [-2.1271026 , -1.0364562 , -0.88286227, -0.33724073],\n",
      "       [ 0.47471374, -1.0832853 , -0.34611532,  0.28345266],\n",
      "       [ 0.45062295,  1.8392057 , -0.29410586,  0.99240947],\n",
      "       [ 0.77238977,  1.1744121 , -0.7375462 , -0.05235371],\n",
      "       [-1.7563099 ,  0.3272527 , -0.55923057, -0.15214175],\n",
      "       [ 0.7468831 , -0.81426483,  0.7107043 , -2.0750642 ],\n",
      "       [-2.2560031 , -0.9306008 , -0.25499016,  0.4314762 ]],\n",
      "      dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279744b8b00>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print_weights = LambdaCallback(on_epoch_end=lambda epoch, logs: print(model.layers[0].get_weights()))\n",
    "    \n",
    "\n",
    "history = model.fit(x = inputs_by_RF[0].reshape(1000, 1, 16), y = np.array([0]*1000), epochs = 10, verbose = 2, callbacks = [print_weights])\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes/TODO\n",
    "\n",
    "* Implement winner-take-all - how to change values in activation tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
