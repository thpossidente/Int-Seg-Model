{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised ANNs for Mutual Information Investigation\n",
    "## Thomas Possidente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Building and Visualization Imports\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers, losses\n",
    "from keras.engine.topology import Layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Value Inits - Specify as needed\n",
    "num_inputs = int(5120)  # number of dummy images in set\n",
    "size = int(16)          # Dimension of each dummy image should be size*size\n",
    "RF_size = int(2)        # Dimensions of the RF to be analyzed should be RF_size*RF_size\n",
    "noise = 0               # percentage (as decimal) of input values that will be flipped \n",
    "\n",
    "# Value Inits - Leave these alone\n",
    "num_of_RFs = int((size*size) / (RF_size*RF_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.read_csv('Pattern_Matrices_Datasets/size16_RF2_noise0.csv')\n",
    "inputs = inputs.drop('X1', axis = 0) # Taking out col names\n",
    "inputs = inputs.apply(pd.to_numeric)  # converting to floats\n",
    "\n",
    "inputs = inputs.values # convert to np ndarray\n",
    "inputs = inputs.reshape(num_inputs, size, size,1) # reshape to desired dims (5000 examples, 16*16 image, 1 channel)\n",
    "flattened_inputs = inputs.reshape(num_inputs, size*size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting Number of Labels Possible to 256 if Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_inputs = flattened_inputs[0:256]\n",
    "flattened_inputs = np.repeat(flattened_inputs, 20, 0)\n",
    "flattened_inputs = np.random.permutation(flattened_inputs)\n",
    "\n",
    "num_inputs = int(flattened_inputs.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros(np.shape(flattened_inputs)[0])\n",
    "storage = np.zeros(np.shape(flattened_inputs))\n",
    "count = 0\n",
    "\n",
    "for i in range(np.shape(flattened_inputs)[0]):\n",
    "    if(~((flattened_inputs[i] == storage).all(1).any())):\n",
    "        labels[i] = count\n",
    "        storage[i] = flattened_inputs[i]\n",
    "        count += 1\n",
    "    elif((flattened_inputs[i] == storage).all(1).any()):\n",
    "        label_index = np.where((flattened_inputs[i] == storage).all(1))\n",
    "        correct_label = labels[label_index]\n",
    "        storage[i] = flattened_inputs[i]\n",
    "        labels[i] = correct_label[0]\n",
    "        \n",
    "labels = labels.astype(int)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Noise to Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(np.shape(flattened_inputs)[0]):\n",
    "    indices_to_flip = np.random.choice(int(size*size), math.ceil(size*size * noise), replace = False)\n",
    "    flattened_inputs[n][indices_to_flip] = 1 - flattened_inputs[n][indices_to_flip]\n",
    "\n",
    "inputs = flattened_inputs.reshape(num_inputs, size, size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 4)           20        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "=================================================================\n",
      "Total params: 262,932\n",
      "Trainable params: 262,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape = (16,16,1), filters=4, kernel_size = RF_size, strides = RF_size, activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dense(256, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(input_shape = (16,16,1), filters=4, kernel_size = RF_size, strides = RF_size, activation = 'relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    model.add(Dense(256, activation = 'softmax'))\n",
    "    model.compile(optimizer = optimizers.adam(lr = 0.0005), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(x = inputs, validation_split = 0.25, y = labels, batch_size = 100, epochs = 25, shuffle=False)\n",
    "    return(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3840 samples, validate on 1280 samples\n",
      "Epoch 1/25\n",
      "3840/3840 [==============================] - 2s 392us/step - loss: 5.3920 - acc: 0.0695 - val_loss: 5.1574 - val_acc: 0.2320\n",
      "Epoch 2/25\n",
      "3840/3840 [==============================] - 1s 245us/step - loss: 4.7348 - acc: 0.6065 - val_loss: 4.3647 - val_acc: 0.8180\n",
      "Epoch 3/25\n",
      "3840/3840 [==============================] - 1s 248us/step - loss: 3.7106 - acc: 0.9352 - val_loss: 3.1632 - val_acc: 0.9922\n",
      "Epoch 4/25\n",
      "3840/3840 [==============================] - 1s 249us/step - loss: 2.3472 - acc: 0.9977 - val_loss: 1.7040 - val_acc: 1.0000\n",
      "Epoch 5/25\n",
      "3840/3840 [==============================] - 1s 252us/step - loss: 1.0154 - acc: 1.0000 - val_loss: 0.5862 - val_acc: 1.0000\n",
      "Epoch 6/25\n",
      "3840/3840 [==============================] - 1s 229us/step - loss: 0.3300 - acc: 1.0000 - val_loss: 0.2011 - val_acc: 1.0000\n",
      "Epoch 7/25\n",
      "3840/3840 [==============================] - 1s 234us/step - loss: 0.1317 - acc: 1.0000 - val_loss: 0.1014 - val_acc: 1.0000\n",
      "Epoch 8/25\n",
      "3840/3840 [==============================] - 1s 228us/step - loss: 0.0739 - acc: 1.0000 - val_loss: 0.0647 - val_acc: 1.0000\n",
      "Epoch 9/25\n",
      "3840/3840 [==============================] - 1s 238us/step - loss: 0.0497 - acc: 1.0000 - val_loss: 0.0462 - val_acc: 1.0000\n",
      "Epoch 10/25\n",
      "3840/3840 [==============================] - 1s 257us/step - loss: 0.0366 - acc: 1.0000 - val_loss: 0.0351 - val_acc: 1.0000\n",
      "Epoch 11/25\n",
      "3840/3840 [==============================] - 1s 242us/step - loss: 0.0283 - acc: 1.0000 - val_loss: 0.0278 - val_acc: 1.0000\n",
      "Epoch 12/25\n",
      "3840/3840 [==============================] - 1s 236us/step - loss: 0.0228 - acc: 1.0000 - val_loss: 0.0227 - val_acc: 1.0000\n",
      "Epoch 13/25\n",
      "3840/3840 [==============================] - 1s 243us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0189 - val_acc: 1.0000\n",
      "Epoch 14/25\n",
      "3840/3840 [==============================] - 1s 241us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 1.0000\n",
      "Epoch 15/25\n",
      "3840/3840 [==============================] - 1s 232us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "Epoch 16/25\n",
      "3840/3840 [==============================] - 1s 222us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.0121 - val_acc: 1.0000\n",
      "Epoch 17/25\n",
      "3840/3840 [==============================] - 1s 239us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.0107 - val_acc: 1.0000\n",
      "Epoch 18/25\n",
      "3840/3840 [==============================] - 1s 245us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 19/25\n",
      "3840/3840 [==============================] - 1s 245us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 1.0000\n",
      "Epoch 20/25\n",
      "3840/3840 [==============================] - 1s 249us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "Epoch 21/25\n",
      "3840/3840 [==============================] - 1s 241us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 22/25\n",
      "3840/3840 [==============================] - 1s 240us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 23/25\n",
      "3840/3840 [==============================] - 1s 268us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 24/25\n",
      "3840/3840 [==============================] - 1s 267us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 25/25\n",
      "3840/3840 [==============================] - 1s 267us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9684fcf256ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mval_loss99\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mone_storage_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam_RF_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc15\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc99\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss15\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss99\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mstorage_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_storage_raw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "storage_raw = np.zeros((30,5))\n",
    "storage_means = np.zeros((6, 8))\n",
    "param_RF_size = np.array([2,4,8])\n",
    "num_inputs = int(5000) \n",
    "size = int(16)        \n",
    "counter = 0\n",
    "\n",
    "for n in range(5):\n",
    "    RF_size = int(param_RF_size[n])\n",
    "    \n",
    "    val_acc15 = np.zeros(10)\n",
    "    val_acc99 = np.zeros(10)\n",
    "    val_loss15 = np.zeros(10)\n",
    "    val_loss99 = np.zeros(10)\n",
    "    for i in range(10):\n",
    "        results = run_model()\n",
    "        val_acc15[i] = results.history['val_acc'][4]\n",
    "        val_acc99[i] = results.history['val_acc'][24]\n",
    "        val_loss15[i] = results.history['val_loss'][4]\n",
    "        val_loss99[i] = results.history['val_loss'][24]\n",
    "        one_storage_raw = np.array([RF_size, val_acc15[i], val_acc99[i], val_loss15[i], val_loss99[i]])\n",
    "        storage_raw[counter] = one_storage_raw\n",
    "        counter += 1\n",
    "\n",
    "    acc15 = np.mean(val_acc15)\n",
    "    acc100 = np.mean(val_acc99)\n",
    "    loss15 = np.mean(val_loss15)\n",
    "    loss100 = np.mean(val_loss99)\n",
    "    acc15_SD = np.std(val_acc15)\n",
    "    acc100_SD = np.std(val_acc99)\n",
    "    loss15_SD = np.std(val_loss15)\n",
    "    loss100_SD = np.std(val_loss99)\n",
    "    \n",
    "    one_res = np.array([acc15, acc100, loss15, loss100, acc15_SD, acc100_SD, loss15_SD, loss100_SD])\n",
    "    \n",
    "    storage_means[n] = one_res\n",
    "    \n",
    "    \n",
    "#print(\"acc15 = \" + str(np.mean(val_acc15)), \"\\n\",\n",
    "#      \"acc100 = \" + str(np.mean(val_acc99)), \"\\n\",\n",
    "#      \"loss15 = \" + str(np.mean(val_loss15)), \"\\n\",\n",
    "#      \"loss100 = \" + str(np.mean(val_loss99)), \"\\n\",\n",
    "#      \"acc15_SD = \" + str(np.std(val_acc15)), \"\\n\",\n",
    "#      \"acc100_SD = \" + str(np.std(val_acc99)), \"\\n\",\n",
    "#      \"loss15_SD = \" + str(np.std(val_loss15)), \"\\n\",\n",
    "#      \"loss100_SD = \" + str(np.std(val_loss99)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.55999990e-03, 4.39999990e-03, 5.63316564e+00, 8.48933345e+00,\n",
       "        1.38794809e-03, 2.12414684e-03, 9.47671172e-03, 1.45830508e-01],\n",
       "       [2.87999994e-03, 3.51999992e-03, 5.62125978e+00, 7.64534548e+00,\n",
       "        1.02449986e-03, 1.56767340e-03, 1.04571201e-02, 1.03240424e-01],\n",
       "       [3.51999992e-03, 3.27999993e-03, 5.61607899e+00, 7.25059834e+00,\n",
       "        1.24963992e-03, 1.21061965e-03, 6.95202617e-03, 6.17776430e-02],\n",
       "       [3.51999992e-03, 3.99999991e-03, 5.60019398e+00, 6.18217104e+00,\n",
       "        1.60798006e-03, 2.29085132e-03, 4.11125980e-03, 3.87551421e-02],\n",
       "       [3.35999992e-03, 2.95999993e-03, 5.59991209e+00, 6.22076390e+00,\n",
       "        1.11999997e-03, 1.24193395e-03, 2.64364437e-03, 2.46607574e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_raw_pd <- pd.DataFrame(storage_raw)\n",
    "storage_raw_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_means_pd <- pd.DataFrame(storage_means)\n",
    "storage_means_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = np.zeros((60, 11))\n",
    "param_RF_size = np.array([2,3,4,6,8])\n",
    "param_noise = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "param_file = np.array([\"Pattern_Matrices_Datasets/size16_RF2_noise0.csv\", \"Pattern_Matrices_Datasets/size16_RF8_noise0.csv\"])\n",
    "num_inputs = int(5000) \n",
    "size = int(16)        \n",
    "\n",
    "counter99 = 0\n",
    "\n",
    "for f in range(2):\n",
    "    file = param_file[f]\n",
    "    \n",
    "    for n in range(6):\n",
    "        noise = param_noise[n]\n",
    "        \n",
    "        # Read in #\n",
    "        inputs = pd.read_csv(file)\n",
    "        inputs = inputs.drop('X1', axis = 0) # Taking out col names\n",
    "        inputs = inputs.apply(pd.to_numeric)  # converting to floats\n",
    "\n",
    "        inputs = inputs.values # convert to np ndarray\n",
    "        inputs = inputs.reshape(num_inputs, size, size,1) # reshape to desired dims (5000 examples, 16*16 image, 1 channel)\n",
    "        flattened_inputs = inputs.reshape(num_inputs, size*size)\n",
    "\n",
    "        # Cutting Labels #\n",
    "        flattened_inputs = flattened_inputs[0:256]\n",
    "        flattened_inputs = np.repeat(flattened_inputs, 20, 0)\n",
    "        flattened_inputs = np.random.permutation(flattened_inputs)\n",
    "        num_inputs = int(flattened_inputs.shape[0])\n",
    "\n",
    "        # Create Labels #\n",
    "        labels = np.zeros(np.shape(flattened_inputs)[0])\n",
    "        storage = np.zeros(np.shape(flattened_inputs))\n",
    "        count = 0\n",
    "        for a in range(np.shape(flattened_inputs)[0]):\n",
    "            if(~((flattened_inputs[a] == storage).all(1).any())):\n",
    "                labels[a] = count\n",
    "                storage[a] = flattened_inputs[a]\n",
    "                count += 1\n",
    "            elif((flattened_inputs[a] == storage).all(1).any()):\n",
    "                label_index = np.where((flattened_inputs[a] == storage).all(1))\n",
    "                correct_label = labels[label_index]\n",
    "                storage[a] = flattened_inputs[i]\n",
    "                labels[a] = correct_label[0]\n",
    "\n",
    "        labels = labels.astype(int)\n",
    "        labels = to_categorical(labels)\n",
    "\n",
    "        # Add Noise #\n",
    "        for s in range(np.shape(flattened_inputs)[0]):\n",
    "            indices_to_flip = np.random.choice(int(size*size), math.ceil(size*size * noise), replace = False)\n",
    "            flattened_inputs[s][indices_to_flip] = 1 - flattened_inputs[s][indices_to_flip]\n",
    "\n",
    "        inputs = flattened_inputs.reshape(num_inputs, size, size, 1)\n",
    "\n",
    "\n",
    "        # Build Model #\n",
    "        def run_model():\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(input_shape = (16,16,1), filters=4, kernel_size = RF_size, strides = RF_size, activation = 'relu'))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(512, activation = 'relu'))\n",
    "            model.add(Dense(256, activation = 'softmax'))\n",
    "            model.compile(optimizer = optimizers.adam(lr = 0.0005), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "            history = model.fit(x = inputs, validation_split = 0.25, y = labels, batch_size = 100, epochs = 25, shuffle=False)\n",
    "            return(history)\n",
    "    \n",
    "        for r in range(5):\n",
    "            RF_size = int(param_RF_size[n])\n",
    "\n",
    "            val_acc15 = np.zeros(10)\n",
    "            val_acc99 = np.zeros(10)\n",
    "            val_loss15 = np.zeros(10)\n",
    "            val_loss99 = np.zeros(10)\n",
    "\n",
    "            \n",
    "            for i in range(10):\n",
    "                results = run_model()\n",
    "                val_acc15[i] = results.history['val_acc'][4]\n",
    "                val_acc99[i] = results.history['val_acc'][24]\n",
    "                val_loss15[i] = results.history['val_loss'][4]\n",
    "                val_loss99[i] = results.history['val_loss'][24]\n",
    "\n",
    "            acc15 = np.mean(val_acc15)\n",
    "            acc100 = np.mean(val_acc99)\n",
    "            loss15 = np.mean(val_loss15)\n",
    "            loss100 = np.mean(val_loss99)\n",
    "            acc15_SD = np.std(val_acc15)\n",
    "            acc100_SD = np.std(val_acc99)\n",
    "            loss15_SD = np.std(val_loss15)\n",
    "            loss100_SD = np.std(val_loss99)\n",
    "\n",
    "            one_res = np.array([file, noise, RF_size, acc15, acc100, loss15, loss100, acc15_SD, acc100_SD, loss15_SD, loss100_SD])\n",
    "\n",
    "            storage[counter99] = one_res\n",
    "            counter99 += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-9a91e1d728fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# summarize history for accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOS\n",
    "* Find Image dataset that's binarized (or binarize existing one) and do MI calculations to find max MI. To do this, sample RF sized chunks of an image until stable average MI value is reached\n",
    "* https://people.cs.umass.edu/~marlin/data.shtml, http://vision.lems.brown.edu/content/available-software-and-databases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
