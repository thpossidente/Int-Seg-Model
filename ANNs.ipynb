{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing and Running ANNs for RF Mutual Information Investigation\n",
    "## Thomas Possidente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Value Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Building Imports\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Value Inits - Specify as needed\n",
    "num_inputs = int(1000)  # number of dummy images in set\n",
    "size = int(16)          # Dimension of each dummy image should be size*size\n",
    "RF_size = int(4)        # Dimensions of the RF to be analyzed should be RF_size*RF_size\n",
    "\n",
    "# Value Inits - Leave these alone\n",
    "num_of_RFs = int((size*size) / (RF_size*RF_size))\n",
    "inputs_by_RF = np.empty([(num_inputs * size), size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.read_csv('test.csv')\n",
    "inputs = inputs.drop('X1', axis = 0) # Taking out col names\n",
    "inputs = inputs.apply(pd.to_numeric)  # converting to floats\n",
    "\n",
    "inputs = inputs.values # convert to np ndarray\n",
    "inputs = inputs.reshape(1000,16,16) # reshape to desired dims (1000 examples, 16*16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through num_inputs and extract each RF input, flatten, and store in ndarray\n",
    "\n",
    "origin_x = 0\n",
    "origin_y = 0\n",
    "count = 0\n",
    "\n",
    "for n in range(0, num_of_RFs): # For each RF field in dummy image\n",
    "    for i in range(0, num_inputs): # For each dummy image in set\n",
    "        single_RF_input = inputs[i, origin_y:(origin_y + RF_size), origin_x:(origin_x + RF_size)] # Extract 1 RF \n",
    "        single_RF_input_flat = single_RF_input.reshape((RF_size*RF_size)) \n",
    "        inputs_by_RF[count,] = single_RF_input_flat \n",
    "        count += 1\n",
    "    if(origin_x == (size - RF_size)):  # Changes RF field over image\n",
    "        origin_x = 0\n",
    "        origin_y += RF_size\n",
    "    else:\n",
    "        origin_x += RF_size\n",
    "        \n",
    "inputs_by_RF = inputs_by_RF.reshape(num_of_RFs, num_inputs, (RF_size*RF_size)) \n",
    "# ^Reshaping to (RF Location, Image number, flattened RF) - this makes it easier to select inputs for each separate ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "class Hebbian(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, output_dim, lmbda=1.0, eta=0.0005, connectivity='random', connectivity_prob=0.25, **kwargs):\n",
    "        '''\n",
    "        Constructor for the Hebbian learning layer.\n",
    "\n",
    "        args:\n",
    "            output_dim - The shape of the output / activations computed by the layer.\n",
    "            lambda - A floating-point valued parameter governing the strength of the Hebbian learning activation.\n",
    "            eta - A floating-point valued parameter governing the Hebbian learning rate.\n",
    "            connectivity - A string which determines the way in which the neurons in this layer are connected to\n",
    "                the neurons in the previous layer.\n",
    "        '''\n",
    "        self.output_dim = output_dim\n",
    "        self.lmbda = lmbda\n",
    "        self.eta = eta\n",
    "        self.connectivity = connectivity\n",
    "        self.connectivity_prob = connectivity_prob\n",
    "\n",
    "        super(Hebbian, self).__init__(**kwargs)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def random_conn_init(self, shape, dtype=None):\n",
    "        A = np.random.normal(0, 1, shape)\n",
    "        A[self.B] = 0\n",
    "        return tf.constant(A, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def zero_init(self, shape, dtype=None):\n",
    "        return np.zeros(shape)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # create weight variable for this layer according to user-specified initialization\n",
    "        if self.connectivity == 'random':\n",
    "            self.B = np.random.random(input_shape[0]) < self.connectivity_prob\n",
    "        elif self.connectivity == 'zero':\n",
    "            self.B = np.zeros(self.output_dim)\n",
    "            \n",
    "        if self.connectivity == 'all':\n",
    "            self.kernel = self.add_weight(name='kernel', shape=(np.prod(input_shape[1:]), \\\n",
    "                        np.prod(self.output_dim)), initializer='uniform', trainable=False)\n",
    "        elif self.connectivity == 'random':\n",
    "            self.kernel = self.add_weight(name='kernel', shape=(np.prod(input_shape[1:]), \\\n",
    "                        np.prod(self.output_dim)), initializer=self.random_conn_init, trainable=False)\n",
    "        elif self.connectivity == 'zero':\n",
    "            self.kernel = self.add_weight(name='kernel', shape=(np.prod(input_shape[1:]), \\\n",
    "                        np.prod(self.output_dim)), initializer=self.zero_init, trainable=False)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "        # call superclass \"build\" function\n",
    "        super(Hebbian, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, x):  # x is the input to the network\n",
    "        x_shape = tf.shape(x)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # reshape to (batch_size, product of other dimensions) shape\n",
    "        x = tf.reshape(x, (batch_size, tf.reduce_prod(x_shape[1:])))\n",
    "\n",
    "        # compute activations using Hebbian-like update rule\n",
    "        activations = self.lmbda * tf.matmul(self.kernel, x) \n",
    "        \n",
    "        # Should implement winner-takes-all on activations here\n",
    "        \n",
    "        for n in range(0, batch_size):\n",
    "            index_max = tf.argmax(activations[[n]])\n",
    "            activations[[n]] = activations[[n]] * 0\n",
    "            activations[[n]][index_max] = 1\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #activations = x + self.lmbda * tf.matmul(self.kernel, x)  # why \"x +\", should this be removed? \n",
    "                                                                  # Matrix Multiplication will give us the activations\n",
    "                                                                  # so why add the input to the activation? This shouldn't\n",
    "                                                                  # even work bc the dims of x are different than\n",
    "                                                                  # the dims of tf.matmul(kernal, x)\n",
    "\n",
    "        # compute outer product of activations matrix with itself\n",
    "            # outer_product = tf.matmul(tf.expand_dims(x, 1), tf.expand_dims(x, 0))  # No idea why you would compute outer prod \n",
    "                                                                               # of inputs w/ itself. This method of weight \n",
    "                                                                               # updating seems wrong for our purposes\n",
    "                    \n",
    "         # update the weight matrix of this layer\n",
    "            # self.kernel = self.kernel + tf.multiply(self.eta, tf.reduce_mean(outer_product, axis=2)) # Still why outer prod?\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                    \n",
    "        # Loop through each input and update weights via hebbian update\n",
    "        for i in range(0, batch_size):  \n",
    "            input_set_1 = x[[i]] # select one input\n",
    "            input_set = tf.tile(input_set_1, tf.shape(self.kernal)[1]) # repeat input\n",
    "            input_set = tf.reshape(input_set, [tf.shape(self.kernal)[1], tf.shape(self.kernal)[0]]) # reshape to transpose of kernal shape\n",
    "            input_set = tf.transpose(input_set) # transpose\n",
    "            self.kernal = self.kernal + (self.eta * (input_set - self.kernal) * activations[[i]])\n",
    "        \n",
    "       \n",
    "        self.kernel = tf.multiply(self.kernel, self.B) # zeroing node connections that started at 0\n",
    "        return K.reshape(activations, x_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Variable_3\n\t [[Node: _retval_Variable_3_0_0 = _Retval[T=DT_FLOAT, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable_3)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable_3\n\t [[Node: _retval_Variable_3_0_0 = _Retval[T=DT_FLOAT, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable_3)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-9aab9614b8f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable_3\n\t [[Node: _retval_Variable_3_0_0 = _Retval[T=DT_FLOAT, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable_3)]]"
     ]
    }
   ],
   "source": [
    "act = tf.Variable([[0.1, 0.9, 0.8, 0.2], [0.1, 0.1, 0.01, 0.4]])\n",
    "#index_max = tf.argmax(act[[0]])\n",
    "#act = act[[0]].assign([0,0,0,0])\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(act))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hebbian_20 (Hebbian)         (None, 4)                 64        \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 0\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Hebbian(input_shape = (1,(RF_size*RF_size)), output_dim = 4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loss(y_true, y_pred): return y_pred             \n",
    "\n",
    "class dummy_opt(keras.optimizers.Optimizer): \n",
    "    def __init__(self): pass\n",
    "    def get_updates(self, loss, params): pass\n",
    "    def get_configs(self): pass\n",
    "    \n",
    "dummyOpt = dummy_opt()\n",
    "\n",
    "model.compile(optimizer= dummyOpt,loss = dummy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"NoneType\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-e0ff613504dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_by_RF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1680\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1681\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1682\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1683\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    991\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m                         loss=self.total_loss)\n\u001b[1;32m--> 993\u001b[1;33m                 \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtraining_updates\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_updates\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m                 \u001b[1;31m# Gets loss and metrics. Updates weights at each call.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m                 self.train_function = K.function(inputs,\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"NoneType\") to list"
     ]
    }
   ],
   "source": [
    "model.fit(x = inputs_by_RF[0].reshape(1000, 1, 16), y = np.array([0]*1000), epochs = 1, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes/TODO\n",
    "\n",
    "* How to not use optimizer or use a dummy optimizer or change the weight updating to happen inside the optimizer function\n",
    "* Implement winner-take-all - how to change values in activation tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
