{
    "collab_server" : "",
    "contents" : "\nsigmoid.activation <- function(x){\n  #return(1 / (1+exp(-x)))\n  return(x)\n}\n\nnoise.in.letter <- function(letter){\n  for(i in 1:(0.1*n.input)){\n    letter[(sample(1:1600,1,replace=T))] <- 1\n  }\n  return(letter)\n}\n\nlearning.measure <- function(input.hidden.weights){\n  all.letters.compared <- numeric(26)\n  best.fit <- numeric(n.hidden)\n  for(i in 1:n.hidden){\n    for(h in 1:26){\n      all.letters.compared[h] <- sum(abs(input.hidden.weights[,i] - alphabet[[h]]))\n    }\n    best.fit[i] <- min(all.letters.compared)\n  }\n  return(best.fit)\n}\n\n\nforward.pass <- function(input, input.hidden.weights, hidden.bias.weights, hidden.output.weights, output.bias.weights){ #calculate output activations with \"winner-takes-all\" method\n  \n  hidden <- numeric(n.hidden)\n  \n  for(i in 1:n.hidden){\n    hidden[i] <- sigmoid.activation(sum(input * input.hidden.weights[,i]) + hidden.bias.weights[i,1])\n  }\n  \n  for(c in 1:ceiling(0.05*n.hidden)){\n    hidden[which.max(hidden)] <- -1\n  }\n  \n  for(j in 1:n.hidden){\n   if(hidden[j] == -1){\n     hidden[j] = 1\n   } else{\n     hidden[j] = 0\n   }\n  }\n  \n  #hidden[hidden != max(hidden)] <- 0\n  #hidden[which.max(hidden)] <- 1\n  \n  output <- numeric(n.output)\n  for(b in 1:n.output){\n    output[b] <- sigmoid.activation(sum(hidden * hidden.output.weights[,b] +  output.bias.weights[b,1]))\n  }\n  \n  for(h in 1:ceiling(0.1*n.output)){\n    output[which.max(output)] <- -1\n  }\n  \n  for(k in 1:n.output){\n    if(output[k] == -1){\n      output[k] = 1\n    } else{\n      output[k] = 0\n    }\n  }\n  \n  #output[output != max(output)] <- 0\n  #output[which.max(output)] <- 1\n  return(list(hidden=hidden, output=output))\n}\n\ntrace.update <- function(input, input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights){\n  \n  forward.pass.results <- forward.pass(input, input.hidden.weights, hidden.bias.weights, hidden.output.weights, output.bias.weights)\n  hidden <- forward.pass.results$hidden\n  output <- forward.pass.results$output\n  \n  for(h in 1:n.hidden){\n    if(hidden[h] == 1){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] - hidden.bias.param.minus\n    }\n    if(hidden[h] == 0){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] + hidden.bias.param.plus\n    }\n    if(hidden.bias.weights[h,1] < 0){\n      hidden.bias.weights[h,1] <- 0\n    }\n  }\n  \n  for(i in 1:n.hidden){\n    trace.hidden[i] <- (1 - trace.param.hidden) * trace.hidden[i] + trace.param.hidden * hidden[i] \n    input.hidden.weights[,i] <- input.hidden.weights[,i] + learning.rate.hidden * trace.hidden[i] * (input - input.hidden.weights[,i])\n  }\n  \n  for(j in 1:n.output){\n    if(output[j] == 1){\n      output.bias.weights[j,1] <- output.bias.weights[j,1] - output.bias.param.minus\n    }\n    if(output[j] == 0){\n      output.bias.weights[j,1] <- output.bias.weights[j,1] + output.bias.param.plus\n    }\n    if(output.bias.weights[j,1] < 0){\n      output.bias.weights[j,1] <- 0\n    }\n  }\n  \n  for(b in 1:n.output){\n    trace.output[b] <- (1 - trace.param.output) * trace.output[b] + trace.param.output * output[b]\n    hidden.output.weights[,b] <- hidden.output.weights[,b] + learning.rate.output * trace.output[b] * (hidden - hidden.output.weights[,b])\n  }\n  \n  return(list(\n    trace.hidden=trace.hidden, \n    hidden = hidden,\n    input.hidden.weights=input.hidden.weights, \n    hidden.bias.weights=hidden.bias.weights,\n    trace.output=trace.output,\n    output=output,\n    hidden.output.weights=hidden.output.weights,\n    output.bias.weights=output.bias.weights))\n}\n\nbatch <- function(n.epochs, network=NA){\n  # network properties #\n  if(is.na(network)){\n    network <- list(\n      input.hidden.weights = matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden), #initialiize weights at random values between 0 and 0.05\n      hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),\n      hidden.output.weights = matrix(runif(n.hidden*n.output, min=0, max=0.05), nrow=n.hidden, ncol=n.output),\n      output.bias.weights = matrix(0, nrow=n.output, ncol=1),\n      trace.hidden = rep(0, times = n.hidden),\n      trace.output = rep(0, times = n.output)\n    )\n  }\n \n  # tracking learning #\n  history <- list(\n    learning.curve = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix\n    bias.tracker = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix\n    output.bias.tracker = matrix(0, nrow = n.epochs/100, ncol= n.output),\n    hidden.win.tracker = matrix(0, nrow=n.epochs, ncol= n.hidden)\n  )\n  \n  pb <- txtProgressBar(min=1, max=n.epochs,style=3)\n  for(i in 1:n.epochs){\n    word <- words[[sample(1:9,1, replace = T)]]\n    \n    if(i %% 100 == 0){\n      history$learning.curve[i / 100,] <- learning.measure(network$input.hidden.weights)\n      history$bias.tracker[i / 100,] <- as.vector(network$hidden.bias.weights)\n      history$output.bias.tracker[i / 100,] <- as.vector(network$output.bias.weights)\n    }\n    \n    for(b in 1:(length(word)/n.input)){\n      \n      # get input vector\n      letter <- word[,b]\n      letter <- noise.in.letter(letter)\n      \n      # update network properties\n      results <- trace.update(letter, network$input.hidden.weights, network$trace.hidden, network$hidden.bias.weights, network$hidden.output.weights, network$trace.output, network$output.bias.weights)\n      network$input.hidden.weights <- results$input.hidden.weights\n      network$hidden.output.weights <- results$hidden.output.weights\n      network$trace.hidden <- results$trace.hidden\n      network$hidden.bias.weights <- results$hidden.bias.weights\n      network$hidden.output.weights <- results$hidden.output.weights\n      network$trace.output <- results$trace.output\n      network$output.bias.weights <- results$output.bias.weights\n      network$hidden.bias.weights <- results$hidden.bias.weights\n      \n      # update learning history\n      history$hidden.win.tracker[i,] <- results$hidden\n      \n      setTxtProgressBar(pb, i)\n    }\n  }\n  test.word.continuity(network, words)\n  return(list(\n    history=history,\n    network=network\n  ))\n}\n",
    "created" : 1492011096256.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1753119130",
    "id" : "CD1D6E62",
    "lastKnownWriteTime" : 1492016624,
    "last_content_update" : 1492016624,
    "path" : "~/GitHub/Int-Seg-Model/multi-layer-network.R",
    "project_path" : "multi-layer-network.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}