{
    "collab_server" : "",
    "contents" : "n.input <- 1600\nn.hidden <- 26\nn.output <- 20\nlearning.rate <- 0.05\nn.epochs <- 10000\nn.test <- 26\ntrace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account\ntrace.param.output <- 0.1\nhidden.bias.param.minus <- 1\nhidden.bias.param.plus <- 0.05\noutput.bias.param.minus <- 1\noutput.bias.param.plus <- 0.60\n\n#install.packages('png')\nlibrary('png')\n#install.packages('abind')\nlibrary('abind')\n\nalphabet <- list(\n  a <- as.vector(t(1-adrop(readPNG('AlphabetPNG/A.png')[,,1,drop=F], drop=3))),\n  b <- as.vector(t(1-adrop(readPNG('AlphabetPNG/B.png')[,,1,drop=F], drop=3))),\n  c <- as.vector(t(1-adrop(readPNG('AlphabetPNG/C.png')[,,1,drop=F], drop=3))),\n  d <- as.vector(t(1-adrop(readPNG('AlphabetPNG/D.png')[,,1,drop=F], drop=3))),\n  e <- as.vector(t(1-adrop(readPNG('AlphabetPNG/E.png')[,,1,drop=F], drop=3))),\n  f <- as.vector(t(1-adrop(readPNG('AlphabetPNG/F.png')[,,1,drop=F], drop=3))),\n  g <- as.vector(t(1-adrop(readPNG('AlphabetPNG/G.png')[,,1,drop=F], drop=3))),\n  h <- as.vector(t(1-adrop(readPNG('AlphabetPNG/H.png')[,,1,drop=F], drop=3))),\n  i <- as.vector(t(1-adrop(readPNG('AlphabetPNG/I.png')[,,1,drop=F], drop=3))),\n  j <- as.vector(t(1-adrop(readPNG('AlphabetPNG/J.png')[,,1,drop=F], drop=3))),\n  k <- as.vector(t(1-adrop(readPNG('AlphabetPNG/K.png')[,,1,drop=F], drop=3))),\n  l <- as.vector(t(1-adrop(readPNG('AlphabetPNG/L.png')[,,1,drop=F], drop=3))),\n  m <- as.vector(t(1-adrop(readPNG('AlphabetPNG/M.png')[,,1,drop=F], drop=3))),\n  n <- as.vector(t(1-adrop(readPNG('AlphabetPNG/N.png')[,,1,drop=F], drop=3))),\n  o <- as.vector(t(1-adrop(readPNG('AlphabetPNG/O.png')[,,1,drop=F], drop=3))),\n  p <- as.vector(t(1-adrop(readPNG('AlphabetPNG/P.png')[,,1,drop=F], drop=3))),\n  q <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Q.png')[,,1,drop=F], drop=3))),\n  r <- as.vector(t(1-adrop(readPNG('AlphabetPNG/R.png')[,,1,drop=F], drop=3))),\n  s <- as.vector(t(1-adrop(readPNG('AlphabetPNG/S.png')[,,1,drop=F], drop=3))),\n  t <- as.vector(t(1-adrop(readPNG('AlphabetPNG/T.png')[,,1,drop=F], drop=3))),\n  u <- as.vector(t(1-adrop(readPNG('AlphabetPNG/U.png')[,,1,drop=F], drop=3))),\n  v <- as.vector(t(1-adrop(readPNG('AlphabetPNG/V.png')[,,1,drop=F], drop=3))),\n  w <- as.vector(t(1-adrop(readPNG('AlphabetPNG/W.png')[,,1,drop=F], drop=3))),\n  x <- as.vector(t(1-adrop(readPNG('AlphabetPNG/X.png')[,,1,drop=F], drop=3))),\n  y <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Y.png')[,,1,drop=F], drop=3))),\n  z <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Z.png')[,,1,drop=F], drop=3)))\n)\n\nwords <- list(\n  abc <- cbind(a,b,c),\n  def <- cbind(d,e,f),\n  ghi <- cbind(g,h,i),\n  jkl <- cbind(j,k,l),\n  mno <- cbind(m,n,o),\n  pqr <- cbind(p,q,r), \n  stu <- cbind(s,t,u), \n  vwx <- cbind(v,w,x), \n  yz <- cbind(y,z)\n  \n)\n\nsigmoid.activation <- function(x){\n  #return(1 / (1+exp(-x)))\n  return(x)\n}\n\n\nforward.pass <- function(input, input.hidden.weights, hidden.bias.weights){ #calculate output activations with \"winner-takes-all\" method\n  \n  hidden <- numeric(n.hidden)\n  for(i in 1:n.hidden){\n    hidden[i] <- sigmoid.activation(sum(input * input.hidden.weights[,i]) + hidden.bias.weights[i,1])\n  }\n  hidden[hidden != max(hidden)] <- 0\n  hidden[which.max(hidden)] <- 1\n  return(hidden)\n}\n\nforward.pass.2 <- function(input, input.hidden.weights, hidden.bias.weights, hidden.output.weights, output.bias.weights){ #calculate output activations with \"winner-takes-all\" method\n  \n  hidden <- numeric(n.hidden)\n  for(i in 1:n.hidden){\n    hidden[i] <- sigmoid.activation(sum(input * input.hidden.weights[,i]) + hidden.bias.weights[i,1])\n  }\n  #\n  hidden[hidden != max(hidden)] <- 0\n  hidden[which.max(hidden)] <- 1\n  \n  output <- numeric(n.output)\n  for(b in 1:n.output){\n    output[b] <- sigmoid.activation(sum(hidden * hidden.output.weights[,b] +  output.bias.weights[b,1]))\n  }\n  output[output != max(output)] <- 0\n  output[which.max(output)] <- 1\n  return(list(hidden=hidden, output=output))\n}\n\n\ntrace.update <- function(input, input.hidden.weights, trace.hidden, hidden.bias.weights){\n  \n  hidden <- forward.pass(input, input.hidden.weights, hidden.bias.weights)\n  \n  for(h in 1:n.hidden){\n    if(hidden[h] == 1){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] - hidden.bias.param.minus\n    }\n    if(hidden[h] == 0){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] + hidden.bias.param.plus\n    }\n    if(hidden.bias.weights[h,1] < 0){\n      hidden.bias.weights[h,1] <- 0\n    }\n  }\n  \n  for(i in 1:n.hidden){\n    trace.hidden[i] <- (1 - trace.param.hidden) * trace.hidden[i] + trace.param.hidden * hidden[i] \n    input.hidden.weights[,i] <- input.hidden.weights[,i] + learning.rate * trace.hidden[i] * (input - input.hidden.weights[,i])\n  }\n  \n  return(list(\n    trace.hidden = trace.hidden,\n    hidden = hidden,\n    input.hidden.weights = input.hidden.weights, \n    hidden.bias.weights = hidden.bias.weights\n  ))\n}\n\nlearning.measure <- function(input.hidden.weights){\n  all.letters.compared <- numeric(26)\n  best.fit <- numeric(n.hidden)\n  for(i in 1:n.hidden){\n    for(h in 1:26){\n      all.letters.compared[h] <- sum(abs(input.hidden.weights[,i] - alphabet[[h]]))\n    }\n    best.fit[i] <- min(all.letters.compared)\n  }\n  return(best.fit)\n}\n\ndisplay.learning.curves <- function(results){\n  for(i in 1:n.hidden){\n    layout(matrix(1:4, nrow=2))\n    plot(results$learning.curve[,i], main=paste(\"Node\",i))\n    plot(results$bias.tracker[,i])\n    image(matrix(results$input.hidden.weights[,i], nrow = 40))\n  }\n}\n\ntrace.update.2 <- function(input, input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights){\n  \n  forward.pass.results <- forward.pass.2(input, input.hidden.weights, hidden.bias.weights, hidden.output.weights, output.bias.weights)\n  hidden <- forward.pass.results$hidden\n  output <- forward.pass.results$output\n  \n  for(h in 1:n.hidden){\n    if(hidden[h] == 1){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] - hidden.bias.param.minus\n    }\n    if(hidden[h] == 0){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] + hidden.bias.param.plus\n    }\n    if(hidden.bias.weights[h,1] < 0){\n      hidden.bias.weights[h,1] <- 0\n    }\n  }\n  \n  for(i in 1:n.hidden){\n    trace.hidden[i] <- (1 - trace.param.hidden) * trace.hidden[i] + trace.param.hidden * hidden[i] \n    input.hidden.weights[,i] <- input.hidden.weights[,i] + learning.rate * trace.hidden[i] * (input - input.hidden.weights[,i])\n  }\n  \n  for(j in 1:n.output){\n    if(output[j] == 1){\n      output.bias.weights[j,1] <- output.bias.weights[j,1] - output.bias.param.minus\n    }\n    if(output[j] == 0){\n      output.bias.weights[j,1] <- output.bias.weights[j,1] + output.bias.param.plus\n    }\n    if(output.bias.weights[j,1] < 0){\n      output.bias.weights[j,1] <- 0\n    }\n  }\n  \n  for(b in 1:n.output){\n    trace.output[b] <- (1 - trace.param.output) * trace.output[b] + trace.param.output * output[b]\n    hidden.output.weights[,b] <- hidden.output.weights[,b] + learning.rate * trace.output[b] * (hidden - hidden.output.weights[,b])\n  }\n  \n  return(list(\n    trace.hidden=trace.hidden, \n    hidden = hidden,\n    input.hidden.weights=input.hidden.weights, \n    hidden.bias.weights=hidden.bias.weights,\n    trace.output=trace.output,\n    output=output,\n    hidden.output.weights=hidden.output.weights,\n    output.bias.weights=output.bias.weights))\n}\n\n\nlearning.measure <- function(input.hidden.weights){\n  all.letters.compared <- numeric(26)\n  best.fit <- numeric(n.hidden)\n  for(i in 1:n.hidden){\n    for(h in 1:26){\n      all.letters.compared[h] <- sum(abs(input.hidden.weights[,i] - alphabet[[h]]))\n    }\n    best.fit[i] <- min(all.letters.compared)\n  }\n  return(best.fit)\n}\n\n\ndisplay.learning.curves <- function(results){\n  for(i in 1:n.hidden){\n    layout(matrix(1:4, nrow=2))\n    plot(results$learning.curve[,i], main=paste(\"Node\",i))\n    plot(results$bias.tracker[,i])\n    image(matrix(results$input.hidden.weights[,i], nrow = 40))\n  }\n}\n\ndisplay.output.bias.tracker <- function(results){\n  for(i in 1:n.output){\n    plot(results$output.bias.tracker[,i], main=paste('Node', i))\n  }\n}\n\n\nbatch <- function(n.epochs){\n  \n  # network properties #\n  input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden) #initialiize weights at random values between 0 and 0.05\n  hidden.bias.weights <- matrix(0, nrow=n.hidden, ncol=1)\n  \n  # tracking learning #\n  learning.curve <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix\n  bias.tracker <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix\n  hidden.win.tracker <- matrix(0, nrow=n.epochs, ncol= n.hidden)\n  \n  pb <- txtProgressBar(min=1, max=n.epochs,style=3)\n  for(i in 1:n.epochs){\n    letter <- alphabet[[sample(1:26,1, replace = T)]]\n    results <- trace.update(letter, input.hidden.weights, trace.hidden, hidden.bias.weights)\n    input.hidden.weights <- results$input.hidden.weights\n    trace.hidden <- results$trace.hidden\n    hidden.bias.weights <- results$hidden.bias.weights\n    hidden.win.tracker[i,] <- results$hidden\n    if(i %% 100 == 0){\n      learning.curve[i / 100,] <- learning.measure(input.hidden.weights)\n      bias.tracker[i / 100,] <- as.vector(hidden.bias.weights)\n    }\n    setTxtProgressBar(pb, i)\n  }\n  return(list(\n    input.hidden.weights=input.hidden.weights, \n    learning.curve=learning.curve, \n    bias.tracker=bias.tracker,\n    hidden.bias.weights=hidden.bias.weights,\n    hidden.win.tracker = hidden.win.tracker\n  ))\n}\n\n\nbatch.2 <- function(n.epochs){ \n  # network properties #\n  input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden) #initialiize weights at random values between 0 and 0.05\n  hidden.bias.weights <- matrix(0, nrow=n.hidden, ncol=1)\n  hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=0.05), nrow=n.hidden, ncol=n.output)\n  output.bias.weights <- matrix(0, nrow=n.output, ncol=1)\n  trace.hidden <- rep(0, times = n.hidden)\n  trace.output <- rep(0, times = n.output)\n  \n  # tracking learning #\n  learning.curve <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix\n  bias.tracker <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix\n  output.bias.tracker <- matrix(0, nrow = n.epochs/100, ncol= n.output)\n  hidden.win.tracker <- matrix(0, nrow=n.epochs, ncol= n.hidden)\n  \n  pb <- txtProgressBar(min=1, max=n.epochs,style=3)\n  for(i in 1:n.epochs){\n    word <- words[[sample(1:9,1, replace = T)]]\n    for(b in 1:(length(word)/n.input)){\n      letter <- word[,b]\n      letter <- noise.in.letter(letter)\n      results <- trace.update.2(letter, input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights)\n      input.hidden.weights <- results$input.hidden.weights\n      hidden.output.weights <- results$hidden.output.weights\n      trace.hidden <- results$trace.hidden\n      hidden.bias.weights <- results$hidden.bias.weights\n      hidden.output.weights <- results$hidden.output.weights\n      trace.output <- results$trace.output\n      output.bias.weights <- results$output.bias.weights\n      hidden.bias.weights <- results$hidden.bias.weights\n      hidden.win.tracker[i,] <- results$hidden\n      if(i %% 100 == 0){\n        learning.curve[i / 100,] <- learning.measure(input.hidden.weights)\n        bias.tracker[i / 100,] <- as.vector(hidden.bias.weights)\n        output.bias.tracker[i / 100] <- as.vector(output.bias.weights)\n      }\n      setTxtProgressBar(pb, i)\n    }\n  }\n  \n  temp.layer.activations(input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights)\n  \n  return(list(\n    input.hidden.weights=input.hidden.weights,\n    hidden.output.weights=hidden.output.weights,\n    learning.curve=learning.curve, \n    bias.tracker=bias.tracker,\n    output.bias.tracker=output.bias.tracker,\n    hidden.bias.weights=hidden.bias.weights,\n    hidden.win.tracker = hidden.win.tracker\n  ))\n}\n\n\n\n## weight/activation image generation and noise func. ##\n\n\nweight.images <- function(){\n  return(\n    for(i in 1:26){\n      image(matrix(input.hidden.weights[,i], nrow = 40))\n    })\n}\n\n\ntemp.layer.activations <- function(input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights){\n  storing.activations <- matrix(0, nrow=n.hidden, ncol=n.output)\n  for(i in 1:length(words)){\n    word <- words[[i]]\n    for(j in 1:(length(word)/n.input)){\n      letter <- word[,j]\n      act.results <- trace.update.2(letter, input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights)\n      storing.activations[((i-1)*3)+j,] <- act.results$output\n    }\n  }\n  image(storing.activations)\n  print(storing.activations)\n}\n\n\nnoise.in.letter <- function(letter){\n  for(i in 1:(0.1*n.input)){\n    letter[(sample(1:1600,1,replace=T))] <- 1\n  }\n  return(letter)\n}\n\n\n## RUN ##\n\nresults <- batch.2(n.epochs) #run training batches\n\ndisplay.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs\ndisplay.output.bias.tracker(results)\n",
    "created" : 1490801554743.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2105163954",
    "id" : "3AC3CB8D",
    "lastKnownWriteTime" : 1491067993,
    "last_content_update" : 1491067993716,
    "path" : "~/GitHub/Int-Seg-Model/Spatial Pooling.R",
    "project_path" : "Spatial Pooling.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}