{
    "collab_server" : "",
    "contents" : "#install.packages('ggplot2')\nlibrary(ggplot2)\n\nn.input <- 1600\nn.hidden <- 100\nn.output <- 30\nlearning.rate.hidden <- 0.005\nlearning.rate.output <- 0.005\nn.epochs <- 10000\ntrace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account\ntrace.param.output <- 0.25 #0.75\nhidden.bias.param.minus <- 2\nhidden.bias.param.plus <- 0.0005\noutput.bias.param.minus <- 0 #0\noutput.bias.param.plus <- 0 #0\nsparseness.percent <- 0.75\nnum.inputs.generated <- 50\nintegration.parameter <- 1 #0 is totally segregated, 1 is totally integrated\npercent.act.input <- 0.05\npercent.act.output <- 0.1\nn.words <- length(words)\ninput.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input) \n                         # if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs\n                         # if 0.5: inputs for each system consistently co-occur\n\nsource('Load Letters.R')\nsource('Visualize Output.R')\nsource('multi-layer-network.R')\n\n## RUN ##\n\nresults <- batch(n.epochs) #run training batches\n\nvisualize.hidden.layer.learning(results$history)\ndisplay.learning.curves(results) \ndisplay.output.bias.tracker(results)\nvisualize.letter.activations(results$network, s)\nvisualize.output.act.match()\nn <- temp.layer.many.activations(network, words)\n\nnetwork <- results$network\n\n",
    "created" : 1517327225798.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "489125608",
    "id" : "BFA33742",
    "lastKnownWriteTime" : 1517329114,
    "last_content_update" : 1517329114,
    "path" : "~/GitHub/Int-Seg-Model/Spatial Pooling.R",
    "project_path" : "Spatial Pooling.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}