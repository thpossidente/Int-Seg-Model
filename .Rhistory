## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 9
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 500000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 9
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 9
learning.rate.hidden <- 0.01
learning.rate.output <- 0.2
n.epochs <- 100000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.2
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run train
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
results$network$hidden.output.weights
results$network$output.bias.weights
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.025
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
results$network$output.bias.weights
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.025
learning.rate.output <- 0.2
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.01
learning.rate.output <- 0.2
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.005
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.005
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.025
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.005
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.0005
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.0001
learning.rate.output <- 0.2
n.epochs <- 30000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.15
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.0001
learning.rate.output <- 0.2
n.epochs <- 30000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training bat
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.00005
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
results$network$hidden.output.weights
results$network$output.bias.weights
display.output.bias.tracker(results)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run trainin
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 30000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.2
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 25
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.2
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #r
display.output.bias.tracker(results)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 25
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
test.word.continuity(results$network, words)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 50
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
test.word.continuity(results$network, words)
temp.layer.activations <- function(network, input.matrix){
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
output.results <- rbind(output.results, c(letter=i, output=which.max(storing.activations[i,])))
}
colnames(output.results) <- c("letter", "output")
g <- ggplot(output.results, aes(x=letter, y=output)) +
geom_point()+
ylim(1,50)+
theme_bw()
print(g)
#image(storing.activations)
print(storing.activations)
}
test.word.continuity(results$network, words)
results <- batch(n.epochs) #run training batches
results$network$hidden.output.weights
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
results$network$output.bias.weights
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.1
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
