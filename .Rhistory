<<<<<<< HEAD
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
=======
<<<<<<< HEAD
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,4], type='b', ylim=c(0,0.02))
=======
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
<<<<<<< HEAD
learning.rate.output <- 0.009 # 0.009
=======
learning.rate.output <- 0.005
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
output.trace.tracker.results <- results$history$trace.output.tracker
network <- results$network
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,9], type = "b")
visualize.output.act.match()
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
=======
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
=======
output.bias.param.minus <- 0.5 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
<<<<<<< HEAD
=======
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
temp.layer.activations.many <- temp.layer.many.activations(network, words)
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,9], type = "b")
Rcpp::sourceCpp('forwardPassCpp.cpp')
visualize.output.act.match()
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
install.packages("RcppArmadillo")
Rcpp::sourceCpp('forwardPassCpp.cpp')
library(RcppArmadillo)
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
=======
Rcpp::sourceCpp('forwardPassCpp.cpp')
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
<<<<<<< HEAD
learning.rate.output <- 0.009 # 0.009
=======
learning.rate.output <- 0.005
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
=======
output.bias.param.minus <- 0.5 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
=======
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
0.005*0.005*100000
+0.009
0.005*0.005*10000
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,4], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,5], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.02))
mutual.info.output()
=======
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,4], type='b', ylim=c(0,0.1))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,5], type='b', ylim=c(0,0.1))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,5], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,4], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.02))
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("letter", "output")
View(storing.activations)
View(output.results)
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
View(input.matrix)
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
Rcpp::sourceCpp('forwardPassCpp.cpp')
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
test.word.continuity1(results$network, words)
test.word.continuity1(results$network, words)[2]
res <- test.word.continuity1(results$network, words)
res <- test.word.continuity1(results$network, words)
res
source('C:/Users/Tom/Desktop/GitHub/Int-Seg-Model/Visualize Output.R')
res <- test.word.continuity1(results$network, words)
res
visualize.hidden.layer.learning(results$history)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,11], type='b', ylim=c(0,0.015))
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
visualize.output.act.match()
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
=======
output.bias.param.minus <- 0.5 #0
<<<<<<< HEAD
=======
output.bias.param.plus <- 0.0005 #0
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
<<<<<<< HEAD
test.word.continuity1(results$network, words)
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.02))
=======
<<<<<<< HEAD
test.word.continuity1(results$network, words)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
=======
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
visualize.output.act.match()
test.word.continuity1(results$network, words)
<<<<<<< HEAD
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
visualize.hidden.layer.learning(results$history)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
results$network$hidden.output.weights
results$network$hidden.output.weights[,1]
sum(results$network$hidden.output.weights[,1])
sum(results$network$hidden.output.weights[,1], na.rm=T)
=======
Rcpp::sourceCpp('forwardPassCpp.cpp')
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 15000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.00005 #0
=======
output.bias.param.minus <- 0.5 #0
<<<<<<< HEAD
output.bias.param.plus <- 0.0005
=======
output.bias.param.plus <- 0.0005 #0
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,2], type='b', ylim=c(0,0.02))
=======
<<<<<<< HEAD
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.005))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.005))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.005))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.005))
=======
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
Rcpp::sourceCpp('forwardPassCpp.cpp')
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
=======
output.bias.param.minus <- 0.5 #0
<<<<<<< HEAD
output.bias.param.plus <- 0.00005 #0
=======
output.bias.param.plus <- 0.0005 #0
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,2], type='b', ylim=c(0,0.02))
test.word.continuity1(results$network, words)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
=======
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
=======
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
=======
<<<<<<< HEAD
output.bias.param.minus <- 0.05 #0
output.bias.param.plus <- 0.00005 #0
=======
output.bias.param.minus <- 0.5 #0
output.bias.param.plus <- 0.0005 #0
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
=======
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
visualize.hidden.layer.learning(results$history)
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,2], type='b', ylim=c(0,0.02))
=======
=======
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0.25 #0
output.bias.param.plus <- 0.0001 #0
=======
<<<<<<< HEAD
output.bias.param.minus <- 0.005 #0
output.bias.param.plus <- 0.00005 #0
=======
output.bias.param.minus <- 0.5 #0
output.bias.param.plus <- 0.0005 #0
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,7], type='b', ylim=c(0,0.02))
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
network <- results$network
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("letter", "output")
View(output.results)
acts <- numeric(3)
word.acts <- matrix(0, nrow = 10, ncol <- 1)
acts <- numeric(3)
counter2 <- 1
for(f in 1:24){
acts[counter2] <- output.results[f,1]
if(f %% 3 == 0){counter2 = 0}
}
word.acts <- matrix(0, nrow = 10, ncol <- 1)
acts <- numeric(3)
counter2 <- 0
counter3 <- 0
for(f in 1:24){
counter2 <- counter2 + 1
acts[counter2] <- output.results[f,1]
if(f %% 3 == 0){
counter2 = 0
counter3 <- counter3 + 1
word.acts[counter3,1] <- acts
}
}
word.acts <- matrix(0, nrow = 10, ncol <- 1)
acts <- numeric(3)
counter2 <- 0
counter3 <- 0
for(f in 1:24){
counter2 <- counter2 + 1
acts[counter2] <- output.results[f,2]
if(f %% 3 == 0){
counter2 = 0
counter3 <- counter3 + 1
word.acts[counter3,1] <- acts
}
}
source('~/GitHub/Int-Seg-Model/Visualize Output.R', echo=TRUE)
word.acts <- matrix(0, nrow = 10, ncol <- 1)
word.acts <- matrix(0, nrow = 10, ncol <- 1)
acts <- numeric(3)
counter2 <- 0
counter3 <- 0
for(f in 1:24){
counter2 <- counter2 + 1
acts[counter2] <- output.results[f,2]
if(f %% 3 == 0){
counter2 = 0
counter3 <- counter3 + 1
word.acts[counter3,1] <- c(acts)
}
}
word.acts <- matrix(0, nrow = 10, ncol <- 3)
acts <- numeric(3)
counter2 <- 0
counter3 <- 0
for(f in 1:24){
counter2 <- counter2 + 1
acts[counter2] <- output.results[f,2]
if(f %% 3 == 0){
counter2 = 0
counter3 <- counter3 + 1
word.acts[counter3,1:3] <- acts
}
}
View(word.acts)
word.acts[9,1:3] <- c(output.results[25,2], output.results[26,2])
word.acts[9,1:2] <- c(output.results[25,2], output.results[26,2])
word.acts <- matrix(0, nrow = 9, ncol <- 3)
acts <- numeric(3)
counter2 <- 0
counter3 <- 0
for(f in 1:24){
counter2 <- counter2 + 1
acts[counter2] <- output.results[f,2]
if(f %% 3 == 0){
counter2 = 0
counter3 <- counter3 + 1
word.acts[counter3,1:3] <- acts
}
}
word.acts[9,1:2] <- c(output.results[25,2], output.results[26,2])
View(word.acts)
(sum(word.acts == word.acts[w,1]))
word.acts[1,1]
word.acts == word.acts[1,1]
sum(word.acts == word.acts[1,1])
mutual.info <- 0
for(w in 1:9){
prob.word <- ((sum(word.acts == word.acts[w,1]))/26) + ((sum(word.acts == word.acts[w,2]))/26) + ((sum(word.acts == word.acts[w,3]))/26)
prob.act <- 1/9
mutual.info = mutual.info + ((1/9)*(log2((1/9)/(prob.act*prob.word)))
}
}
mutual.info <- 0
for(w in 1:9){
prob.word <- ((sum(word.acts == word.acts[w,1]))/26) + ((sum(word.acts == word.acts[w,2]))/26) + ((sum(word.acts == word.acts[w,3]))/26)
prob.act <- 1/9
mutual.info = mutual.info + ((1/9)*(log2((1/9)/(prob.act*prob.word)))
}
}
mutual.info <- 0
for(w in 1:9){
prob.word <- ((sum(word.acts == word.acts[w,1]))/26) + ((sum(word.acts == word.acts[w,2]))/26) + ((sum(word.acts == word.acts[w,3]))/26)
prob.act <- 1/9
mutual.info = mutual.info + ((1/9)*(log2((1/9)/(prob.act*prob.word))))
}
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,2], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.02))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,3], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.02))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.25 #0
output.bias.param.plus <- 0.0001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
mutual.info.output()
<<<<<<< HEAD
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.02))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.25 #0
output.bias.param.plus <- 0.0001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 4000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.25 #0
output.bias.param.plus <- 0.0001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.02))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 6000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.25 #0
output.bias.param.plus <- 0.0001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.02))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 7000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.25 #0
output.bias.param.plus <- 0.0001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.02))
mutual.info.output()
test.word.continuity1(results$network, words)
test.word.continuity1(results$network, words)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 700
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.25 #0
output.bias.param.plus <- 0.0001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.02))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,5], type='b', ylim=c(0,0.02))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 700
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.02 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
mutual.info.output()
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,5], type='b', ylim=c(0,0.02))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,7], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,6], type='b', ylim=c(0,0.02))
=======
>>>>>>> bcac8d363f32e16dd42e11cced28c8c7a39c5470
>>>>>>> 7077b495b8d455a16ff26ddd56c5c35e8693424e
