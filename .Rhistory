<<<<<<< HEAD
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
=======
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
temp.layer.activations(network, input.matrix)
}
temp.layer.activations <- function(network, input.matrix){
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("letter", "output")
## accuracy measurement ##
counter <- 1
num.matches <- 0
act.per.word <- ceiling(length(output.results$output)/(n.output*percent.act.output*3))
for(b in seq(from = act.per.word, to = length(output.results$output) + act.per.word, by = act.per.word)){
freq <- rle(sort(output.results$output[counter:b]))
counter <- counter + 9
for(h in 1:length(freq$lengths)){
if(freq$lengths[h] > 1){
num.matches = num.matches + freq$lengths[h]
}
}
}
percentage <- num.matches/78
###
g <- ggplot(output.results, aes(x=letter, y=output)) +
geom_point()+
ylim(1,50)+
theme_bw()
return(percentage*100)
}
visualize.letter.activations <- function(network, input){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
nplots <- length(active.nodes) + 2
nrow <- round(sqrt(nplots))
ncol <- ceiling(nplots / nrow)
layout(matrix(1:(nrow*ncol), nrow=nrow))
image(t(apply(matrix(input, nrow = 40),1,rev)))
for(act in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,act], nrow = 40),1,rev)))
}
all.active.nodes <- network$input.hidden.weights[,active.nodes]
average.weights <- calculate.mean.weights(all.active.nodes)
image(t(apply(matrix(average.weights, nrow = 40),1,rev)))
}
calculate.mean.weights <- function(active.nodes){
m.fun <- function(x) { return(mean(x, na.rm=T)) }
average.weights <- apply(active.nodes, 1, m.fun)
return(average.weights)
}
hidden.layer.similarity <- function(letter, network, comparison.letter=NA){
result <- forward.pass(letter, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
all.active.nodes <- network$input.hidden.weights[,active.nodes]
average.weights <- calculate.mean.weights(all.active.nodes)
if(!all(is.na(comparison.letter))){
similarity <- sum(abs(comparison.letter - average.weights), na.rm = T)
} else {
similarity <- sum(abs(letter - average.weights), na.rm = T)
}
return(similarity)
}
batch.hidden.layer.learning <- function(letters, network){
result <- data.frame(input=names(letters), similarity=NA)
for(i in 1:nrow(result)){
result[i,"similarity"] <- hidden.layer.similarity(letters[[names(letters)[i]]], network)
}
return(result)
}
visualize.hidden.layer.learning <- function(history){
plotting.data <- expand.grid(letter=names(letters), time=1:nrow(history$hidden.letter.similarity.tracking))
plotting.data$similarity <- mapply(function(l, t){
return(history$hidden.letter.similarity.tracking[t,which(names(letters)==l)])
}, plotting.data$letter, plotting.data$time)
summary.data <- plotting.data %>% group_by(time) %>% summarize(mean.similarity = mean(similarity))
ggplot(plotting.data, aes(x=time, y=similarity, color = letter))+ geom_line() +
geom_line(data=summary.data, aes(x=time, y=mean.similarity, color=NA), size=2)+
labs(x='time', y='difference between network representation and input letter')
}
hidden.layer.stability <- function(letter, input, network, history){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
previous.active.nodes <- history$hidden.stability.tracking[[letter]]
change <- length(active.nodes) - sum(active.nodes %in% previous.active.nodes)
return(change)
}
batch.hidden.layer.stability <- function(letters, network, history){
result <- data.frame(input=names(letters), stability=NA)
for(i in 1:nrow(result)){
result[i,"stability"] <- hidden.layer.stability(names(letters)[i], letters[[names(letters)[i]]], network, history)
}
return(result$stability)
}
update.hidden.layer.stability <- function(letters, network){
tracker <- sapply(names(letters), function(x){
result <- forward.pass(letters[[x]], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
return(active.nodes)
}, USE.NAMES = T, simplify=FALSE)
return(tracker)
}
visualize.output.act.match <- function(){
plot(results$history$output.match.tracker, ylim = range(0, 100), type='o', ann = F)
title(xlab = 'Time', ylab = 'Percentage of activation matches')
}
#install.packages('png')
library('png')
#install.packages('abind')
library('abind')
alphabet <- list(
a <- as.vector(t(1-adrop(readPNG('AlphabetPNG/A.png')[,,1,drop=F], drop=3))),
b <- as.vector(t(1-adrop(readPNG('AlphabetPNG/B.png')[,,1,drop=F], drop=3))),
c <- as.vector(t(1-adrop(readPNG('AlphabetPNG/C.png')[,,1,drop=F], drop=3))),
d <- as.vector(t(1-adrop(readPNG('AlphabetPNG/D.png')[,,1,drop=F], drop=3))),
e <- as.vector(t(1-adrop(readPNG('AlphabetPNG/E.png')[,,1,drop=F], drop=3))),
f <- as.vector(t(1-adrop(readPNG('AlphabetPNG/F.png')[,,1,drop=F], drop=3))),
g <- as.vector(t(1-adrop(readPNG('AlphabetPNG/G.png')[,,1,drop=F], drop=3))),
h <- as.vector(t(1-adrop(readPNG('AlphabetPNG/H.png')[,,1,drop=F], drop=3))),
i <- as.vector(t(1-adrop(readPNG('AlphabetPNG/I.png')[,,1,drop=F], drop=3))),
j <- as.vector(t(1-adrop(readPNG('AlphabetPNG/J.png')[,,1,drop=F], drop=3))),
k <- as.vector(t(1-adrop(readPNG('AlphabetPNG/K.png')[,,1,drop=F], drop=3))),
l <- as.vector(t(1-adrop(readPNG('AlphabetPNG/L.png')[,,1,drop=F], drop=3))),
m <- as.vector(t(1-adrop(readPNG('AlphabetPNG/M.png')[,,1,drop=F], drop=3))),
n <- as.vector(t(1-adrop(readPNG('AlphabetPNG/N.png')[,,1,drop=F], drop=3))),
o <- as.vector(t(1-adrop(readPNG('AlphabetPNG/O.png')[,,1,drop=F], drop=3))),
p <- as.vector(t(1-adrop(readPNG('AlphabetPNG/P.png')[,,1,drop=F], drop=3))),
q <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Q.png')[,,1,drop=F], drop=3))),
r <- as.vector(t(1-adrop(readPNG('AlphabetPNG/R.png')[,,1,drop=F], drop=3))),
s <- as.vector(t(1-adrop(readPNG('AlphabetPNG/S.png')[,,1,drop=F], drop=3))),
t <- as.vector(t(1-adrop(readPNG('AlphabetPNG/T.png')[,,1,drop=F], drop=3))),
u <- as.vector(t(1-adrop(readPNG('AlphabetPNG/U.png')[,,1,drop=F], drop=3))),
v <- as.vector(t(1-adrop(readPNG('AlphabetPNG/V.png')[,,1,drop=F], drop=3))),
w <- as.vector(t(1-adrop(readPNG('AlphabetPNG/W.png')[,,1,drop=F], drop=3))),
x <- as.vector(t(1-adrop(readPNG('AlphabetPNG/X.png')[,,1,drop=F], drop=3))),
y <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Y.png')[,,1,drop=F], drop=3))),
z <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Z.png')[,,1,drop=F], drop=3)))
)
letters <- list(
"A" = a,
"B" = b,
"C" = c,
"D" = d,
"E" = e,
"F" = f,
"G" = g,
"H" = h,
"I" = i,
"J" = j,
"K" = k,
"L" = l,
"M" = m,
"N" = n,
"O" = o,
"P" = p,
"Q" = q,
"R" = r,
"S" = s,
"T" = t,
"U" = u,
"V" = v,
"W" = w,
"X" = x,
"Y" = y,
"Z" = z
)
words <- list(
abc <- cbind(a,b,c),
def <- cbind(d,e,f),
ghi <- cbind(g,h,i),
jkl <- cbind(j,k,l),
mno <- cbind(m,n,o),
pqr <- cbind(p,q,r),
stu <- cbind(s,t,u),
vwx <- cbind(v,w,x),
yz <- cbind(y,z)
)
sigmoid.activation <- function(x){
#return(1 / (1+exp(-x)))
return(x)
}
noise.in.letter <- function(letter){
for(i in 1:(0.1*n.input)){
letter[(sample(1:1600,1,replace=T))] <- 1
}
return(letter)
}
learning.measure <- function(input.hidden.weights){
all.letters.compared <- numeric(26)
best.fit <- numeric(n.hidden)
for(i in 1:n.hidden){
for(h in 1:26){
all.letters.compared[h] <- sum(abs(input.hidden.weights[,i] - alphabet[[h]]))
}
best.fit[i] <- min(all.letters.compared)
}
return(best.fit)
}
>>>>>>> b3102444d605f6c298c8a8eaa56bd610efa20724
>>>>>>> origin/master
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
learning.rate.hidden <- 0.005
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
results <- batch(n.epochs) #run training batches
?error
?Error
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
hidden <- numeric(n.hidden)
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
results <- batch(n.epochs) #run training batches
n.words <- length(words)
results <- batch(n.epochs) #run training batches
hidden <- numeric(n.hidden)
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.output <- 0.1
percent.act.input <- 0.05
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
results <- batch(n.epochs) #run training batches
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
View(batch)
results <- trace.update(letter, network$input.hidden.weights, network$trace.hidden, network$hidden.bias.weights, network$hidden.output.weights, network$trace.output, network$output.bias.weights)
for(b in 1:(length(word)/n.input)){
# get input vector
letter <- word[,b]
letter <- noise.in.letter(letter)
# update network properties
results <- trace.update(letter, network$input.hidden.weights, network$trace.hidden, network$hidden.bias.weights, network$hidden.output.weights, network$trace.output, network$output.bias.weights)
network$input.hidden.weights <- results$input.hidden.weights
network$trace.hidden <- results$trace.hidden
network$hidden.bias.weights <- results$hidden.bias.weights
network$trace.output <- results$trace.output
network$output.bias.weights <- results$output.bias.weights
network$hidden.output.weights <- results$hidden.output.weights
}
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
=======
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, s)
network <- results$network
temp.layer.many.activations(network, words)
visualize.output.act.match()
visualize.output.act.match()
n <- temp.layer.many.activations(network, words)
n[1,]
n[27,]
n[22,]
n[48,]
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
#install.packages('ggplot2')
library(ggplot2)
>>>>>>> origin/master
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
<<<<<<< HEAD
=======
trace.param.output <- 0.75 #0.75
>>>>>>> origin/master
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
<<<<<<< HEAD
=======
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
=======
>>>>>>> origin/master
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
for(b in 1:(length(word)/n.input)){
# get input vector
letter <- word[,b]
letter <- noise.in.letter(letter)
# update network properties
results <- trace.update(letter, network$input.hidden.weights, network$trace.hidden, network$hidden.bias.weights, network$hidden.output.weights, network$trace.output, network$output.bias.weights)
network$input.hidden.weights <- results$input.hidden.weights
network$trace.hidden <- results$trace.hidden
network$hidden.bias.weights <- results$hidden.bias.weights
network$trace.output <- results$trace.output
network$output.bias.weights <- results$output.bias.weights
network$hidden.output.weights <- results$hidden.output.weights
}
for(i in 1:n.epochs){
word <- words[[sample(1:n.words,1, replace = T)]]
if(i == 2 || i %% 100 == 0){
history$learning.curve[i / 100,] <- learning.measure(network$input.hidden.weights)
history$bias.tracker[i / 100,] <- as.vector(network$hidden.bias.weights)
history$output.bias.tracker[i / 100,] <- as.vector(network$output.bias.weights)
history$hidden.letter.similarity.tracking[i / 100, ] <- batch.hidden.layer.learning(letters, network)$similarity
history$hidden.stability[ i / 100, ] <- batch.hidden.layer.stability(letters, network, history)
history$hidden.stability.tracking <- update.hidden.layer.stability(letters, network)
history$output.match.tracker[i / 100] <- test.word.continuity(network, words)
}
for(b in 1:(length(word)/n.input)){
# get input vector
letter <- word[,b]
letter <- noise.in.letter(letter)
# update network properties
results <- trace.update(letter, network$input.hidden.weights, network$trace.hidden, network$hidden.bias.weights, network$hidden.output.weights, network$trace.output, network$output.bias.weights)
network$input.hidden.weights <- results$input.hidden.weights
network$trace.hidden <- results$trace.hidden
network$hidden.bias.weights <- results$hidden.bias.weights
network$trace.output <- results$trace.output
network$output.bias.weights <- results$output.bias.weights
network$hidden.output.weights <- results$hidden.output.weights
}
# update learning history
#history$hidden.win.tracker[i,] <- results$hidden
setTxtProgressBar(pb, i)
}
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=1), nrow=n.input, ncol=n.hidden)
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=1), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
if(is.na(network)){
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
for(h in 1:(sparseness.percent*(n.input * n.hidden))){ ## work on correct implementation of sparseness with split network
network[[1]][sample(1:n.input, 1, replace=TRUE), sample(1:n.hidden, 1, replace=TRUE)] <- NA
}
for(g in 1:(sparseness.percent*(n.hidden*n.output))){
network[[3]][sample(1:n.hidden, 1, replace=TRUE), sample(1:n.output, 1, replace=TRUE)] <- NA
}
}
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
hidden <- numeric(n.hidden)
<<<<<<< HEAD
for(i in 1:n.hidden){
hidden[i] <- sigmoid.activation(sum(na.omit(input * input.hidden.weights[,i]) + hidden.bias.weights[i,1]))
}
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=1), nrow=n.input, ncol=n.hidden)
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=1), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
if(is.na(network)){
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
for(h in 1:(sparseness.percent*(n.input * n.hidden))){ ## work on correct implementation of sparseness with split network
network[[1]][sample(1:n.input, 1, replace=TRUE), sample(1:n.hidden, 1, replace=TRUE)] <- NA
}
for(g in 1:(sparseness.percent*(n.hidden*n.output))){
network[[3]][sample(1:n.hidden, 1, replace=TRUE), sample(1:n.output, 1, replace=TRUE)] <- NA
}
}
network <- results$network
=======
>>>>>>> b3102444d605f6c298c8a8eaa56bd610efa20724
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
>>>>>>> origin/master
