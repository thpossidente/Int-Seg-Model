n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.03))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.03))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,17], type='b', ylim=c(0,0.03))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,18], type='b', ylim=c(0,0.03))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,18], type='b', ylim=c(0,0.03))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.03))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.05))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,11], type='b', ylim=c(0,0.05))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,11], type='b', ylim=c(0,0.05))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.05))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.05 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.05))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.5))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,11], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,12], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,13], type='b', ylim=c(0,0.5))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,19], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,20], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,21], type='b', ylim=c(0,0.5))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,22], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,23], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,24], type='b', ylim=c(0,0.5))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.5 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,24], type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,11], type='b', ylim=c(0,1))
results$history$output.bias.tracker
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- (1/30) #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,11], type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,1))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,21], type='b', ylim=c(0,1))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,1))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.output.act.match()
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.00005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
results$history$output.bias.tracker
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,1))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.1))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.01))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.001))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.001))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.001))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.001))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.003))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.004))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.004))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.004))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.00009 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
display.learning.curves(results)
visualize.letter.activations(results$network, j)
visualize.output.act.match()
temp.layer.activations.many <- temp.layer.many.activations(network, words)
output.trace.tracker.results <- results$history$output.trace.tracker
temp.layer.activations.many[27,]
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,30], type = "b")
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.004))
test.word.continuity1(results$network, words)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,24], type='b', ylim=c(0,0.004))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,21], type='b', ylim=c(0,0.004))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,21], type='b', ylim=c(0,0.004))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.004))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.004))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.004))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0009 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.004))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.01))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.1))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.09))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.05))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.01))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.02))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.02))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0007 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,19], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,2], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,3], type='b', ylim=c(0,0.02))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0006 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
