letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,5))
mutual.info.output()
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("word", "output")
View(output.results)
counter5 <- 1
for(z in 1:26){
output.results[z,1] <- counter
if(z %% 3 == 0){
counter5 <- counter + 1
}
}
mutual.info <- 0
for(w in 1:26){
prob.word <- sum(output.results[,1] == output.results[w,1]) / length(output.results[,1])
prob.act <- sum(output.results[,2] == output.results[w,2]) / length(output.results[,2])
joint.prob <- sum((output.results[,1] == output.results[w,1]) & (output.results[,2] == output.results[w,2])) / length(output.results[,1])
mutual.info = mutual.info + (joint.prob * (log2((joint.prob)/(prob.act*prob.word))))
}
View(output.results)
counter5 <- 1
for(z in 1:26){
output.results[z,1] <- counter
if(z %% 3 == 0){
counter5 <- counter + 1
}
}
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("word", "output")
counter5 <- 1
for(z in 1:26){
output.results[z,1] <- counter
if(z %% 3 == 0){
counter5 <- counter + 1
}
}
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,6], type='b', ylim=c(0,0.02))
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("word", "output")
View(output.results)
network <- results$network
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("word", "output")
counter5 <- 1
for(z in 1:26){
output.results[z,1] <- counter
if(z %% 3 == 0){
counter5 <- counter5 + 1
}
}
View(output.results)
counter5 <- 1
counter5 <- 1
for(z in 1:26){
output.results[z,1] <- counter
if(z %% 3 = 0){
counter5 <- counter5 + 1
}
}
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("word", "output")
counter5 <- 1
for(z in 1:26){
output.results[z,1] <- counter5
if(z %% 3 == 0){
counter5 <- counter5 + 1
}
}
View(output.results)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
counter <- 1    #change to start what batch 2nd layer starts learning
counter.bias <- 5001 #change to start what batch output bias node starts at
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=1), nrow=n.input, ncol=n.hidden)
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=1), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
network <- NA
if(is.na(network)){
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
## work on correct implementation of sparseness with split network
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
}
history <- list(               #initializes learning data matrices
learning.curve = matrix(0, nrow = n.epochs/100, ncol = n.hidden),
output.match.tracker = rep(0, times = n.epochs/100),
hidden.letter.similarity.tracking = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
output.trace.tracker = matrix(0, nrow = n.epochs/100, ncol = n.output),
output.bias.tracker = matrix(0, nrow=n.epochs/100, ncol = n.output),
output.act.unique.tracker <- rep(0, times=n.epochs/100),
mutual.info.tracker <- rep(0, times = n.epochs/100)
)
pb <- txtProgressBar(min=1, max=n.epochs,style=3)
for(i in 1:n.epochs){
counter = counter + 1
counter.bias = counter.bias + 1
word <- words[[sample(1:n.words,1, replace = T)]]
if(i == 2 || i %% 100 == 0){
history$learning.curve[i / 100,] <- learningMeasure(network$input.hidden.weights, n.hidden, alphabet)
history$hidden.letter.similarity.tracking[i / 100, ] <- batch.hidden.layer.learning(letters, network)$similarity
history$output.match.tracker[i / 100] <- test.word.continuity(network, words)
history$output.trace.tracker[i / 100, ] <- network$trace.output
history$output.bias.tracker[i / 100, ] <- network$output.bias.weights[,1]
history$output.act.unique.tracker[i / 100] <- output.act.unique(network, words)
history$mutual.info.tracker[i /100] <- mutual.info.output()
}
for(b in 1:(length(word)/n.input)){
# get input vector
input <- word[,b]
input <- noiseInLetter(input, n.input, letter.noise.param, n.epochs)
# update network properties
results <- traceUpdate(trace.param.hidden, trace.param.output,
learning.rate.hidden, learning.rate.output,
output.bias.param.plus, output.bias.param.minus,
hidden.bias.param.minus, hidden.bias.param.plus,
percent.act.input, percent.act.output,
n.output, n.hidden,
input, network$input.hidden.weights,
network$trace.hidden, network$hidden.bias.weights,
network$hidden.output.weights, network$trace.output,
network$output.bias.weights, counter, counter.bias)
network$input.hidden.weights <- results$inputToHiddenWeights
network$trace.hidden <- results$traceHidden
network$hidden.bias.weights <- results$hiddenBiasWeights
network$trace.output <- results$traceOutput
network$output.bias.weights <- results$outputBiasWeights
network$hidden.output.weights <- results$hiddenToOutputWeights
}
setTxtProgressBar(pb, i)
}
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,6], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,8))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.25 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
