<<<<<<< HEAD
=======
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
trace.param.output <- 0.7 #0.8
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
<<<<<<< HEAD
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
visualize.hidden.layer.learning(results$history)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
results$network$hidden.output.weights
results$network$hidden.output.weights[,1]
sum(results$network$hidden.output.weights[,1])
sum(results$network$hidden.output.weights[,1], na.rm=T)
=======
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
>>>>>>> origin/master
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.9 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.00005 #0
=======
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.005))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.005))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.005))
plot(x=seq(from=100, to=15000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.005))
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.9 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0.5 #0
output.bias.param.plus <- 0.00005 #0
=======
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0.05 #0
output.bias.param.plus <- 0.00005 #0
=======
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
visualize.hidden.layer.learning(results$history)
=======
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
<<<<<<< HEAD
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.005 #0
output.bias.param.plus <- 0.00005 #0
=======
trace.param.output <- 0.8 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
log2(3/26)
log2(3/26)*(1/26)*8
(log2(3/26)*(1/26)*8)+log2(2/26)*(1/26)
log2(26/3)*(1/26)
(log2(26/3)*(1/26)*8) + log2(26/2)*(1/26)
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 100  #10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8 #0.8
hidden.bias.param.minus <- 0.05
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
delay.param = 3  # number of hidden activations saved and used as input along with current input into output layer
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
test <- plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
test
test
test <- plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
test
test <- plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
restults$history$mutual.info.tracker
results$history$mutual.info.tracker
counter = 1
res = matrix(0, nrow=50, ncol = 2)
for(i in seq(0.96, 1, 0.02)){
counter = counter + 1
trace.param.output = i
res[counter, 1] = i
results <- batch(n.epochs) #run training batches
res[counter, 2] = mean(results$history$mutual.info.tracker[65:75])
}
res
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 7500  #10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8 #0.8
hidden.bias.param.minus <- 0.05
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
delay.param = 3  # number of hidden activations saved and used as input along with current input into output layer
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
counter = 1
res = matrix(0, nrow=100, ncol = 2)
for(i in seq(0.1, 1, 0.02)){
trace.param.output = i
res[counter, 1] = i
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
one <- matrix(data = c(1,0,1,0), nrow = 2, ncol = 2)
View(one)
one <- matrix(data = c(1,0,0,1), nrow = 2, ncol = 2)
two <- matrix((data = c(0,1,1,0), nrow = 2, ncol = 2)
}
two <- matrix(data = c(0,1,1,0), nrow = 2, ncol = 2)
View(two)
install.packages(combinat)
install.packages("combinat")
library("combinat")
permn(c(one,one,two,two))
permn(c(one,one,two,two))
permn(one,one,two,two)
permn(letters[1:3])
permn(1,2,3,)
permn(1,2,3)
permn(a,b)
permutations <- function( x, prefix = c() )
{
if(length(x) == 0 ) return(prefix)
do.call(rbind, sapply(1:length(x), FUN = function(idx) permutations( x[-idx], c( prefix, x[idx])), simplify = FALSE))
}
permutations("a", "b", "c")
permutations(c("a", "b", "c"))
permutations(c(one, one, two, two))
permutations(c(1,1,2,2))
configs <- permutations(c(1,1,2,2))
unique(configs)
configs <- numeric(2^num.blocks)
configs <- numeric(2^num.blocks)
while()
num.blocks <- 4
configs <- numeric(2^num.blocks)
test <- list(1,1,1,1,2)
2 %in% test
3 %in% test
test <- list(1)
list.append(test, 2)
install.packages("rlist")
library("rlist")
test <- list(1)
list.append(test, 2)
test <- list.append(test, 2)
grid.inputs <- function(num.blocks){
install.packages("rlist")
library("rlist")
one <- matrix(data = c(1,0,0,1), nrow = 2, ncol = 2)
two <- matrix(data = c(0,1,1,0), nrow = 2, ncol = 2)
count <- 1
configs <- list()
while(length(configs) == 2^num.blocks){
try <- sample(c(one,two), num.blocks, replace = F)
if(try %in% configs == F){
configs <- list.append(configs, try)
}
}
}
grid.inputs <- function(num.blocks){
install.packages("rlist")
library("rlist")
one <- matrix(data = c(1,0,0,1), nrow = 2, ncol = 2)
two <- matrix(data = c(0,1,1,0), nrow = 2, ncol = 2)
count <- 1
configs <- list()
while(length(configs) == 2^num.blocks){
try <- sample(c(one,two), num.blocks, replace = F)
if(try %in% configs == F){
configs <- list.append(configs, try)
}
}
}
grid.inputs(4)
grid.inputs <- function(num.blocks){
one <- matrix(data = c(1,0,0,1), nrow = 2, ncol = 2)
two <- matrix(data = c(0,1,1,0), nrow = 2, ncol = 2)
count <- 1
configs <- list()
while(length(configs) == 2^num.blocks){
try <- sample(c(one,two), num.blocks, replace = F)
if(try %in% configs == F){
configs <- list.append(configs, try)
}
}
}
grid.inputs(4)
grid.inputs <- function(num.blocks){
one <- matrix(data = c(1,0,0,1), nrow = 2, ncol = 2)
two <- matrix(data = c(0,1,1,0), nrow = 2, ncol = 2)
count <- 1
configs <- list()
while(length(configs) == 2^num.blocks){
try <- sample(c(one,two), num.blocks, replace = F)
if(try %in% configs == F){
configs <- list.append(configs, try)
}
}
return(configs)
}
grid.inputs(4)
grid.inputs <- function(num.blocks){
one <- matrix(data = c(1,0,0,1), nrow = 2, ncol = 2)
two <- matrix(data = c(0,1,1,0), nrow = 2, ncol = 2)
count <- 1
configs <- list()
while(length(configs) < 2^num.blocks){
try <- sample(c(one,two), num.blocks, replace = F)
if(try %in% configs == F){
configs <- list.append(configs, try)
}
}
return(configs)
}
grid.inputs(4)
warnings()
try <- sample(c(one,two), num.blocks, replace = F)
one <- matrix(data = c(1,0,0,1), nrow = 2, ncol = 2)
two <- matrix(data = c(0,1,1,0), nrow = 2, ncol = 2)
try <- sample(c(one,two), num.blocks, replace = F)
num.blocks <- 4
try <- sample(c(one,two), num.blocks, replace = F)
list(1,2,1,2)
test <- list(1,1,1,2)
test <- list(c(1,2),c(1,1))
c(1,2) %in% test
C(1,2) == test
test == c(1,2)
expand.grid(rep(list(1:5), 2))
expand.grid(rep(list(1:2), 2))
expand.grid(rep(list(1:2), 4))
expand.grid(rep(list(one, two), 4))
expand.grid(rep(list(1:2), 4))
test <- expand.grid(rep(list(1:2), 4))
View(test)
configs <- expand.grid(rep(list(1:2), 4))
configs <- expand.grid(rep(list("one", 'two'), 4))
View(configs)
configs <- expand.grid(rep(c("one", 'two'), 4))
View(configs)
list(1:2)
View(configs)
configs <- expand.grid(rep(list(1:2), 4))
View(configs)
configs <- expand.grid(rep(list(c(one,two)), 4))
configs <- expand.grid(rep(list(1:2), 4))
configs <- expand.grid(rep(list(1:2), 9))
configs <- expand.grid(rep(list(1:2), num.blocks))
configs <- expand.grid(rep(list(c("a", "b")), num.blocks))
list(c("a", "b")
Q
list(c("a", "b")
Q
list(c("a", "b"))
configs <- expand.grid(rep(list(c("a", "b")), num.blocks))
View(configs)
(list(c(one, two))
!
list(c(one, two))
configs <- expand.grid(rep(list(1:2), num.blocks))
configs[1,1]
configs[1,2]
configs.done <- list()
for(i in 1:length(configs)){
for(k in 1:num.blocks){
if(configs[i,k] == 1){
block <- one
} else{block <- two}
configs.done[i][k] <- block
}
}
warnings()
configs <- expand.grid(rep(list(1:2), 9))
View(configs)
list(1:2)
test <- as.matrix(c(1,0,0,1))
View(test)
one <- c(1,0,0,1)
two <- c(0,1,1,0)
num.blocks
configs <- expand.grid(rep(list(1:2), num.blocks))
length(configs)
length(configs[,1])
configs.done <- list()
for(i in 1:length(configs[,1])){
for(k in 1:num.blocks){
if(configs[i,k] == 1){
block <- one
}else{block <- two}
configs.done[i] <- c(configs.done[i], block)
}
}
warnings()
configs.done[i] <- list.append(configs.done[i], block)
one <- c(1,0,0,1)
two <- c(0,1,1,0)
configs <- expand.grid(rep(list(1:2), num.blocks))
configs.done <- list()
configs.done <- list()
for(i in 1:length(configs[,1])){
for(k in 1:num.blocks){
if(configs[i,k] == 1){
block <- one
}else{block <- two}
configs.done[i] <- list.append(configs.done[i], block)
}
}
warnings()
configs.done <- list()
configs.done <- list()
for(i in 1:length(configs[,1])){
for(k in 1:num.blocks){
if(configs[i,k] == 1){
block <- one
}else{block <- two}
configs.done[i] <- block
}
}
warnings()
test <- list()
test[1] <- c(1,0)
test[1] <- "hamster"
test
test <- list()
test[1][1:2] <- c(1,0)
test[1]
test[1][1]
test[[1]]
test[[1]] <- c(1,0)
one <- c(1,0,0,1)
two <- c(0,1,1,0)
configs <- expand.grid(rep(list(1:2), num.blocks))
configs.done <- list()
for(i in 1:length(configs[,1])){
for(k in 1:num.blocks){
if(configs[i,k] == 1){
block <- one
}else{block <- two}
configs.done[[i]] <- block
}
}
one <- c(1,0,0,1)
two <- c(0,1,1,0)
configs <- expand.grid(rep(list(1:2), num.blocks))
configs.done <- list()
for(i in 1:length(configs[,1])){
for(k in 1:num.blocks){
if(configs[i,k] == 1){
block <- one
}else{block <- two}
configs.done[[i]] <- c(configs.done[[i]], block)
}
}
configs.done <- list(rep(0,length(configs[,1])))
test <- numeric(1)
test
test <- numeric(1,1)
numeric(1)
numeric(c(1,1))
one <- c(1,0,0,1)
two <- c(0,1,1,0)
configs <- expand.grid(rep(list(1:2), num.blocks))
configs.done <- numeric(length(configs[,1]))
for(i in 1:length(configs[,1])){
for(k in 1:num.blocks){
if(configs[i,k] == 1){
block <- one
}else{block <- two}
configs.done[i] <- c(configs[i], block)
}
}
one <- c(1,0,0,1)
two <- c(0,1,1,0)
configs <- expand.grid(rep(list(1:2), num.blocks))
configs.done <- numeric(length(configs[,1]))
for(i in 1:length(configs[,1])){
for(k in 1:num.blocks){
if(configs[i,k] == 1){
block <- one
}else{block <- two}
configs.done[i] <- c(configs.done[i], block)
}
}
warnings()
configs.done <- array()
for(i in 1:length(configs[,1])){
for(k in 1:num.blocks){
if(configs[i,k] == 1){
block <- one
}else{block <- two}
configs.done[i] <- c(configs.done[i], block)
}
}
=======
res[counter, 2] = mean(results$history$mutual.info.tracker[65:75])
counter = counter + 1
}
res
>>>>>>> origin/master
