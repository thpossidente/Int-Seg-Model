}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
if(is.na(network)){
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
for(h in 1:(sparseness.percent*(n.input * n.hidden))){ ## work on correct implementation of sparseness with split network
network[[1]][sample(1:n.input, 1, replace=TRUE), sample(1:n.hidden, 1, replace=TRUE)] <- NA
}
for(g in 1:(sparseness.percent*(n.hidden*n.output))){
network[[3]][sample(1:n.hidden, 1, replace=TRUE), sample(1:n.output, 1, replace=TRUE)] <- NA
}
}
# tracking learning #
history <- list(
learning.curve = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
bias.tracker = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
output.bias.tracker = matrix(0, nrow = n.epochs/100, ncol= n.output),
hidden.win.tracker = matrix(0, nrow=n.epochs, ncol= n.hidden),
hidden.letter.similarity.tracking = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
hidden.stability = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
hidden.stability.tracking = update.hidden.layer.stability(letters, network)
)
pb <- txtProgressBar(min=1, max=n.epochs,style=3)
for(i in 1:n.epochs){
word <- words[[sample(1:n.words,1, replace = T)]]
if(i %% 100 == 0){
history$learning.curve[i / 100,] <- learning.measure(network$input.hidden.weights)
history$bias.tracker[i / 100,] <- as.vector(network$hidden.bias.weights)
history$output.bias.tracker[i / 100,] <- as.vector(network$output.bias.weights)
history$hidden.letter.similarity.tracking[i / 100, ] <- batch.hidden.layer.learning(letters, network)$similarity
history$hidden.stability[ i / 100, ] <- batch.hidden.layer.stability(letters, network, history)
history$hidden.stability.tracking <- update.hidden.layer.stability(letters, network)
}
for(b in 1:(length(word)/n.input)){
# get input vector
letter <- word[,b]
letter <- noise.in.letter(letter)
# update network properties
results <- trace.update(letter, network$input.hidden.weights, network$trace.hidden, network$hidden.bias.weights, network$hidden.output.weights, network$trace.output, network$output.bias.weights)
network$input.hidden.weights <- results$input.hidden.weights
network$hidden.output.weights <- results$hidden.output.weights
network$trace.hidden <- results$trace.hidden
network$hidden.bias.weights <- results$hidden.bias.weights
network$hidden.output.weights <- results$hidden.output.weights
network$trace.output <- results$trace.output
network$output.bias.weights <- results$output.bias.weights
network$hidden.bias.weights <- results$hidden.bias.weights
# update learning history
history$hidden.win.tracker[i,] <- results$hidden
setTxtProgressBar(pb, i)
}
}
test.word.continuity(network, words)
return(list(
history=history,
network=network
))
}
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.0
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inp
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.0
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.letter.activations(results$network, u)
visualize.letter.activations(results$network, x)
visualize.letter.activations(results$network, z)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, d)
visualize.letter.activations(results$network, e)
visualize.letter.activations(results$network, f)
visualize.letter.activations(results$network, g)
visualize.letter.activations(results$network, k)
visualize.letter.activations(results$network, m)
visualize.letter.activations(results$network, n)
visualize.letter.activations(results$network, p)
visualize.letter.activations(results$network,q)
visualize.letter.activations(results$network, u)
visualize.letter.activations(results$network, v)
results$history$hidden.stability
#install.packages('png')
library('png')
#install.packages('abind')
library('abind')
alphabet <- list(
a <- as.vector(t(1-adrop(readPNG('AlphabetPNG/A.png')[,,1,drop=F], drop=3))),
b <- as.vector(t(1-adrop(readPNG('AlphabetPNG/B.png')[,,1,drop=F], drop=3))),
c <- as.vector(t(1-adrop(readPNG('AlphabetPNG/C.png')[,,1,drop=F], drop=3))),
d <- as.vector(t(1-adrop(readPNG('AlphabetPNG/D.png')[,,1,drop=F], drop=3))),
e <- as.vector(t(1-adrop(readPNG('AlphabetPNG/E.png')[,,1,drop=F], drop=3))),
f <- as.vector(t(1-adrop(readPNG('AlphabetPNG/F.png')[,,1,drop=F], drop=3))),
g <- as.vector(t(1-adrop(readPNG('AlphabetPNG/G.png')[,,1,drop=F], drop=3))),
h <- as.vector(t(1-adrop(readPNG('AlphabetPNG/H.png')[,,1,drop=F], drop=3))),
i <- as.vector(t(1-adrop(readPNG('AlphabetPNG/I.png')[,,1,drop=F], drop=3))),
j <- as.vector(t(1-adrop(readPNG('AlphabetPNG/J.png')[,,1,drop=F], drop=3))),
k <- as.vector(t(1-adrop(readPNG('AlphabetPNG/K.png')[,,1,drop=F], drop=3))),
l <- as.vector(t(1-adrop(readPNG('AlphabetPNG/L.png')[,,1,drop=F], drop=3))),
m <- as.vector(t(1-adrop(readPNG('AlphabetPNG/M.png')[,,1,drop=F], drop=3))),
n <- as.vector(t(1-adrop(readPNG('AlphabetPNG/N.png')[,,1,drop=F], drop=3))),
o <- as.vector(t(1-adrop(readPNG('AlphabetPNG/O.png')[,,1,drop=F], drop=3))),
p <- as.vector(t(1-adrop(readPNG('AlphabetPNG/P.png')[,,1,drop=F], drop=3))),
q <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Q.png')[,,1,drop=F], drop=3))),
r <- as.vector(t(1-adrop(readPNG('AlphabetPNG/R.png')[,,1,drop=F], drop=3))),
s <- as.vector(t(1-adrop(readPNG('AlphabetPNG/S.png')[,,1,drop=F], drop=3))),
t <- as.vector(t(1-adrop(readPNG('AlphabetPNG/T.png')[,,1,drop=F], drop=3))),
u <- as.vector(t(1-adrop(readPNG('AlphabetPNG/U.png')[,,1,drop=F], drop=3))),
v <- as.vector(t(1-adrop(readPNG('AlphabetPNG/V.png')[,,1,drop=F], drop=3))),
w <- as.vector(t(1-adrop(readPNG('AlphabetPNG/W.png')[,,1,drop=F], drop=3))),
x <- as.vector(t(1-adrop(readPNG('AlphabetPNG/X.png')[,,1,drop=F], drop=3))),
y <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Y.png')[,,1,drop=F], drop=3))),
z <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Z.png')[,,1,drop=F], drop=3)))
)
letters <- list(
"A" = a,
"B" = b,
"C" = c,
"D" = d,
"E" = e,
"F" = f,
"G" = g,
"H" = h,
"I" = i,
"J" = j,
"K" = k,
"L" = l,
"M" = m,
"N" = n,
"O" = o,
"P" = p,
"Q" = q,
"R" = r,
"S" = s,
"T" = t,
"U" = u,
"V" = v,
"W" = w,
"X" = x,
"Y" = y,
"Z" = z
)
words <- list(
abc <- cbind(a,b,c),
def <- cbind(d,e,f),
ghi <- cbind(g,h,i),
jkl <- cbind(j,k,l),
mno <- cbind(m,n,o),
pqr <- cbind(p,q,r),
stu <- cbind(s,t,u),
vwx <- cbind(v,w,x),
yz <- cbind(y,z)
)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.0
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training bat
n.output <- 30
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.8
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.letter.activations(results$network, v)
visualize.letter.activations(results$network, c)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.3
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #ru
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, d)
visualize.letter.activations(results$network, e)
visualize.letter.activations(results$network, f)
visualize.letter.activations(results$network, g)
visualize.letter.activations(results$network, h)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, i)
visualize.letter.activations(results$network, j)
visualize.letter.activations(results$network, k)
visualize.letter.activations(results$network, l)
visualize.letter.activations(results$network, m)
visualize.letter.activations(results$network, v)
visualize.letter.activations(results$network, n)
visualize.letter.activations(results$network, o)
visualize.letter.activations(results$network, p)
visualize.letter.activations(results$network, r)
visualize.letter.activations(results$network, b)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.1
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, v)
visualize.letter.activations(results$network, m)
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, c)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.1
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, d)
visualize.letter.activations(results$network, e)
visualize.letter.activations(results$network, l)
test.word.continuity(results$network, words)
results$history$hidden.stability
results$history$hidden.stability
results$history$hidden.stability[30:50,]
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.9
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.letter.activations(results$network, l)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, d)
visualize.letter.activations(results$network, e)
visualize.letter.activations(results$network, f)
visualize.letter.activations(results$network, g)
visualize.letter.activations(results$network, h)
visualize.letter.activations(results$network, i)
visualize.letter.activations(results$network, t)
visualize.letter.activations(results$network, k)
visualize.letter.activations(results$network, j)
visualize.letter.activations(results$network, m)
visualize.letter.activations(results$network, v)
visualize.letter.activations(results$network, n)
visualize.letter.activations(results$network, h)
visualize.letter.activations(results$network, o)
visualize.letter.activations(results$network, p)
visualize.letter.activations(results$network, r)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, q)
results$history$hidden.stability[30:50,]
test.word.continuity(results$network, words)
results$history$hidden.stability[30:50,]
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.01
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.01
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.01
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.letter.activations(results$network, q)
visualize.letter.activations(results$network, r)
visualize.letter.activations(results$network, p)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, d)
visualize.letter.activations(results$network, n)
visualize.letter.activations(results$network, h)
visualize.letter.activations(results$network, n)
visualize.letter.activations(results$network, f)
visualize.letter.activations(results$network, e)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, g)
visualize.letter.activations(results$network, i)
visualize.letter.activations(results$network, t)
visualize.letter.activations(results$network, j)
visualize.letter.activations(results$network, k)
visualize.letter.activations(results$network, l)
visualize.letter.activations(results$network, m)
visualize.letter.activations(results$network, v)
visualize.letter.activations(results$network, u)
