n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=0.5), nrow=n.input, ncol=n.hidden)   # Random normalization
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=0.5), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
if(is.na(network)){
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output))
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
} else{
network$hidden.bias.weights <- matrix(0, nrow = n.hidden, ncol = 1)
network$output.bias.weights <- matrix(0, nrow = n.output, ncol = 1)
}
network = NA
if(is.na(network)){
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output))
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
} else{
network$hidden.bias.weights <- matrix(0, nrow = n.hidden, ncol = 1)
network$output.bias.weights <- matrix(0, nrow = n.output, ncol = 1)
}
history <- list(               #initializes learning data matrices
mutual.info.spatial.track <- rep(0, times = n.epochs/100),
learning.curve = matrix(0, nrow = n.epochs/100, ncol = n.hidden),
hidden.letter.similarity.tracking = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
output.trace.tracker = matrix(0, nrow = n.epochs, ncol = n.output),
output.bias.tracker = matrix(0, nrow=n.epochs/100, ncol = n.output),
output.act.unique.tracker <- rep(0, times=n.epochs/100)
#mutual.info.tracker <- rep(0, times = n.epochs/100)
)
mutual.info.spatial.track(network)
mutual.info.spatial(network)
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.spatial.track , type = 'b', ylim=c(0,5), xlab = "Epochs", ylab = "Mutual Information")
test.word.continuity1(results$network, words)
results$network$hidden.output.weights
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0.02 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.spatial.track , type = 'b', ylim=c(0,5), xlab = "Epochs", ylab = "Mutual Information")
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,16], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,5], type='b', ylim=c(0,0.02))
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0.02 #0
output.bias.param.plus <- 0.005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.spatial.track , type = 'b', ylim=c(0,5), xlab = "Epochs", ylab = "Mutual Information")
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,5], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,5], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,6], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,19], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,19], type='b', ylim=c(0,0.5))
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,19], type='b', ylim=c(0,1))
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,19], type='b', ylim=c(0,5))
network = results$network
input.mat <- matrix(0, ncol=n.input, nrow=26*10)
r <- 1
for(i in 1:26){
for(j in 1:10){
input.mat[r,] <- letters[[i]]
r <- r + 1
}
}
storing.acts <- matrix(0, nrow=nrow(input.mat), ncol=n.output)
for(i in 1:nrow(input.mat)){
act.res <- forwardPass(n.output, percent.act.input,
percent.act.output, n.hidden,
input.mat[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.acts[i,] <- act.res$output
}
output.res <- data.frame(letter=numeric(),output=numeric())
output.res
storing.acts
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,5), xlab = "Epochs", ylab = "Mutual Information")
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.spatial.track , type = 'b', ylim=c(0,5), xlab = "Epochs", ylab = "Mutual Information")
network = NA
delay = 1
counter <- 4500 #change to start what batch 2nd layer starts learning (start at 1 will have layer start learning after 5000 epochs)
counter.bias <- 5000 #change to start what batch output bias node starts at
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=0.5), nrow=n.input, ncol=n.hidden)   # Random normalization
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=0.5), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
if(is.na(network)){
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output))
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
} else{
network$hidden.bias.weights <- matrix(0, nrow = n.hidden, ncol = 1)
network$output.bias.weights <- matrix(0, nrow = n.output, ncol = 1)
}
history <- list(               #initializes learning data matrices
mutual.info.spatial.track <- rep(0, times = n.epochs/100),
learning.curve = matrix(0, nrow = n.epochs/100, ncol = n.hidden),
hidden.letter.similarity.tracking = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
output.trace.tracker = matrix(0, nrow = n.epochs, ncol = n.output),
output.bias.tracker = matrix(0, nrow=n.epochs/100, ncol = n.output),
output.act.unique.tracker <- rep(0, times=n.epochs/100)
#mutual.info.tracker <- rep(0, times = n.epochs/100)
)
mutual.info.spatial(network)
sum(noiseInLetter(letters[[1]]) != letters[[1]]
)
sum(noiseInLetter(letters[[1]], n.inputs, letter.noise.param) != letters[[1]])
sum(noiseInLetter(letters[[1]], n.input, letter.noise.param) != letters[[1]])
c(1:n.input)
rand <- sample(c(1:n.input), 1, TRUE)
rand
rand <- sample(c(1:n.input), 1, TRUE)
rand
noise.in.letter <- function(input){
for(i in 1:letter.noise.param*n.input){
rand <- sample(c(1:n.input), 1, TRUE)
if(input[rand] == 1){
input[rand] = 0
} else{
input[rand] = 1
}
}
}
sum(noise.in.letter(letters[[1]]) != letters[[1]])
letter.noise.param*n.input
rand
letters[[1]][58]
input <- letters[[1]]
rand <- sample(c(1:n.input), 1, TRUE)
input[rand]
sum(noiseInLetter(letters[[1]], n.input, letter.noise.param) != letters[[1]])
sum(noiseInLetter(letters[[1]], n.input, letter.noise.param) != letters[[1]], na.rm = T)
n.input
letter.noise.param
letters[[1]]
sum(noiseInLetter(letters[[1]], n.input, letter.noise.param) != letters[[1]], na.rm = T)
noise.in.letter(letters[[1]])
noise.in.letter(letters[[1]])
noise.in.letter <- function(input){
for(i in 1:letter.noise.param*n.input){
rand <- sample(c(1:n.input), 1, TRUE)
if(input[rand] > 0.5){
input[rand] = 0
} else{
input[rand] = 1
}
}
return(input)
}
noise.in.letter(letters[[1]])
sum(noise.in.letter(letters[[1]]) != letters[[1]])
sum(noise.in.letter(letters[[1]]) != letters[[1]])
1:letter.noise.param*n.input
1:n.hidden
1:(letter.noise.param*n.input)
noise.in.letter <- function(input){
for(i in 1:(letter.noise.param*n.input)){
rand <- sample(c(1:n.input), 1, TRUE)
if(input[rand] > 0.5){
input[rand] = 0
} else{
input[rand] = 1
}
}
return(input)
}
sum(noise.in.letter(letters[[1]]) != letters[[1]])
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
noiseInLetter(letters[[1]], n.input, letter.noise.param)
sum(noiseInLetter(letters[[1]], n.input, letter.noise.param) != letters[[1]])
sum(noiseInLetter(letters[[1]], n.input, letter.noise.param) != letters[[1]])
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,5), xlab = "Epochs", ylab = "Mutual Information")
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$mutual.info.spatial.track , type = 'b', ylim=c(0,5), xlab = "Epochs", ylab = "Mutual Information")
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=n.epochs, by=100), y=results$history$output.bias.tracker[,19], type='b', ylim=c(0,5))
