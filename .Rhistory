hidden.letter.similarity.tracking = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
hidden.stability = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
hidden.stability.tracking = update.hidden.layer.stability(letters, network)
)
pb <- txtProgressBar(min=1, max=n.epochs,style=3)
for(i in 1:n.epochs){
word <- words[[sample(1:n.words,1, replace = T)]]
if(i == 2 || i %% 100 == 0){
history$learning.curve[i / 100,] <- learning.measure(network$input.hidden.weights)
history$bias.tracker[i / 100,] <- as.vector(network$hidden.bias.weights)
history$output.bias.tracker[i / 100,] <- as.vector(network$output.bias.weights)
history$hidden.letter.similarity.tracking[i / 100, ] <- batch.hidden.layer.learning(letters, network)$similarity
history$hidden.stability[ i / 100, ] <- batch.hidden.layer.stability(letters, network, history)
history$hidden.stability.tracking <- update.hidden.layer.stability(letters, network)
history$output.match.tracker[i / 100] <- test.word.continuity(network, words)
}
for(b in 1:(length(word)/n.input)){
# get input vector
letter <- word[,b]
letter <- noise.in.letter(letter)
# update network properties
results <- trace.update(letter, network$input.hidden.weights, network$trace.hidden, network$hidden.bias.weights, network$hidden.output.weights, network$trace.output, network$output.bias.weights)
network$input.hidden.weights <- results$input.hidden.weights
network$trace.hidden <- results$trace.hidden
network$hidden.bias.weights <- results$hidden.bias.weights
network$trace.output <- results$trace.output
network$output.bias.weights <- results$output.bias.weights
network$hidden.output.weights <- results$hidden.output.weights
}
# update learning history
#history$hidden.win.tracker[i,] <- results$hidden
setTxtProgressBar(pb, i)
}
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
n <- temp.layer.many.activations(network, words)
network <- results$network
n <- temp.layer.many.activations(network, words)
network$trace.hidden
network$trace.output
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
warnings()
history <- list(
learning.curve = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
bias.tracker = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
output.bias.tracker = matrix(0, nrow = n.epochs/100, ncol= n.output),
output.match.tracker <- rep(0, times = n.epochs/100),
hidden.letter.similarity.tracking = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
hidden.stability = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
hidden.stability.tracking = update.hidden.layer.stability(letters, network),
output.trace.tracker <- matrix(0, nrow <- n.epochs/100, ncol <- n.output)
)
View(output.trace.tracker)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
history$output.trace.tracker
network <- results$network
history$output.trace.tracker
results$history$output.trace.tracker
output.trace.track <- results$history$output.trace.tracker
View(output.trace.track)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
output.trace.track <- results$history$output.trace.tracker
View(output.trace.tracker)
View(output.trace.track)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
output.trace.track <- results$history$output.trace.tracker
View(output.trace.track)
Rcpp::sourceCpp('forwardPassCpp.cpp')
