<<<<<<< HEAD
n.output <- 50
=======
<<<<<<< HEAD
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
=======
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,0.03))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
>>>>>>> origin/master
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
hidden.bias.param.minus <- 1
trace.param.output <- 0.86
output.bias.param.minus <- 1 #0
hidden.bias.param.plus <- 0.0005
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
percent.act.input <- 0.05
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
visualize.output.act.match()
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
counter <- 1
num.matches <- 0
act.per.word <- ceiling(length(output.results$output)/(n.output*percent.act.output*3))
length(output.results$output)
colnames(output.results) <- c("letter", "output")
colnames(output.results) <- c("letter", "output")
length(output.results$output)
26*3
26*5
130/(5*3)
130/(5*5)
ceiling(5.2)
act.per.word <- ceiling(length(output.results$output)/(n.output*percent.act.output*(n.output*percent.act.output)))
26*5
30*0.1
50*0.1
act.per.word <- n.output * percent.act.output * 3
visualize.output.act.match()
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 50  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
<<<<<<< HEAD
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
=======
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,11], type='b', ylim=c(0,0.015))
test.word.continuity1(results$network, words)
b <- visualize.output.act.match()
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
visualize.output.act.match()
b <- visualize.output.act.match()
b <- visualize.output.act.match()
test.word.continuity1(results$network, words)
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
ceiling(5)
test.word.continuity1(results$network, words)
<<<<<<< HEAD
visualize.output.act.match()
plot(results$history$output.match.tracker, ylim = range(0, 100), type='o', ann = F)
title(xlab = 'Time', ylab = 'Percentage of activation matches')
results$history$output.match.tracker
plot(results$history$output.match.tracker, ylim = range(0, 250), type='o', ann = F)
plot(results$history$output.match.tracker, ylim = range(0, 100), type='o', ann = F)
test.word.continuity1(results$network, words)
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
network <- results$network
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("letter", "output")
View(output.results)
counter <- 1
num.matches <- 0
act.per.word <- n.output * percent.act.output * 3
for(b in seq(from = act.per.word, to = length(output.results$output) + act.per.word, by = act.per.word)){
freq <- rle(sort(output.results$output[counter:b]))
counter <- counter + 9
for(h in 1:length(freq$lengths)){
if(freq$lengths[h] > 1){
num.matches = num.matches + freq$lengths[h]
}
}
}
percentage <- num.matches/(length(output.results$output))
counter <- 1
num.matches <- 0
act.per.word <- n.output * percent.act.output * 3
for(b in seq(from = act.per.word, to = length(output.results$output) + act.per.word, by = act.per.word)){
freq <- rle(sort(output.results$output[counter:b]))
counter <- counter + act.per.word
for(h in 1:length(freq$lengths)){
if(freq$lengths[h] > 1){
num.matches = num.matches + freq$lengths[h]
}
}
}
percentage <- num.matches/(length(output.results$output))
visualize.output.act.match()
source('~/GitHub/Int-Seg-Model/Visualize Output.R', echo=TRUE)
visualize.output.act.match()
counter <- 1
num.matches <- 0
act.per.word <- n.output * percent.act.output * 3
for(b in seq(from = act.per.word, to = length(output.results$output) + act.per.word, by = act.per.word)){
freq <- rle(sort(output.results$output[counter:b]))
counter <- counter + act.per.word
for(h in 1:length(freq$lengths)){
if(freq$lengths[h] > 1){
num.matches = num.matches + freq$lengths[h]
}
}
}
percentage <- num.matches/(length(output.results$output))
=======
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
>>>>>>> origin/master
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 50  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
m <- matrix(0, nrow=10, ncol=10)
m
sample(m, 10, replace=F) <- 1
length(m)
m[sample(1:length(m),10,replace=F)] <- 1
m
=======
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
<<<<<<< HEAD
=======
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,24], type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,11], type='b', ylim=c(0,1))
results$history$output.bias.tracker
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
>>>>>>> origin/master
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
<<<<<<< HEAD
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
=======
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
n.input <- 1600
n.hidden <- 100
n.output <- 20  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
<<<<<<< HEAD
n.epochs <- 10
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75   # 1-sparseness.percent is % nodes active
=======
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
<<<<<<< HEAD
=======
letter.noise.param <- 0.1
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
<<<<<<< HEAD
results <- batch(n.epochs) #run training batche
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
## work on correct implementation of sparseness with split network
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=1), nrow=n.input, ncol=n.hidden)
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=1), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
## work on correct implementation of sparseness with split network
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
network$input.hidden.weights
=======
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
<<<<<<< HEAD
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,11], type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,10], type='b', ylim=c(0,1))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,21], type='b', ylim=c(0,1))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,1))
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
<<<<<<< HEAD
n.epochs <- 10
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
=======
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86 #0.86
hidden.bias.param.minus <- 1
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.001 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
<<<<<<< HEAD
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=1), nrow=n.input, ncol=n.hidden)
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=1), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
## work on correct implementation of sparseness with split network
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
network$input.hidden.weights
=======
## RUN ##
results <- batch(n.epochs) #run training batches
>>>>>>> origin/master
visualize.output.act.match()
test.word.continuity1(results$network, words)
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
<<<<<<< HEAD
n.epochs <- 10
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
=======
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86
hidden.bias.param.minus <- 1
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
results$history$output.bias.tracker
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,1))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.1))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.01))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.001))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.001))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.001))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.001))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.003))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.004))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.004))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.004))
>>>>>>> origin/master
#install.packages('ggplot2')
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
output.trace.tracker.results <- results$history$trace.output.tracker
network <- results$network
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,9], type = "b")
visualize.output.act.match()
=======
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
<<<<<<< HEAD
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,24], type='b', ylim=c(0,0.004))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,21], type='b', ylim=c(0,0.004))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,21], type='b', ylim=c(0,0.004))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,1], type='b', ylim=c(0,0.004))
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
>>>>>>> origin/master
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
<<<<<<< HEAD
temp.layer.activations.many <- temp.layer.many.activations(network, words)
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,9], type = "b")
Rcpp::sourceCpp('forwardPassCpp.cpp')
visualize.output.act.match()
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
install.packages("RcppArmadillo")
Rcpp::sourceCpp('forwardPassCpp.cpp')
library(RcppArmadillo)
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
=======
visualize.output.act.match()
test.word.continuity1(results$network, words)
<<<<<<< HEAD
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,9], type='b', ylim=c(0,0.004))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,30], type='b', ylim=c(0,0.004))
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
>>>>>>> origin/master
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results)
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
visualize.output.act.match()
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.00005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
<<<<<<< HEAD
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
test.word.continuity1(results$network, words)
test.word.continuity1(results$network, words)
test.word.continuity1(results$network, words)[2]
res <- test.word.continuity1(results$network, words)
res <- test.word.continuity1(results$network, words)
res
source('C:/Users/Tom/Desktop/GitHub/Int-Seg-Model/Visualize Output.R')
res <- test.word.continuity1(results$network, words)
res
visualize.hidden.layer.learning(results$history)
=======
test.word.continuity1(results$network, words)
>>>>>>> 23f8874c802d655fd651bddaa2f17174aea764cb
