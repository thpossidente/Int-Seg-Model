n <- as.vector(t(1-adrop(readPNG('AlphabetPNG/N.png')[,,1,drop=F], drop=3))),
o <- as.vector(t(1-adrop(readPNG('AlphabetPNG/O.png')[,,1,drop=F], drop=3))),
p <- as.vector(t(1-adrop(readPNG('AlphabetPNG/P.png')[,,1,drop=F], drop=3))),
q <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Q.png')[,,1,drop=F], drop=3))),
r <- as.vector(t(1-adrop(readPNG('AlphabetPNG/R.png')[,,1,drop=F], drop=3))),
s <- as.vector(t(1-adrop(readPNG('AlphabetPNG/S.png')[,,1,drop=F], drop=3))),
t <- as.vector(t(1-adrop(readPNG('AlphabetPNG/T.png')[,,1,drop=F], drop=3))),
u <- as.vector(t(1-adrop(readPNG('AlphabetPNG/U.png')[,,1,drop=F], drop=3))),
v <- as.vector(t(1-adrop(readPNG('AlphabetPNG/V.png')[,,1,drop=F], drop=3))),
w <- as.vector(t(1-adrop(readPNG('AlphabetPNG/W.png')[,,1,drop=F], drop=3))),
x <- as.vector(t(1-adrop(readPNG('AlphabetPNG/X.png')[,,1,drop=F], drop=3))),
y <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Y.png')[,,1,drop=F], drop=3))),
z <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Z.png')[,,1,drop=F], drop=3)))
)
letters <- list(
"A" = a,
"B" = b,
"C" = c,
"D" = d,
"E" = e,
"F" = f,
"G" = g,
"H" = h,
"I" = i,
"J" = j,
"K" = k,
"L" = l,
"M" = m,
"N" = n,
"O" = o,
"P" = p,
"Q" = q,
"R" = r,
"S" = s,
"T" = t,
"U" = u,
"V" = v,
"W" = w,
"X" = x,
"Y" = y,
"Z" = z
)
words <- list(
abc <- cbind(a,b,c),
def <- cbind(d,e,f),
ghi <- cbind(g,h,i),
jkl <- cbind(j,k,l),
mno <- cbind(m,n,o),
pqr <- cbind(p,q,r),
stu <- cbind(s,t,u),
vwx <- cbind(v,w,x),
yz <- cbind(y,z)
)
sigmoid.activation <- function(x){
#return(1 / (1+exp(-x)))
return(x)
}
noise.in.letter <- function(letter){
for(i in 1:(0.1*n.input)){
letter[(sample(1:1600,1,replace=T))] <- 1
}
return(letter)
}
learning.measure <- function(input.hidden.weights){
all.letters.compared <- numeric(26)
best.fit <- numeric(n.hidden)
for(i in 1:n.hidden){
for(h in 1:26){
all.letters.compared[h] <- sum(abs(input.hidden.weights[,i] - alphabet[[h]]))
}
best.fit[i] <- min(all.letters.compared)
}
return(best.fit)
}
>>>>>>> b3102444d605f6c298c8a8eaa56bd610efa20724
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=1), nrow=n.input, ncol=n.hidden)
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=1), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
if(is.na(network)){
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
for(h in 1:(sparseness.percent*(n.input * n.hidden))){ ## work on correct implementation of sparseness with split network
network[[1]][sample(1:n.input, 1, replace=TRUE), sample(1:n.hidden, 1, replace=TRUE)] <- NA
}
for(g in 1:(sparseness.percent*(n.hidden*n.output))){
network[[3]][sample(1:n.hidden, 1, replace=TRUE), sample(1:n.output, 1, replace=TRUE)] <- NA
}
}
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
n.words <- length(words)
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, s)
network <- results$network
temp.layer.many.activations(network, words)
visualize.output.act.match()
visualize.output.act.match()
n <- temp.layer.many.activations(network, words)
n[1,]
n[27,]
n[22,]
n[48,]
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
Rcpp::sourceCpp('multi-layer-network-cpp.cpp')
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
=======
n.words <- length(words)
results <- batch(n.epochs) #run training batches
hidden <- numeric(n.hidden)
>>>>>>> b3102444d605f6c298c8a8eaa56bd610efa20724
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
output.trace.tracker.results <- results$history$trace.output.tracker
View(output.trace.tracker.results)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
