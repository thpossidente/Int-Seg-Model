which.max(storing.activations[1,])
storing.activations[1,]
storing.activations[1,]
?which.max
which(storing.activations[1,] == which.max(storing.activations[1,]))
which(storing.activations[1,] == max(storing.activations[1,]))
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0,1000)
#plot(results$history$bias.tracker[,i], ylim = 0,1000)
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.output.bias.tracker <- function(results){
for(i in 1:n.output){
plot(results$history$output.bias.tracker[,i], main=paste('Node', i))
}
}
test.word.continuity <- function(network, words){
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
temp.layer.activations(network, input.matrix)
}
temp.layer.activations <- function(network, input.matrix){
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[1,] == max(storing.activations[1,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("letter", "output")
g <- ggplot(output.results, aes(x=letter, y=output)) +
geom_point()+
ylim(1,50)+
theme_bw()
print(g)
#image(storing.activations)
print(storing.activations)
}
test.word.continuity(results$network, words)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.2
learning.rate.output <- 0.3
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.8
num.inputs.generated <- 50
integration.parameter <- 1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system.
# if 0: temporal patterns of input for both systems are correlated (consistently co-occur)
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run tra
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
visualize.letter.activations <- function(network, input){
result <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
layout(matrix(1:active.nodes, nrow=round(sqrt(active.nodes))))
for(i in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
active.nodes
visualize.letter.activations <- function(network, input){
result <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
layout(matrix(1:length(active.nodes), nrow=round(sqrt(length(active.nodes)))))
for(i in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
active.nodes
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
input <-  words[[sample(1:9,1, replace = T)]][,1]
input
network <- results$network
result <- forward.pass(input[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
layout(matrix(1:length(active.nodes), nrow=round(sqrt(length(active.nodes)))))
nrow <- round(sqrt(length(active.nodes))
layout(matrix(1:length(active.nodes), nrow=(sqrt(length(active.nodes)))))
for(i in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
nrow <- round(sqrt(length(active.nodes)))
ncol <- ceiling(length(active.nodes) / nrow)
visualize.letter.activations <- function(network, input){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
nrow <- round(sqrt(length(active.nodes)))
ncol <- ceiling(length(active.nodes) / nrow)
layout(matrix(1:(nrow*ncol), nrow=nrow))
for(i in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
visualize.letter.activations <- function(network, input){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
nplots <- length(active.nodes) + 1
nrow <- round(sqrt(nplots))
ncol <- ceiling(nplots / nrow)
layout(matrix(1:(nrow*ncol), nrow=nrow))
image(t(apply(matrix(input, nrow = 40),1,rev)))
for(i in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
visualize.letter.activations(results$network, words[[sample(1:9,1, replace = T)]][,1])
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.2
learning.rate.output <- 0.3
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.02
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.8
num.inputs.generated <- 50
integration.parameter <- 1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system.
# if 0: temporal patterns of input for both systems are correlated (consistently co-occur)
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
results$network$hidden.output.weights
results$network$hidden.bias.weights
#results <- batch(n.epochs, network = results$network)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
results$network$hidden.bias.weights
results$network$input.hidden.weights
results$history$bias.tracker[,1]
results$history$bias.tracker[,2]
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0,1000)
plot(results$history$bias.tracker[,i], ylim = 0,1000)
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
results$history$bias.tracker[2,]
results$history$bias.tracker[,1]
results$history$bias.tracker[,100]
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0,1000)
plot(results$history$bias.tracker[,i], ylim = c(0,1000))
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0,1000)
plot(results$history$bias.tracker[,i])
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.2
learning.rate.output <- 0.3
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.002
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.8
num.inputs.generated <- 50
integration.parameter <- 1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system.
# if 0: temporal patterns of input for both systems are correlated (consistently co-occur)
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
visualize.letter.activations(results$network, b)
results$network$hidden.output.weights
results$network$hidden.bias.weights
#results <- batch(n.epochs, network = results$network)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, d)
visualize.letter.activations(results$network, q)
visualize.letter.activations(results$network, z)
network <- results$network
input <- z
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
nplots <- length(active.nodes) + 1
nrow <- round(sqrt(nplots))
ncol <- ceiling(nplots / nrow)
layout(matrix(1:(nrow*ncol), nrow=nrow))
image(t(apply(matrix(input, nrow = 40),1,rev)))
for(i in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
network$input.hidden.weights[,active.nodes]
qq <- network$input.hidden.weights[,active.nodes]
?apply
qqq <- apply(qq, 2, mean)
?mean
m.fun <- function(x) { return(mean(x, na.rm=T)) }
qqq <- apply(qq, 2, m.fun)
image(t(apply(matrix(qqq, nrow = 40),1,rev)))
qqq <- apply(qq, 1, m.fun)
image(t(apply(matrix(qqq, nrow = 40),1,rev)))
visualize.letter.activations <- function(network, input){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
nplots <- length(active.nodes) + 2
nrow <- round(sqrt(nplots))
ncol <- ceiling(nplots / nrow)
layout(matrix(1:(nrow*ncol), nrow=nrow))
image(t(apply(matrix(input, nrow = 40),1,rev)))
for(i in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
all.active.nodes <- network$input.hidden.weights[,active.nodes]
m.fun <- function(x) { return(mean(x, na.rm=T)) }
average.weights <- apply(all.active.nodes, 1, m.fun)
image(t(apply(matrix(average.weights, nrow = 40),1,rev)))
}
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, d)
visualize.letter.activations(results$network, e)
visualize.letter.activations(results$network, f)
visualize.letter.activations(results$network, g)
visualize.letter.activations(results$network, h)
visualize.letter.activations(results$network, i)
visualize.letter.activations <- function(network, input){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
nplots <- length(active.nodes) + 2
nrow <- round(sqrt(nplots))
ncol <- ceiling(nplots / nrow)
layout(matrix(1:(nrow*ncol), nrow=nrow))
image(t(apply(matrix(input, nrow = 40),1,rev)))
for(act in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,act], nrow = 40),1,rev)))
}
all.active.nodes <- network$input.hidden.weights[,active.nodes]
m.fun <- function(x) { return(mean(x, na.rm=T)) }
average.weights <- apply(all.active.nodes, 1, m.fun)
image(t(apply(matrix(average.weights, nrow = 40),1,rev)))
}
visualize.letter.activations(results$network, i)
source('Load Letters.R')
visualize.letter.activations(results$network, i)
visualize.letter.activations(results$network, j)
visualize.letter.activations(results$network, k)
visualize.letter.activations(results$network, l)
test.word.continuity(results$network, words)
source('~/GitHub/Int-Seg-Model/Visualize Output.R', echo=TRUE)
test.word.continuity(results$network, words)
source('~/GitHub/Int-Seg-Model/Visualize Output.R', echo=TRUE)
test.word.continuity(results$network, words)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.1
learning.rate.output <- 0.3
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.8
num.inputs.generated <- 50
integration.parameter <- 1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training b
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
visualize.letter.activations(results$network, c)
visualize.letter.activations(results$network, d)
visualize.letter.activations(results$network, e)
visualize.letter.activations(results$network, f)
visualize.letter.activations(results$network, g)
visualize.letter.activations(results$network, h)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.2
learning.rate.output <- 0.3
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.8
num.inputs.generated <- 50
integration.parameter <- 1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.letter.activations(results$network, h)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
visualize.letter.activations(results$network, i)
visualize.letter.activations(results$network, j)
visualize.letter.activations(results$network, k)
visualize.letter.activations(results$network, z)
visualize.letter.activations(results$network, w)
visualize.letter.activations(results$network, d)
visualize.letter.activations(results$network, n)
random.inputs <- function(){
one.zero <- c(0,1)
input.list <- vector('list', num.inputs.generated)
for(i in 1:num.inputs.generated){
input.list[[i]] <- c(sample(one.zero, n.input/2, replace = TRUE, c(0.9, 0.1)))
}
grouped.inputs <- vector('list', (num.inputs.generated/5))
counter <- -1
for(b in 1:(num.inputs.generated/5)){
counter <- counter + 1
grouped.inputs[[b]] <- list(input.list[[(counter*5)+1]],
input.list[[(counter*5)+2]],
input.list[[(counter*5)+3]],
input.list[[(counter*5)+4]],
input.list[[(counter*5)+5]]
)
}
return(list(input.list, grouped.inputs))
}
results.random.inputs <- random.inputs()
input.list <- results.random.inputs[1]
grouped.inputs <- results.random.inputs[2]
if(input.gen.parameter == 0){ #System 1 gets temporal patterns of 5 inputs in order. System 2 gets random input every time
inputs <- matrix(NA, nrow=n.epochs, ncol=n.input)
for(i in 1:n.epochs/5){
random.group <- grouped.inputs[[1]][[sample(1:(num.inputs.generated/5),1,replace=T)]]
for(h in 1:5){
inputs[i+h-1,] <- c(random.group[[h]], input.list[[1]][[sample(1:num.inputs.generated,1, replace = TRUE)]])
}
}
return(inputs) ##inputs data not quite right
}
input.generation(input.gen.parameter)
results.random.inputs <- random.inputs()
input.list <- results.random.inputs[1]
grouped.inputs <- results.random.inputs[2]
if(input.gen.parameter == 0){ #System 1 gets temporal patterns of 5 inputs in order. System 2 gets random input every time
inputs <- matrix(NA, nrow=n.epochs, ncol=n.input)
for(i in 1:n.epochs/5){
random.group <- grouped.inputs[[1]][[sample(1:(num.inputs.generated/5),1,replace=T)]]
for(h in 1:5){
inputs[i+h-1,] <- c(random.group[[h]], input.list[[1]][[sample(1:num.inputs.generated,1, replace = TRUE)]])
}
}
#return(inputs) ##inputs data not quite right
}
View(inputs)
n.input
nrow(inputs)
ncol(inputs)
results.random.inputs <- random.inputs()
input.list <- results.random.inputs[1]
grouped.inputs <- results.random.inputs[2]
if(input.gen.parameter == 0){ #System 1 gets temporal patterns of 5 inputs in order. System 2 gets random input every time
inputs <- matrix(NA, nrow=n.epochs, ncol=n.input)
for(i in 0:n.epochs/5){
random.group <- grouped.inputs[[1]][[sample(1:(num.inputs.generated/5),1,replace=T)]]
for(h in 1:5){
inputs[(i*5)+h,] <- c(random.group[[h]], input.list[[1]][[sample(1:num.inputs.generated,1, replace = TRUE)]])
}
}
return(inputs) ##inputs data not quite right
}
View(inputs)
results.random.inputs <- random.inputs()
input.list <- results.random.inputs[1]
grouped.inputs <- results.random.inputs[2]
if(input.gen.parameter == 0){ #System 1 gets temporal patterns of 5 inputs in order. System 2 gets random input every time
inputs <- matrix(NA, nrow=n.epochs, ncol=n.input)
for(i in 0:((n.epochs/5)-1)){
random.group <- grouped.inputs[[1]][[sample(1:(num.inputs.generated/5),1,replace=T)]]
for(h in 1:5){
inputs[(i*5)+h,] <- c(random.group[[h]], input.list[[1]][[sample(1:num.inputs.generated,1, replace = TRUE)]])
}
}
return(inputs) ##inputs data not quite right
}
View(inputs)
?sample
v <- numeric(3)
for(i in 1:3){
v[i] <- sample(1:3, 1, replace = F)
}
new.input.list1 <- input.list
random1 <- sample(1:num.inputs.generated, 1, replace=T)
new.input.list1 <- new.input.list1[[1]][[-random1]]
new.input.list1 <- input.list
new.input.list2 <- input.list
random1 <- sample(1:num.inputs.generated, 1, replace=T)
random2 <- sample(1:num.inputs.generated, 1, replace=T)
new.input.list1[[1]][[random1]] <- NULL
new.input.list2[[1]][[random2]] <- NULL
inputs <- matrix(NA, nrow=n.epochs, ncol=n.input)
paired.inputs <- matrix(NA, nrow = num.inputs.generated, ncol = n.input)
new.input.list1 <- input.list
new.input.list2 <- input.list
for(b in 1:num.inputs.generated){
random1 <- sample(1:length(new.input.list1), 1, replace=T)
random2 <- sample(1:length(new.input.list2), 1, replace=T)
paired.inputs[b,] <- c(new.input.list1[[1]][[random1]], new.input.list2[[1]][[random2]])
new.input.list1[[1]][[random1]] <- NULL
new.input.list2[[1]][[random2]] <- NULL
}
View(paired.inputs)
inputs <- matrix(NA, nrow=n.epochs, ncol=n.input)
paired.inputs <- matrix(NA, nrow = num.inputs.generated, ncol = n.input)
new.input.list1 <- input.list
new.input.list2 <- input.list
for(b in 1:num.inputs.generated){
random1 <- sample(1:length(new.input.list1), 1, replace=T)
random2 <- sample(1:length(new.input.list2), 1, replace=T)
paired.inputs[b,] <- c(new.input.list1[[1]][[random1]], new.input.list2[[1]][[random2]])
new.input.list1[[1]][[random1]] <- NULL
new.input.list2[[1]][[random2]] <- NULL
}
for(c in 1:n.epochs){
inputs[c,] <- paired.inputs[sample(1:num.inputs.generated,1, replace=T),]
}
View(inputs)
