learning.rate.hidden <- 0.0005
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.0001
learning.rate.output <- 0.2
n.epochs <- 30000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.15
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.0001
learning.rate.output <- 0.2
n.epochs <- 30000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training bat
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.00005
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
results$network$hidden.output.weights
results$network$output.bias.weights
display.output.bias.tracker(results)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run trainin
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 30000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 9
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.2
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 25
learning.rate.hidden <- 0.05
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.2
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #r
display.output.bias.tracker(results)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 25
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
test.word.continuity(results$network, words)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 50
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
test.word.continuity(results$network, words)
temp.layer.activations <- function(network, input.matrix){
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
output.results <- rbind(output.results, c(letter=i, output=which.max(storing.activations[i,])))
}
colnames(output.results) <- c("letter", "output")
g <- ggplot(output.results, aes(x=letter, y=output)) +
geom_point()+
ylim(1,50)+
theme_bw()
print(g)
#image(storing.activations)
print(storing.activations)
}
test.word.continuity(results$network, words)
results <- batch(n.epochs) #run training batches
results$network$hidden.output.weights
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
results$network$output.bias.weights
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.1
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
network[[1]]
network <- list(
input.hidden.weights = matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden), #initialiize weights at random values between 0 and 0.05
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = matrix(runif(n.hidden*n.output, min=0, max=0.05), nrow=n.hidden, ncol=n.output),
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
network[[1]]
network[[1]][1,1]
?sample
network <- list(
input.hidden.weights = matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden), #initialiize weights at random values between 0 and 0.05
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = matrix(runif(n.hidden*n.output, min=0, max=0.05), nrow=n.hidden, ncol=n.output),
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
for(h in 1:(sparseness.percent*(n.input * n.hidden))){
network[[1]][sample(1:n.input, 1, replace=TRUE), sample(1:n.hidden, 1, replace=True)] <- NA
}
sparseness.percent <- 0.25
for(h in 1:(sparseness.percent*(n.input * n.hidden))){
network[[1]][sample(1:n.input, 1, replace=TRUE), sample(1:n.hidden, 1, replace=True)] <- NA
}
for(h in 1:(sparseness.percent*(n.input * n.hidden))){
network[[1]][sample(1:n.input, 1, replace=TRUE), sample(1:n.hidden, 1, replace=TRUE)] <- NA
}
for(g in 1:(sparseness.percent*(n.hidden*n.output))){
network[[3]][sample(1:n.hidden, 1, replace=TRUE), sample(1:n.output, 1, replace=TRUE)] <- NA
}
network[[1]]
input.hidden.weights
network[[1]][,1]
na.omit(network[[1]][,1])
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.1
learning.rate.output <- 0.3
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.25
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batch
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.1
learning.rate.output <- 0.3
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.25
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run traini
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
test.word.continuity(results$network, words)
display.output.bias.tracker(results)
results$network$hidden.output.weights
results$network$output.bias.weights
results$network$hidden.bias.weights
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
?image
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
?image
display.output.bias.tracker(results)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
?image
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.1
learning.rate.output <- 0.3
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.25
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.output.bias.tracker(results)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = )
#plot(results$history$bias.tracker[,i])
#for(u in 1:n.hidden){
#for(h in 1:n.input){
#if(is.na(results$network$input.hidden.weights[h,u])){
#results$network$input.hidden.weights[h,u] <- 0
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = )
#plot(results$history$bias.tracker[,i])
#for(u in 1:n.hidden){
#for(h in 1:n.input){
#if(is.na(results$network$input.hidden.weights[h,u])){
#results$network$input.hidden.weights[h,u] <- 0
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0:1000)
plot(results$history$bias.tracker[,i], ylim = 0:1000)
#for(u in 1:n.hidden){
#for(h in 1:n.input){
#if(is.na(results$network$input.hidden.weights[h,u])){
#results$network$input.hidden.weights[h,u] <- 0
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0,1000)
plot(results$history$bias.tracker[,i], ylim = 0,1000)
#for(u in 1:n.hidden){
#for(h in 1:n.input){
#if(is.na(results$network$input.hidden.weights[h,u])){
#results$network$input.hidden.weights[h,u] <- 0
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0,1000)
#plot(results$history$bias.tracker[,i], ylim = 0,1000)
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
n.output <- 30
learning.rate.hidden <- 0.1
learning.rate.output <- 0.3
n.epochs <- 100000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.1
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.25
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0,1000)
#plot(results$history$bias.tracker[,i], ylim = 0,1000)
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
