<<<<<<< HEAD
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
=======
<<<<<<< HEAD
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
hidden.bias.param.minus <- 2
=======
<<<<<<< HEAD
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
=======
>>>>>>> origin/master
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
output.trace.tracker.results <- results$history$output.trace.tracker
plot(x=seq(from = 1, to = 40, by = 1), y=output.trace.tracker.results[,2], type = "b")
output.trace.tracker.results
plot(x=seq(from = 1, to = 10, by = 1), y=output.trace.tracker.results[,2], type = "b")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
<<<<<<< HEAD
sparseness.percent <- 0.75
=======
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
<<<<<<< HEAD
=======
letter.noise.param <- 0.1
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
emp.layer.activations.many <- temp.layer.many.activations(network, words)
output.trace.tracker.results <- results$history$output.trace.tracker
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,2], type = "b")
visualize.output.act.match()
#install.packages('ggplot2')
=======
<<<<<<< HEAD
Rcpp::sourceCpp('forwardPassCpp.cpp')
#install.packages('ggplot2')
=======
library(ggplot2)
install.packages('ggplot2')
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> origin/master
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
<<<<<<< HEAD
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
=======
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
<<<<<<< HEAD
sparseness.percent <- 0.75
=======
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
#install.packages('ggplot2')
library(ggplot2)
=======
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
#install.packages('ggplot2')
library(ggplot2)
=======
Rcpp::sourceCpp('forwardPassCpp.cpp')
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, q)
visualize.output.act.match()
temp.layer.activations.many <- temp.layer.many.activations(network, words)
output.trace.tracker.results <- results$history$trace.output.tracker
temp.layer.activations.many[27,]
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> origin/master
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
<<<<<<< HEAD
n.epochs <- 10000
=======
<<<<<<< HEAD
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
=======
n.epochs <- 4500
>>>>>>> origin/master
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
<<<<<<< HEAD
sparseness.percent <- 0.75
=======
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
<<<<<<< HEAD
n.epochs <- 10000
=======
<<<<<<< HEAD
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
n.epochs <- 10
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
<<<<<<< HEAD
=======
=======
n.epochs <- 4500
>>>>>>> origin/master
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
Rcpp::sourceCpp('forwardPassCpp.cpp')
<<<<<<< HEAD
=======
=======
results <- batch(n.epochs) #run training batches
>>>>>>> origin/master
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
<<<<<<< HEAD
=======
emp.layer.activations.many <- temp.layer.many.activations(network, words)
output.trace.tracker.results <- results$history$trace.output.tracker
temp.layer.activations.many[27,]
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
temp.layer.activations.many[1,]
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
Rcpp::sourceCpp('forwardPassCpp.cpp')
>>>>>>> origin/master
Rcpp::sourceCpp('forwardPassCpp.cpp')
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
=======
<<<<<<< HEAD
alphabet[1]
noiseInLetter(alphabet[1], n.input, letter.noise.param, n.epochs)
typeof(alphabet[1])
typeof(alphabet[[1]])
words[1]
words[[1]]
typeof(words[[1]])
test[,1]
word <- words[[sample(1:n.words,1, replace = T)]]
word
word[,1]
word1 <- word[,1]
typeof(word1)
noiseInLetter(alphabet[1], n.input, letter.noise.param, n.epochs)
noiseInLetter(alphabet[[1]], n.input, letter.noise.param, n.epochs)
word <- words[[sample(1:n.words,1, replace = T)]]
word1 <- word[,1]
typeof(word1)
=======
<<<<<<< HEAD
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
m <- matrix(0, nrow=10, ncol=10)
m
sample(m, 10, replace=F) <- 1
length(m)
m[sample(1:length(m),10,replace=F)] <- 1
m
<<<<<<< HEAD
=======
=======
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, q)
visualize.output.act.match()
temp.layer.activations.many[1,]
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
output.trace.tracker.results <- results$history$output.trace.tracker
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 5, by = 1), y=output.trace.tracker.results[,1], type = "b")
temp.layer.activations.many[1,]
temp.layer.activations.many[27,]
temp.layer.activations.many[2,]
temp.layer.activations.many[28,]
results$network$hidden.output.weights
results$network$input.hidden.weights
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> origin/master
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
word <- words[[sample(1:n.words,1, replace = T)]]
input <- word[,b]
noiseInLetter(input, n.input, letter.noise.param, n.epochs)
=======
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, q)
output.trace.tracker.results <- results$history$output.trace.tracker
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 40, by = 1), y=output.trace.tracker.results[,1], type = "b")
results$network$hidden.output.weights
results$network$input.hidden.weights
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
>>>>>>> origin/master
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75   # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batche
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
## work on correct implementation of sparseness with split network
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=1), nrow=n.input, ncol=n.hidden)
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=1), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
## work on correct implementation of sparseness with split network
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
network$input.hidden.weights
<<<<<<< HEAD
=======
=======
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
=======
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, q)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 50, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 101, by = 1), y=output.trace.tracker.results[,1], type = "b")
output.trace.tracker.results\
output.trace.tracker.results
plot(x=seq(from = 1, to = 40, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 40, by = 1), y=output.trace.tracker.results[,2], type = "b")
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> origin/master
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
<<<<<<< HEAD
=======
<<<<<<< HEAD
n.epochs <- 10000
=======
<<<<<<< HEAD
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
n.epochs <- 10
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
<<<<<<< HEAD
=======
=======
n.epochs <- 1000
>>>>>>> origin/master
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
<<<<<<< HEAD
## RUN ##
results <- batch(n.epochs) #run training batches
learningMeasure(network$input.hidden.weights, n.hidden, alphabet)
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
learningMeasure(network$input.hidden.weights, n.hidden, alphabet)
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
learningMeasure(network$input.hidden.weights, n.hidden, alphabet)
learningMeasure(network$input.hidden.weights, n.hidden, alphabet)
learningMeasure(network$input.hidden.weights, n.hidden, alphabet)
=======
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=1), nrow=n.input, ncol=n.hidden)
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=1), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
## work on correct implementation of sparseness with split network
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
network$input.hidden.weights
>>>>>>> origin/master
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
<<<<<<< HEAD
=======
<<<<<<< HEAD
n.epochs <- 10000
=======
<<<<<<< HEAD
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
n.epochs <- 10
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
<<<<<<< HEAD
=======
=======
n.epochs <- 1000
>>>>>>> origin/master
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, q)
visualize.output.act.match()
emp.layer.activations.many <- temp.layer.many.activations(network, words)
visualize.output.act.match()
temp.layer.activations.many[28,]
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,2], type = "b")
output.trace.tracker.results
#install.packages('ggplot2')
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
display.learning.curves(results)
visualize.letter.activations(results$network, o)
visualize.letter.activations(results$network, e)
visualize.letter.activations(results$network, f)
visualize.letter.activations(results$network, t)
visualize.output.act.match()
visualize.output.act.match()
temp.layer.activations.many[2,]
temp.layer.activations.many <- temp.layer.many.activations(network, words)
output.trace.tracker.results <- results$history$output.trace.tracker
temp.layer.activations.many[2,]
temp.layer.activations.many[1,]
temp.layer.activations.many[3,]
temp.layer.activations.many[4,]
temp.layer.activations.many[5,]
temp.layer.activations.many[9,]
temp.layer.activations.many[0,]
temp.layer.activations.many[7,]
temp.layer.activations.many[8,]
temp.layer.activations.many[9,]
temp.layer.activations.many[,0]
26*5
temp.layer.activations.many[9,]
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, t)
visualize.letter.activations(results$network, j)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
temp.layer.activations.many <- temp.layer.many.activations(network, words)
output.trace.tracker.results <- results$history$output.trace.tracker
temp.layer.activations.many[9,]
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,20], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,20], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,2], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,3], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,4], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,10], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,19], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,17], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,16], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,24], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,30], type = "b")
visualize.hidden.layer.learning(results$history)
=======
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
output.trace.tracker.results <- results$history$trace.output.tracker
network <- results$network
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,9], type = "b")
visualize.output.act.match()
<<<<<<< HEAD
=======
=======
>>>>>>> cfe2c7c7913958c3f1d088d14e40577f65a7436a
>>>>>>> origin/master
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
<<<<<<< HEAD
=======
<<<<<<< HEAD
visualize.letter.activations(results$network, j)
visualize.output.act.match()
=======
<<<<<<< HEAD
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
temp.layer.activations.many <- temp.layer.many.activations(network, words)
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,9], type = "b")
Rcpp::sourceCpp('forwardPassCpp.cpp')
visualize.output.act.match()
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
install.packages("RcppArmadillo")
Rcpp::sourceCpp('forwardPassCpp.cpp')
library(RcppArmadillo)
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
Rcpp::sourceCpp('Int-Seg-Model/forwardPassCpp.cpp')
<<<<<<< HEAD
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
Rcpp::sourceCpp('forwardPassCpp.cpp')
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # 1-sparseness.percent is % nodes active
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
=======
>>>>>>> origin/master
>>>>>>> 50f6d12a4c386111ccca84fbe5c95b4acd30e3d5
