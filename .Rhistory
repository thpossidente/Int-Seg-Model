<<<<<<< HEAD
n <- as.vector(t(1-adrop(readPNG('AlphabetPNG/N.png')[,,1,drop=F], drop=3))),
o <- as.vector(t(1-adrop(readPNG('AlphabetPNG/O.png')[,,1,drop=F], drop=3))),
p <- as.vector(t(1-adrop(readPNG('AlphabetPNG/P.png')[,,1,drop=F], drop=3))),
q <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Q.png')[,,1,drop=F], drop=3))),
r <- as.vector(t(1-adrop(readPNG('AlphabetPNG/R.png')[,,1,drop=F], drop=3))),
s <- as.vector(t(1-adrop(readPNG('AlphabetPNG/S.png')[,,1,drop=F], drop=3))),
t <- as.vector(t(1-adrop(readPNG('AlphabetPNG/T.png')[,,1,drop=F], drop=3))),
u <- as.vector(t(1-adrop(readPNG('AlphabetPNG/U.png')[,,1,drop=F], drop=3))),
v <- as.vector(t(1-adrop(readPNG('AlphabetPNG/V.png')[,,1,drop=F], drop=3))),
w <- as.vector(t(1-adrop(readPNG('AlphabetPNG/W.png')[,,1,drop=F], drop=3))),
x <- as.vector(t(1-adrop(readPNG('AlphabetPNG/X.png')[,,1,drop=F], drop=3))),
y <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Y.png')[,,1,drop=F], drop=3))),
z <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Z.png')[,,1,drop=F], drop=3)))
)
letters <- list(
"A" = a,
"B" = b,
"C" = c,
"D" = d,
"E" = e,
"F" = f,
"G" = g,
"H" = h,
"I" = i,
"J" = j,
"K" = k,
"L" = l,
"M" = m,
"N" = n,
"O" = o,
"P" = p,
"Q" = q,
"R" = r,
"S" = s,
"T" = t,
"U" = u,
"V" = v,
"W" = w,
"X" = x,
"Y" = y,
"Z" = z
)
words <- list(
abc <- cbind(a,b,c),
def <- cbind(d,e,f),
ghi <- cbind(g,h,i),
jkl <- cbind(j,k,l),
mno <- cbind(m,n,o),
pqr <- cbind(p,q,r),
stu <- cbind(s,t,u),
vwx <- cbind(v,w,x),
yz <- cbind(y,z)
)
sigmoid.activation <- function(x){
#return(1 / (1+exp(-x)))
return(x)
}
noise.in.letter <- function(letter){
for(i in 1:(0.1*n.input)){
letter[(sample(1:1600,1,replace=T))] <- 1
}
return(letter)
}
learning.measure <- function(input.hidden.weights){
all.letters.compared <- numeric(26)
best.fit <- numeric(n.hidden)
for(i in 1:n.hidden){
for(h in 1:26){
all.letters.compared[h] <- sum(abs(input.hidden.weights[,i] - alphabet[[h]]))
}
best.fit[i] <- min(all.letters.compared)
}
return(best.fit)
}
>>>>>>> b3102444d605f6c298c8a8eaa56bd610efa20724
=======
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
>>>>>>> dc079ba76e04b220b2300b091f8fa7d3af15cd18
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
history$output.trace.tracker
network <- results$network
history$output.trace.tracker
results$history$output.trace.tracker
output.trace.track <- results$history$output.trace.tracker
View(output.trace.track)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
output.trace.track <- results$history$output.trace.tracker
View(output.trace.tracker)
View(output.trace.track)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
output.trace.track <- results$history$output.trace.tracker
View(output.trace.track)
Rcpp::sourceCpp('forwardPassCpp.cpp')
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, s)
output.trace.track <- results$history$output.trace.tracker
output.trace.tracker.results <- results$history$trace.output.tracker
View(output.trace.tracker.results)
ggplot(data = output.trace.tracker.results) +
geom_smooth()
ggplot(data = output.trace.tracker.results[ ,1]) +
geom_smooth()
?plot
?seq
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1])
?plot
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], b)
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = b)
?plot
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,2], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,3], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,4], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,5], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,30], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,25], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,23], type = "b")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
display.output.bias.tracker(results)
visualize.output.act.match()
temp.layer.activations.many <- temp.layer.many.activations(network, words)
network <- results$network
temp.layer.activations.many <- temp.layer.many.activations(network, words)
View(temp.layer.activations.many)
temp.layer.activations.many[1,]
temp.layer.activations.many[27,]
output.trace.tracker.results <- results$history$trace.output.tracker
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,23], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,1], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,3], type = "b")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 1 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
temp.layer.activations.many <- temp.layer.many.activations(network, words)
temp.layer.activations.many[1,]
temp.layer.activations.many[27,]
temp.layer.activations.many[28,]
temp.layer.activations.many[2,]
temp.layer.activations.many[3,]
temp.layer.activations.many[29,]
temp.layer.activations.many[4,]
temp.layer.activations.many[30,]
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.1 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
output.trace.tracker.results <- results$history$trace.output.tracker
View(output.trace.tracker.results)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.75 #0.75
trace.param.output <- 0.25 #0.75
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
=======
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
output.trace.tracker.results <- results$history$trace.output.tracker
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,3], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,6], type = "b")
plot(x=seq(from = 1, to = 100, by = 1), y=output.trace.tracker.results[,9], type = "b")
temp.layer.activations.many[1,]
temp.layer.activations.many[27,]
temp.layer.activations.many[2,]
temp.layer.activations.many[28,]
>>>>>>> dc079ba76e04b220b2300b091f8fa7d3af15cd18
