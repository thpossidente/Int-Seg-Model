hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
results$network$hidden.output.weights
results$network$output.bias.weights
#results <- batch(n.epochs, network = results$network)
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
plot(results$history$learning.curve[,i], main=paste("Node",i))
plot(results$history$bias.tracker[,i])
image(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),2,rev))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
plot(results$history$learning.curve[,i], main=paste("Node",i))
plot(results$history$bias.tracker[,i])
image(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
plot(results$history$learning.curve[,i], main=paste("Node",i))
plot(results$history$bias.tracker[,i])
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.5
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.1
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
results$network$hidden.output.weights
results$network$output.bias.weights
#results <- batch(n.epochs, network = results$network)
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.5
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.4
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
results$network$hidden.output.weights
results$network$output.bias.weights
#results <- batch(n.epochs, network = results$network)
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.5
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
results$network$hidden.output.weights
results$network$output.bias.weights
#results <- batch(n.epochs, network = results$network)
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.3
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.25
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
results$network$hidden.output.weights
results$network$output.bias.weights
#results <- batch(n.epochs, network = results$network)
library(parallel)
detectCores()
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
learning.rate.output <- 0.1
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.1
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.25
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.1
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.5
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run train
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.2
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.05
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.9
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0.1
output.bias.param.plus <- 0.000005
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0.03
output.bias.param.plus <- 0.0005
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
results$network$output.bias.weights
results$network$hidden.output.weights
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0.5
output.bias.param.plus <- 0.0000005
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
results$network$hidden.output.weights
results$network$output.bias.weights
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.1
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0.5
output.bias.param.plus <- 0.00005
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.1
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0.5
output.bias.param.plus <- 0.00005
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.15
n.epochs <- 20000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0.5
output.bias.param.plus <- 0.00005
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate.hidden <- 0.15
learning.rate.output <- 0.4
n.epochs <- 100000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.4
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 9
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 500000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 9
learning.rate.hidden <- 0.15
learning.rate.output <- 0.2
n.epochs <- 5000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 26
n.output <- 9
learning.rate.hidden <- 0.01
learning.rate.output <- 0.2
n.epochs <- 100000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.2
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run train
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
results$network$hidden.output.weights
results$network$output.bias.weights
display.output.bias.tracker(results)
test.word.continuity(results$network, words)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
library(ggplot2)
n.input <- 1600
n.hidden <- 150
n.output <- 9
learning.rate.hidden <- 0.00005
learning.rate.output <- 0.2
n.epochs <- 50000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 0
output.bias.param.plus <- 0
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #r
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
View(forward.pass)
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
test.word.continuity(results$network, words)
