count3 <- 0
for(k in 1:9){
for(h in 1:10){
count3 <- count3 + 1
joint.prob[count3] <- sum((output.results[,1] == k) & (output.results[,2] == h)) / length(output.results[,1])
}
}
probs.joint
probs.joint <- numeric(90)
count3 <- 0
for(k in 1:9){
for(h in 1:10){
count3 <- count3 + 1
probs.joint[count3] <- sum((output.results[,1] == k) & (output.results[,2] == h)) / length(output.results[,1])
}
}
probs.joint
sum(probs.joint)
mutual.info <- 0
for(x in 1:9){
for(t in 1:10){
count4 <- count4 + 1
mutual.info = mutual.info + (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.word[x]))))
}
}
count4 <- 0
for(x in 1:9){
for(t in 1:10){
count4 <- count4 + 1
mutual.info = mutual.info + (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.word[x]))))
}
}
mutual.info <- numeric(90)
mutual.info <- 0
probs.word <- numeric(9)
probs.act <- numeric(10)
probs.joint <- numeric(90)
mutual.info <- numeric(90)
count3 <- 0
count4 <- 0
for(w in 1:9){
probs.word[w] <- sum(output.results[,1] == w) / length(output.results[,1])
}
for(a in 1:10){
probs.act[a] <- sum(output.results[,2] == a) / length(output.results[,2])
}
for(k in 1:9){
for(h in 1:10){
count3 <- count3 + 1
probs.joint[count3] <- sum((output.results[,1] == k) & (output.results[,2] == h)) / length(output.results[,1])
}
}
for(x in 1:9){
for(t in 1:10){
count4 <- count4 + 1
mutual.info[count4] = mutual.info + (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.word[x]))))
}
}
warnings()
mutual.info
mutual.info <- 0
probs.word <- numeric(9)
probs.act <- numeric(10)
probs.joint <- numeric(90)
mutual.info <- numeric(90)
count3 <- 0
count4 <- 0
for(w in 1:9){
probs.word[w] <- sum(output.results[,1] == w) / length(output.results[,1])
}
for(a in 1:10){
probs.act[a] <- sum(output.results[,2] == a) / length(output.results[,2])
}
for(k in 1:9){
for(h in 1:10){
count3 <- count3 + 1
probs.joint[count3] <- sum((output.results[,1] == k) & (output.results[,2] == h)) / length(output.results[,1])
}
}
for(x in 1:9){
for(t in 1:10){
count4 <- count4 + 1
mutual.info[count4] = (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.word[x]))))
}
}
mutual.info
mutual.info <- sum(mutual.info, na.rm = T)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,8))
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
max.minfo <- matrix(0, nrow <- 26, ncol <- 2)
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,8))
max.minfo <- matrix(0, nrow <- 26, ncol <- 2)
max.minfo[,1] <- c(1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7,8,8,8,9,9)
max.minfo[,2] <- c(1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7,8,8,8,9,9)
View(max.minfo)
mutual.info <- 0
probs.word <- numeric(9)
probs.act <- numeric(10)
probs.joint <- numeric(90)
mutual.info <- numeric(90)
count3 <- 0
count4 <- 0
for(w in 1:9){
probs.word[w] <- sum(max.info[,1] == w) / length(max.info[,1])
}
max.minfo <- matrix(0, nrow <- 26, ncol <- 2)
max.minfo[,1] <- c(1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7,8,8,8,9,9)
max.minfo[,2] <- c(1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7,8,8,8,9,9)
mutual.info <- 0
probs.word <- numeric(9)
probs.act <- numeric(10)
probs.joint <- numeric(90)
mutual.info <- numeric(90)
count3 <- 0
count4 <- 0
for(w in 1:9){
probs.word[w] <- sum(max.minfo[,1] == w) / length(max.minfo[,1])
}
for(a in 1:10){
probs.act[a] <- sum(max.minfo[,2] == a) / length(max.minfo[,2])
}
for(k in 1:9){
for(h in 1:10){
count3 <- count3 + 1
probs.joint[count3] <- sum((max.minfo[,1] == k) & (max.minfo[,2] == h)) / length(max.minfo[,1])
}
}
for(x in 1:9){
for(t in 1:10){
count4 <- count4 + 1
mutual.info[count4] = (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.word[x]))))
}
}
mutual.info <- sum(mutual.info, na.rm = T)
log2(10)
View(max.minfo)
log2(9)
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forwardPass(n.output, percent.act.input, percent.act.output,
n.hidden, input.matrix[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("word", "output")
counter5 <- 1
for(z in 1:26){
output.results[z,1] <- counter5
if(z %% 3 == 0){
counter5 <- counter5 + 1
}
}
mutual.info <- 0
probs.word <- numeric(9)
probs.act <- numeric(10)
probs.joint <- numeric(90)
mutual.info <- numeric(90)
count3 <- 0
count4 <- 0
for(w in 1:9){
probs.word[w] <- sum(output.results[,1] == w) / length(output.results[,1])
}
for(a in 1:10){
probs.act[a] <- sum(output.results[,2] == a) / length(output.results[,2])
}
for(k in 1:9){
for(h in 1:10){
count3 <- count3 + 1
probs.joint[count3] <- sum((output.results[,1] == k) & (output.results[,2] == h)) / length(output.results[,1])
}
}
for(x in 1:9){
for(t in 1:10){
count4 <- count4 + 1
mutual.info[count4] = (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.word[x]))))
}
}
mutual.info <- sum(mutual.info, na.rm = T)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.05 #0
output.bias.param.plus <- 0.0005 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,8))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,6], type='b', ylim=c(0,0.02))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,8))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,6], type='b', ylim=c(0,0.02))
source('~/GitHub/Int-Seg-Model/Spatial.Pooling.R', echo=TRUE)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.0025 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,8))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.005 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,8))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.01 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,8))
plot(x=seq(from=100, to=10000, by=100), y=results$history$output.bias.tracker[,6], type='b', ylim=c(0,0.02))
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,8))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.9 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.5))
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2))
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.95 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 250
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8 #0.9
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
