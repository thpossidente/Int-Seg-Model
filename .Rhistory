# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
network = results$network
history = results$history
network1 = network
history1 = history
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 100
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
network = results$network
history = results$history
network1 = network
history1 = history
history$hidden.letter.similarity.tracking
history$mutual.info.spatial.track[i / 100] <- mutual.info.spatial(network)
i
i = 100
history$mutual.info.spatial.track[i / 100] <- mutual.info.spatial(network)
sum(network1$input.hidden.weights != network$input.hidden.weights, na.rm = T)
sum(network1$hidden.bias.weights != network$hidden.bias.weights, na.rm = T)
sum(network1$hidden.output.weights != network$hidden.output.weights, na.rm = T)
sum(network1$output.bias.weights != network$output.bias.weights, na.rm = T)
sum(network1$trace.hidden != network$trace.hidden, na.rm = T)
sum(network1$trace.output != network$trace.output, na.rm = T)
sum(history$learning.curve != history$learning.curve, na.rm = T)
sum(history$hidden.letter.similarity.tracking != history$hidden.letter.similarity.tracking, na.rm = T)
sum(history$output.trace.tracker != history$output.trace.tracker, na.rm = T)
sum(history$output.bias.tracker != history$output.bias.tracker, na.rm = T)
sum(history$output.act.unique.tracker != history$output.act.unique.tracker, na.rm = T)
sum(history$mutual.info.spatial.track != history$mutual.info.spatial.track, na.rm = T)
sum(history$learning.curve != history1$learning.curve, na.rm = T)
sum(history$hidden.letter.similarity.tracking != history1$hidden.letter.similarity.tracking, na.rm = T)
sum(history$output.trace.tracker != history1$output.trace.tracker, na.rm = T)
sum(history$output.bias.tracker != history1$output.bias.tracker, na.rm = T)
sum(history$output.act.unique.tracker != history1$output.act.unique.tracker, na.rm = T)
sum(history$mutual.info.spatial.track != history1$``, na.rm = T)
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 100
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
results$history$hidden.letter.similarity.tracking
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 500
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 500
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
delay = 1
counter <- 4500#change to start what batch 2nd layer starts learning (start at 1 will have layer start learning after 5000 epochs)
counter.bias <- 5000 #change to start what batch output bias node starts at
pre.input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=0.5), nrow=n.input, ncol=n.hidden)   # Random normalization
pre.hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=0.5), nrow=n.hidden, ncol=n.output)
for(input in 1:(n.input/2)){
for(hidden in (n.hidden/2 + 1):n.hidden){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(input in (n.input/2 + 1):n.input){
for(hidden in 1:(n.hidden/2)){
if(runif(1) > integration.parameter){
pre.input.hidden.weights[input,hidden] <- NA
}
}
}
for(hidden in 1:(n.hidden/2)){
for(output in (n.output/2 + 1):n.output){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
for(hidden in (n.hidden/2 + 1):n.hidden){
for(output in 1:(n.output/2)){
if(runif(1) > integration.parameter){
pre.hidden.output.weights[hidden,output] <- NA
}
}
}
network = NA
if(is.na(network)){
network <- list(
input.hidden.weights = pre.input.hidden.weights,
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = pre.hidden.output.weights,
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output))
network[[1]][sample(1:(n.input*n.hidden), sparseness.percent*(n.input*n.hidden), replace=F)] <- NA
network[[3]][sample(1:(n.output*n.hidden), sparseness.percent*(n.output*n.hidden), replace=F)] <- NA
} else{
network$hidden.bias.weights <- matrix(0, nrow = n.hidden, ncol = 1)
network$output.bias.weights <- matrix(0, nrow = n.output, ncol = 1)
}
history <- list(               #initializes learning data matrices
mutual.info.spatial.track <- rep(0, times = n.epochs/100),
learning.curve = matrix(0, nrow = n.epochs/100, ncol = n.hidden),
hidden.letter.similarity.tracking = matrix(0, nrow=n.epochs/100, ncol = length(letters)),
output.trace.tracker = matrix(0, nrow = n.epochs, ncol = n.output),
output.bias.tracker = matrix(0, nrow=n.epochs/100, ncol = n.output),
output.act.unique.tracker <- rep(0, times=n.epochs/100)
#mutual.info.tracker <- rep(0, times = n.epochs/100)
)
iter = 0
iter1 = 0
counter = counter + 1
counter.bias = counter.bias + 1
word <- words[[sample(1:n.words,1, replace = T)]]
i
i = 1
if(i == 1 || i %% 100 == 0){
#history$mutual.info.spatial.track[i / 100] <- mutual.info.spatial(network)
history$learning.curve[i / 100,] <- learningMeasure(network$input.hidden.weights, n.hidden, alphabet)
history$hidden.letter.similarity.tracking[i / 100, ] <- batch.hidden.layer.learning(letters, network)$similarity
history$output.trace.tracker[i / 100, ] <- network$trace.output
history$output.bias.tracker[i / 100, ] <- network$output.bias.weights[,1]
history$output.act.unique.tracker[i / 100] <- output.act.unique(network, words)
#history$mutual.info.tracker[i /100] <- mutual.info.output(network)
}
history
i = 1
i <- 1
i = 100
history$learning.curve[i / 100,] <- learningMeasure(network$input.hidden.weights, n.hidden, alphabet)
history$hidden.letter.similarity.tracking
network
history$hidden.letter.similarity.tracking
history$hidden.letter.similarity.tracking[i / 100, ] <- batch.hidden.layer.learning(letters, network)$similarity
history$hidden.letter.similarity.tracking
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 100
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
results$history$hidden.letter.similarity.tracking
network
alphabet
letters
alphabet == letters
alphabet[1]
letters[1]
alphabet[[1]] == letterrs[[1]]
alphabet[[1]] == letters[[1]]
sum(alphabet[[1]] != letters[[1]])\
sum(alphabet[[1]] != letters[[1]])
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 100
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
results$history$hidden.letter.similarity.tracking
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 100
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
results$history$hidden.letter.similarity.tracking
input.mat <- matrix(0, ncol=n.input, nrow=26*10)
network
network = results$network
input.mat <- matrix(0, ncol=n.input, nrow=26*10)
r <- 1
for(i in 1:26){
for(j in 1:10){
input.mat[r,] <- noiseInLetter(letters[[i]], n.input, letter.noise.param)
r <- r + 1
}
}
storing.acts <- matrix(0, nrow=nrow(input.mat), ncol=n.output)
for(i in 1:nrow(input.mat)){
act.res <- forwardPass(n.output, percent.act.input,
percent.act.output, n.hidden,
input.mat[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.acts[i,] <- act.res$output
}
output.res <- data.frame(letter=numeric(),output=numeric())
tick <- 1
for(h in 1:nrow(storing.acts)){
output.res[h,1] <- tick
if(h %% 10 == 0){
tick = tick + 1
}
}
for(k in 1:nrow(storing.acts)){
output.res[k,2] <- which.max(storing.acts[k,])
}
probs.letter <- numeric(26)
probs.act <- numeric(30)
probs.joint <- numeric(26*30)
mutual.info <- numeric(26*30)
for(w in 1:length(probs.letter)){
probs.letter[w] <- sum(output.res[,1] == w) / length(output.res[,1])
}
for(x in 1:length(probs.act)){
probs.act[x] <- sum(output.res[,2] == x) / length(output.res[,2])
}
count3 = 0
for(k in 1:length(probs.letter)){
for(h in 1:length(probs.act)){
count3 <- count3 + 1
probs.joint[count3] <- sum((output.res[,1] == k) & (output.res[,2] == h)) / length(output.res[,1])
}
}
count4 = 0
for(v in 1:length(probs.letter)){
for(t in 1:length(probs.act)){
count4 <- count4 + 1
mutual.info[count4] = (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.letter[v]))))
}
}
v
mutual.info <- sum(mutual.info, na.rm = T)
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 100
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
results$history$hidden.letter.similarity.tracking
