n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.02 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 100
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
visualize.output.act.match()
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 50
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 300
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
visualize.output.act.match()
test.word.continuity1(results$network, words)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
source('~/GitHub/Int-Seg-Model/Input Generation.R', echo=TRUE)
input.generation(1)
result <- input.generation(1)
View(result)
result <- input.generation(1)
View(result)
num.inputs.generated <- n.input/2 # half of total inputs
result <- input.generation(1)
View(result)
View(result)
grouped.inputs <- vector('list', (num.inputs.generated/5))
one.zero <- c(1,0)
input.list <- vector('list', num.inputs.generated)
for(i in 1:num.inputs.generated){
input.list[[i]] <- c(sample(one.zero, n.input/2, replace = TRUE, c(0.1, 0.9)))
}
grouped.inputs <- vector('list', (num.inputs.generated/5))
counter <- -1
for(b in 1:(num.inputs.generated/5)){
counter <- counter + 1
grouped.inputs[[b]] <- list(input.list[[(counter*5)+1]],
input.list[[(counter*5)+2]],
input.list[[(counter*5)+3]],
input.list[[(counter*5)+4]],
input.list[[(counter*5)+5]]
)
}
results.random.inputs <- random.inputs()
input.list <- results.random.inputs[1]
grouped.inputs <- results.random.inputs[2]
inputs <- matrix(NA, nrow=n.epochs, ncol=n.input)
for(i in 0:((n.epochs/5)-1)){
random.group <- grouped.inputs[[1]][[sample(1:(num.inputs.generated/5),1,replace=T)]]
for(h in 1:5){
inputs[(i*5)+h,] <- c(random.group[[h]], input.list[[1]][[sample(1:num.inputs.generated,1, replace = TRUE)]])
}
}
result <- input.generation(0.5)
View(result)
sum(is.na(results))
inputs <- matrix(NA, nrow=n.epochs, ncol=n.input)
paired.inputs <- matrix(NA, nrow = num.inputs.generated, ncol = n.input)
new.input.list1 <- input.list
new.input.list2 <- input.list
for(b in 1:num.inputs.generated){
random1 <- sample(1:length(new.input.list1), 1, replace=T)
random2 <- sample(1:length(new.input.list2), 1, replace=T)
paired.inputs[b,] <- c(new.input.list1[[1]][[random1]], new.input.list2[[1]][[random2]])
new.input.list1[[1]][[random1]] <- NULL
new.input.list2[[1]][[random2]] <- NULL
}
inputs <- matrix(NA, nrow=n.epochs, ncol=n.input)
paired.inputs <- matrix(NA, nrow = num.inputs.generated, ncol = n.input)
new.input.list1 <- input.list
new.input.list2 <- input.list
for(b in 1:num.inputs.generated){
random1 <- sample(1:length(new.input.list1), 1, replace=T)
random2 <- sample(1:length(new.input.list2), 1, replace=T)
paired.inputs[b,] <- c(new.input.list1[[1]][[random1]], new.input.list2[[1]][[random2]])
new.input.list1[[1]][[random1]] <- NULL
new.input.list2[[1]][[random2]] <- NULL
}
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
visualize.output.act.match()
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.6 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.4 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.9 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 150
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.7 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
