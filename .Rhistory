# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
test.word.continuity(results$network, words)
library(dplyr)
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0,1000)
plot(results$history$bias.tracker[,i])
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.output.bias.tracker <- function(results){
for(i in 1:n.output){
plot(results$history$output.bias.tracker[,i], main=paste('Node', i))
}
}
test.word.continuity <- function(network, words){
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
temp.layer.activations(network, input.matrix)
}
temp.layer.activations <- function(network, input.matrix){
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("letter", "output")
## accuracy measurement ##
n <- 1
g <- (n.output*percent.act.output) * 3
g. <- (n.output*percent.act.output) * 3
counter <- 0
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:(g-(n.output*(percent.act.output)))]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:(g-(n.output*(percent.act.output)))])))
percentage <- counter/(((n.output*(percent.act.output))*2*8.5))
###
g <- ggplot(output.results, aes(x=letter, y=output)) +
geom_point()+
ylim(1,50)+
theme_bw()
print(g)
print(storing.activations)
print(percentage)
}
visualize.letter.activations <- function(network, input){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
nplots <- length(active.nodes) + 2
nrow <- round(sqrt(nplots))
ncol <- ceiling(nplots / nrow)
layout(matrix(1:(nrow*ncol), nrow=nrow))
image(t(apply(matrix(input, nrow = 40),1,rev)))
for(act in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,act], nrow = 40),1,rev)))
}
all.active.nodes <- network$input.hidden.weights[,active.nodes]
average.weights <- calculate.mean.weights(all.active.nodes)
image(t(apply(matrix(average.weights, nrow = 40),1,rev)))
}
calculate.mean.weights <- function(active.nodes){
m.fun <- function(x) { return(mean(x, na.rm=T)) }
average.weights <- apply(active.nodes, 1, m.fun)
return(average.weights)
}
hidden.layer.similarity <- function(letter, network, comparison.letter=NA){
result <- forward.pass(letter, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
all.active.nodes <- network$input.hidden.weights[,active.nodes]
average.weights <- calculate.mean.weights(all.active.nodes)
if(!all(is.na(comparison.letter))){
similarity <- sum(abs(comparison.letter - average.weights), na.rm = T)
} else {
similarity <- sum(abs(letter - average.weights), na.rm = T)
}
return(similarity)
}
batch.hidden.layer.learning <- function(letters, network){
result <- data.frame(input=names(letters), similarity=NA)
for(i in 1:nrow(result)){
result[i,"similarity"] <- hidden.layer.similarity(letters[[names(letters)[i]]], network)
}
return(result)
}
visualize.hidden.layer.learning <- function(history){
plotting.data <- expand.grid(letter=names(letters), time=1:nrow(history$hidden.letter.similarity.tracking))
plotting.data$similarity <- mapply(function(l, t){
return(history$hidden.letter.similarity.tracking[t,which(names(letters)==l)])
}, plotting.data$letter, plotting.data$time)
summary.data <- plotting.data %>% group_by(time) %>% summarize(mean.similarity = mean(similarity))
ggplot(plotting.data, aes(x=time, y=similarity, color = letter))+ geom_line() +
geom_line(data=summary.data, aes(x=time, y=mean.similarity, color=NA), size=2)+
labs(x='time', y='difference between network representation and letter')
}
hidden.layer.stability <- function(letter, input, network, history){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
previous.active.nodes <- history$hidden.stability.tracking[[letter]]
change <- length(active.nodes) - sum(active.nodes %in% previous.active.nodes)
return(change)
}
batch.hidden.layer.stability <- function(letters, network, history){
result <- data.frame(input=names(letters), stability=NA)
for(i in 1:nrow(result)){
result[i,"stability"] <- hidden.layer.stability(names(letters)[i], letters[[names(letters)[i]]], network, history)
}
return(result$stability)
}
update.hidden.layer.stability <- function(letters, network){
tracker <- sapply(names(letters), function(x){
result <- forward.pass(letters[[x]], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
return(active.nodes)
}, USE.NAMES = T, simplify=FALSE)
return(tracker)
}
source('~/GitHub/Int-Seg-Model/Spatial Pooling.R', echo=TRUE)
visualize.hidden.layer.learning(results$history)
library(dplyr)
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
#plot(results$history$learning.curve[,i], main=paste("Node",i), ylim = 0,1000)
plot(results$history$bias.tracker[,i])
image(t(apply(matrix(results$network$input.hidden.weights[,i], nrow = 40),1,rev)))
}
}
display.output.bias.tracker <- function(results){
for(i in 1:n.output){
plot(results$history$output.bias.tracker[,i], main=paste('Node', i))
}
}
test.word.continuity <- function(network, words){
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
temp.layer.activations(network, input.matrix)
}
temp.layer.activations <- function(network, input.matrix){
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
output.results <- data.frame(letter=numeric(),output=numeric())
for(i in 1:nrow(storing.activations)){
for(j in which(storing.activations[i,] == max(storing.activations[i,]))){
output.results <- rbind(output.results, c(letter=i, output=j))
}
}
colnames(output.results) <- c("letter", "output")
## accuracy measurement ##
n <- 1
g <- (n.output*percent.act.output) * 3
g. <- (n.output*percent.act.output) * 3
counter <- 0
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:g]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:g])))
n <- g + 1
g <- g + g.
for(h in 1:n.output){
counter <- counter + sum(c(output.results$output[n:(g-(n.output*(percent.act.output)))]) == h)
}
counter <- counter - length(unique(c(output.results$output[n:(g-(n.output*(percent.act.output)))])))
percentage <- counter/(((n.output*(percent.act.output))*2*8.5))
###
g <- ggplot(output.results, aes(x=letter, y=output)) +
geom_point()+
ylim(1,50)+
theme_bw()
print(g)
print(storing.activations)
print(percentage)
}
visualize.letter.activations <- function(network, input){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
nplots <- length(active.nodes) + 2
nrow <- round(sqrt(nplots))
ncol <- ceiling(nplots / nrow)
layout(matrix(1:(nrow*ncol), nrow=nrow))
image(t(apply(matrix(input, nrow = 40),1,rev)))
for(act in active.nodes){
image(t(apply(matrix(network$input.hidden.weights[,act], nrow = 40),1,rev)))
}
all.active.nodes <- network$input.hidden.weights[,active.nodes]
average.weights <- calculate.mean.weights(all.active.nodes)
image(t(apply(matrix(average.weights, nrow = 40),1,rev)))
}
calculate.mean.weights <- function(active.nodes){
m.fun <- function(x) { return(mean(x, na.rm=T)) }
average.weights <- apply(active.nodes, 1, m.fun)
return(average.weights)
}
hidden.layer.similarity <- function(letter, network, comparison.letter=NA){
result <- forward.pass(letter, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
all.active.nodes <- network$input.hidden.weights[,active.nodes]
average.weights <- calculate.mean.weights(all.active.nodes)
if(!all(is.na(comparison.letter))){
similarity <- sum(abs(comparison.letter - average.weights), na.rm = T)
} else {
similarity <- sum(abs(letter - average.weights), na.rm = T)
}
return(similarity)
}
batch.hidden.layer.learning <- function(letters, network){
result <- data.frame(input=names(letters), similarity=NA)
for(i in 1:nrow(result)){
result[i,"similarity"] <- hidden.layer.similarity(letters[[names(letters)[i]]], network)
}
return(result)
}
visualize.hidden.layer.learning <- function(history){
plotting.data <- expand.grid(letter=names(letters), time=1:nrow(history$hidden.letter.similarity.tracking))
plotting.data$similarity <- mapply(function(l, t){
return(history$hidden.letter.similarity.tracking[t,which(names(letters)==l)])
}, plotting.data$letter, plotting.data$time)
summary.data <- plotting.data %>% group_by(time) %>% summarize(mean.similarity = mean(similarity))
ggplot(plotting.data, aes(x=time, y=similarity, color = letter))+ geom_line() +
geom_line(data=summary.data, aes(x=time, y=mean.similarity, color=NA), size=2)+
labs(x='time', y='difference between network representation and input letter')
}
hidden.layer.stability <- function(letter, input, network, history){
result <- forward.pass(input, network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
previous.active.nodes <- history$hidden.stability.tracking[[letter]]
change <- length(active.nodes) - sum(active.nodes %in% previous.active.nodes)
return(change)
}
batch.hidden.layer.stability <- function(letters, network, history){
result <- data.frame(input=names(letters), stability=NA)
for(i in 1:nrow(result)){
result[i,"stability"] <- hidden.layer.stability(names(letters)[i], letters[[names(letters)[i]]], network, history)
}
return(result$stability)
}
update.hidden.layer.stability <- function(letters, network){
tracker <- sapply(names(letters), function(x){
result <- forward.pass(letters[[x]], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
active.nodes <- which(result$hidden == max(result$hidden))
return(active.nodes)
}, USE.NAMES = T, simplify=FALSE)
return(tracker)
}
visualize.hidden.layer.learning(results$history)
test.word.continuity(results$network, words)
visualize.hidden.layer.learning(results$history)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.01
learning.rate.output <- 0.35
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
test.word.continuity(results$network, words)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.01
learning.rate.output <- 0.01
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
test.word.continuity(results$network, words)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.01
learning.rate.output <- 0.01
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.9
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
test.word.continuity(results$network, words)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.01
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.95
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
test.word.continuity(results$network, words)
library(ggplot2)
n.input <- 1600
n.hidden <- 100
n.output <- 30
learning.rate.hidden <- 0.005
learning.rate.output <- 0.01
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.90
hidden.bias.param.minus <- 2
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0
output.bias.param.plus <- 0
sparseness.percent <- 0.75
num.inputs.generated <- 50
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
test.word.continuity(results$network, words)
