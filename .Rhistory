percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.8 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
<<<<<<< HEAD
output.bias.param.minus <- 0.05 #0
output.bias.param.plus <- 0.00005 #0
=======
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
<<<<<<< HEAD
visualize.hidden.layer.learning(results$history)
visualize.hidden.layer.learning(results$history)
=======
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
#install.packages('ggplot2')
library(ggplot2)
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 500
n.output <- 10  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden <- 0.005
learning.rate.output <- 0.009 # 0.009
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
<<<<<<< HEAD
trace.param.output <- 0.5 #0.86
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0.005 #0
output.bias.param.plus <- 0.00005 #0
=======
trace.param.output <- 0.8 #0.8
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05
percent.act.output <- 0.1
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch(n.epochs) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.output.act.match()
plot(x=seq(from = 1, to = n.epochs/100, by = 1), y=results$history$output.act.unique.tracker, type='b', ylim=c(0,1))
test.word.continuity1(results$network, words)
<<<<<<< HEAD
log2(3/26)
log2(3/26)*(1/26)*8
(log2(3/26)*(1/26)*8)+log2(2/26)*(1/26)
log2(26/3)*(1/26)
(log2(26/3)*(1/26)*8) + log2(26/2)*(1/26)
=======
plot(x=seq(from=100, to=10000, by=100), y=results$history$mutual.info.tracker, type = 'b', ylim=c(0,3.2), xlab = "Epochs", ylab = "Mutual Information")
>>>>>>> c0ee7fc38e4e79e87821f7184147806b4c3a6fae
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 100
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
results$history$learning.curve
results$history$hidden.letter.similarity.tracking
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 500
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, s)
visualize.letter.activations(results$network, C)
visualize.letter.activations(results$network, a)
visualize.letter.activations(results$network, b)
visualize.hidden.layer.learning(results$history)
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 500
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
history = results$history
input.mat <- matrix(0, ncol=n.input, nrow=26*10)
r <- 1
for(i in 1:26){
for(j in 1:10){
input.mat[r,] <- noiseInLetter(letters[[i]], n.input, letter.noise.param)
r <- r + 1
}
}
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 500
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
visualize.hidden.layer.learning(results$history)
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 100
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
0.5 * 0.5
network
results$network
input.mat <- matrix(0, ncol=n.input, nrow=26*10)
r <- 1
for(i in 1:26){
for(j in 1:10){
input.mat[r,] <- noiseInLetter(letters[[i]], n.input, letter.noise.param)
r <- r + 1
}
}
storing.acts <- matrix(0, nrow=nrow(input.mat), ncol=n.output)
for(i in 1:nrow(input.mat)){
act.res <- forwardPass(n.output, percent.act.input,
percent.act.output, n.hidden,
input.mat[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.acts[i,] <- act.res$output
}
network = results$network
storing.acts <- matrix(0, nrow=nrow(input.mat), ncol=n.output)
for(i in 1:nrow(input.mat)){
act.res <- forwardPass(n.output, percent.act.input,
percent.act.output, n.hidden,
input.mat[i,], network$input.hidden.weights,
network$hidden.bias.weights, network$hidden.output.weights,
network$output.bias.weights)
storing.acts[i,] <- act.res$output
}
output.res <- data.frame(letter=numeric(),output=numeric())
tick <- 1
for(h in 1:nrow(storing.acts)){
output.res[h,1] <- tick
if(h %% 10 == 0){
tick = tick + 1
}
}
for(k in 1:nrow(storing.acts)){
output.res[k,2] <- which.max(storing.acts[k,])
}
probs.letter <- numeric(26)
probs.act <- numeric(30)
probs.joint <- numeric(26*30)
mutual.info <- numeric(26*30)
for(w in 1:length(probs.letter)){
probs.letter[w] <- sum(output.res[,1] == w) / length(output.res[,1])
}
for(x in 1:length(probs.act)){
probs.act[x] <- sum(output.res[,2] == x) / length(output.res[,2])
}
count3 = 0
for(k in 1:length(probs.letter)){
for(h in 1:length(probs.act)){
count3 <- count3 + 1
probs.joint[count3] <- sum((output.res[,1] == k) & (output.res[,2] == h)) / length(output.res[,1])
}
}
count4 = 0
for(v in 1:length(probs.letter)){
for(t in 1:length(probs.act)){
count4 <- count4 + 1
mutual.info[count4] = (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.letter[v]))))
}
}
mutual.info <- sum(mutual.info, na.rm = T)
probs.letter <- numeric(26)
probs.act <- numeric(30)
probs.joint <- numeric(26*30)
mutual.info <- numeric(26*30)
for(w in 1:length(probs.letter)){
probs.letter[w] <- sum(output.res[,1] == w) / length(output.res[,1])
}
for(x in 1:length(probs.act)){
probs.act[x] <- sum(output.res[,2] == x) / length(output.res[,2])
}
count3 = 0
for(k in 1:length(probs.letter)){
for(h in 1:length(probs.act)){
count3 <- count3 + 1
probs.joint[count3] <- sum((output.res[,1] == k) & (output.res[,2] == h)) / length(output.res[,1])
}
}
count4 = 0
for(v in 1:length(probs.letter)){
for(t in 1:length(probs.act)){
count4 <- count4 + 1
mutual.info[count4] = (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.letter[v]))))
}
}
mutual.info
probs.letter <- numeric(26)
probs.act <- numeric(30)
probs.joint <- numeric(26*30)
mutual.info <- numeric(26*30)
for(w in 1:length(probs.letter)){
probs.letter[w] <- sum(output.res[,1] == w) / length(output.res[,1])
}
for(x in 1:length(probs.act)){
probs.act[x] <- sum(output.res[,2] == x) / length(output.res[,2])
}
count3 = 0
for(k in 1:length(probs.letter)){
for(h in 1:length(probs.act)){
count3 <- count3 + 1
probs.joint[count3] <- sum((output.res[,1] == k) & (output.res[,2] == h)) / length(output.res[,1])
}
}
count4 = 0
for(v in 1:length(probs.letter)){
for(t in 1:length(probs.act)){
count4 <- count4 + 1
mutual.info[count4] = (probs.joint[count4] * (log2((probs.joint[count4])/(probs.act[t]*probs.letter[v]))))
if(mutual.info[count4] < 0){
mutual.info[count4] = 0
}
}
}
mutual.info
0.25 * log2(0.25 / (0.25^2))
0.25 * log2(0.25 / (0.5 * 0.5))
0.25 * log2(0.25 / (0.5 * 0.5))
0.25 * log2(0.25 / (0.25 * 0.5))
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
Rcpp::sourceCpp('forwardPassCpp.cpp')
n.input <- 1600
n.hidden <- 500
n.output <- 30  #Must be multiple of 10 due to activation percentage calculation
learning.rate.hidden.max = 0.5
learning.rate.hidden.min = 0.0001
learning.rate.hidden <- 0.00001
learning.rate.output <- 0.0001
learning.rate.output.max <- 0.5 # 0.009
learning.rate.output.min <- 0.00001
restarts <- 5 # 5
n.epochs <- 1000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 1
hidden.bias.param.minus <- 0 # 0.05
hidden.bias.param.plus <- 0 # 0.0005
output.bias.param.minus <- 0 #0
output.bias.param.plus <- 0 #0
sparseness.percent <- 0.75  # sparseness.percent is % nodes inactive #0.75
num.inputs.generated <- n.input/2 # half of total inputs
integration.parameter <- 1 #0 is totally segregated, 1 is totally integrated
percent.act.input <- 0.05 # 0.05
percent.act.output <- 0.03333 # .10
n.words <- length(words)
letter.noise.param <- 0.1
input.gen.parameter <- 0 # if 1: temporal pattern of input for one system, random pattern for other system. (one system predicts next input)
# if 0: Next inputs are predicted by combination of both systems' previous inputs - one system alone cannot predict next inputs
# if 0.5: inputs for each system consistently co-occur
## RUN ##
results <- batch_split(n.epochs, network=NA) #run training batches
#install.packages('ggplot2')
#install.packages('ggplot')
#install.packages('png')
#install.packages('abind')
#install.packages('dplyr')
library('ggplot2')
#load('trained_hidden_weights_20000_CosAnneal') # loading weights as variable 'network'
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network-split.R')
source('Load Letters.R')
source('Visualize Output.R')
