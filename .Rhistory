}
if(output[j] == 0){
output.bias.weights[j,1] <- output.bias.weights[j,1] + output.bias.param.plus
}
if(output.bias.weights[j,1] < 0){
output.bias.weights[j,1] <- 0
}
}
for(b in 1:n.output){
trace.output[b] <- (1 - trace.param.output) * trace.output[b] + trace.param.output * output[b]
hidden.output.weights[,b] <- hidden.output.weights[,b] + learning.rate * trace.output[b] * (hidden - hidden.output.weights[,b])
}
return(list(
trace.hidden=trace.hidden,
hidden = hidden,
input.hidden.weights=input.hidden.weights,
hidden.bias.weights=hidden.bias.weights,
trace.output=trace.output,
output=output,
hidden.output.weights=hidden.output.weights,
output.bias.weights=output.bias.weights))
}
learning.measure <- function(input.hidden.weights){
all.letters.compared <- numeric(26)
best.fit <- numeric(n.hidden)
for(i in 1:n.hidden){
for(h in 1:26){
all.letters.compared[h] <- sum(abs(input.hidden.weights[,i] - alphabet[[h]]))
}
best.fit[i] <- min(all.letters.compared)
}
return(best.fit)
}
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
plot(results$learning.curve[,i], main=paste("Node",i))
plot(results$bias.tracker[,i])
image(matrix(results$input.hidden.weights[,i], nrow = 40))
}
}
display.output.bias.tracker <- function(results){
for(i in 1:n.output){
plot(results$output.bias.tracker[,i], main=paste('Node', i))
}
}
batch <- function(n.epochs){
# network properties #
input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden) #initialiize weights at random values between 0 and 0.05
hidden.bias.weights <- matrix(0, nrow=n.hidden, ncol=1)
# tracking learning #
learning.curve <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix
bias.tracker <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix
hidden.win.tracker <- matrix(0, nrow=n.epochs, ncol= n.hidden)
pb <- txtProgressBar(min=1, max=n.epochs,style=3)
for(i in 1:n.epochs){
letter <- alphabet[[sample(1:26,1, replace = T)]]
results <- trace.update(letter, input.hidden.weights, trace.hidden, hidden.bias.weights)
input.hidden.weights <- results$input.hidden.weights
trace.hidden <- results$trace.hidden
hidden.bias.weights <- results$hidden.bias.weights
hidden.win.tracker[i,] <- results$hidden
if(i %% 100 == 0){
learning.curve[i / 100,] <- learning.measure(input.hidden.weights)
bias.tracker[i / 100,] <- as.vector(hidden.bias.weights)
}
setTxtProgressBar(pb, i)
}
return(list(
input.hidden.weights=input.hidden.weights,
learning.curve=learning.curve,
bias.tracker=bias.tracker,
hidden.bias.weights=hidden.bias.weights,
hidden.win.tracker = hidden.win.tracker
))
}
batch.2 <- function(n.epochs){
# network properties #
input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden) #initialiize weights at random values between 0 and 0.05
hidden.bias.weights <- matrix(0, nrow=n.hidden, ncol=1)
hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=0.05), nrow=n.hidden, ncol=n.output)
output.bias.weights <- matrix(0, nrow=n.output, ncol=1)
trace.hidden <- rep(0, times = n.hidden)
trace.output <- rep(0, times = n.output)
# tracking learning #
learning.curve <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix
bias.tracker <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix
output.bias.tracker <- matrix(0, nrow = n.epochs/100, ncol= n.output)
hidden.win.tracker <- matrix(0, nrow=n.epochs, ncol= n.hidden)
pb <- txtProgressBar(min=1, max=n.epochs,style=3)
for(i in 1:n.epochs){
word <- words[[sample(1:9,1, replace = T)]]
for(b in 1:(length(word)/n.input)){
letter <- word[,b]
letter <- noise.in.letter(letter)
results <- trace.update.2(letter, input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights)
input.hidden.weights <- results$input.hidden.weights
hidden.output.weights <- results$hidden.output.weights
trace.hidden <- results$trace.hidden
hidden.bias.weights <- results$hidden.bias.weights
hidden.output.weights <- results$hidden.output.weights
trace.output <- results$trace.output
output.bias.weights <- results$output.bias.weights
hidden.bias.weights <- results$hidden.bias.weights
hidden.win.tracker[i,] <- results$hidden
if(i %% 100 == 0){
learning.curve[i / 100,] <- learning.measure(input.hidden.weights)
bias.tracker[i / 100,] <- as.vector(hidden.bias.weights)
output.bias.tracker[i / 100,] <- as.vector(output.bias.weights)
}
setTxtProgressBar(pb, i)
}
}
temp.layer.activations(input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights)
return(list(
input.hidden.weights=input.hidden.weights,
hidden.output.weights=hidden.output.weights,
learning.curve=learning.curve,
bias.tracker=bias.tracker,
output.bias.tracker=output.bias.tracker,
hidden.bias.weights=hidden.bias.weights,
hidden.win.tracker = hidden.win.tracker
))
}
## weight/activation image generation and noise func. ##
weight.images <- function(){
return(
for(i in 1:26){
image(matrix(input.hidden.weights[,i], nrow = 40))
})
}
temp.layer.activations <- function(input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights){
storing.activations <- matrix(0, nrow=n.hidden, ncol=n.output)
for(i in 1:length(words)){
word <- words[[i]]
for(j in 1:(length(word)/n.input)){
letter <- word[,j]
act.results <- trace.update.2(letter, input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights)
storing.activations[((i-1)*3)+j,] <- act.results$output
}
}
image(storing.activations)
print(storing.activations)
}
noise.in.letter <- function(letter){
for(i in 1:(0.1*n.input)){
letter[(sample(1:1600,1,replace=T))] <- 1
}
return(letter)
}
results <- batch.2(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
results
results$output.bias.tracker
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.5
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
results$output.bias.tracker[1,]
results$output.bias.tracker[,1]
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
results$bias.tracker
results$output.bias.tracker
history <- list(
learning.curve <- matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
bias.tracker <- matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
output.bias.tracker <- matrix(0, nrow = n.epochs/100, ncol= n.output),
hidden.win.tracker <- matrix(0, nrow=n.epochs, ncol= n.hidden)
)
history <- list(
learning.curve = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
bias.tracker = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
output.bias.tracker = matrix(0, nrow = n.epochs/100, ncol= n.output),
hidden.win.tracker = matrix(0, nrow=n.epochs, ncol= n.hidden)
)
words
words[[1]]
words[[1]][,1]
image(words[[1]][,1])
length(words)
ncol(words[[1]])
ncol(words[[9]])
input.list <- list()
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.list <- c(input.list, words[[i]][,j])
}
}
input.list
input.list[1]
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
results
test.word.continuity(results$network, words)
test.word.continuity <- function(network, words){
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
temp.layer.activations(network, input.matrix)
}
test.word.continuity(results$network, words)
temp.layer.activations <- function(network, input.matrix){
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:length(input.list)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
image(storing.activations)
print(storing.activations)
}
test.word.continuity(results$network, words)
temp.layer.activations <- function(network, input.matrix){
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
image(storing.activations)
print(storing.activations)
}
test.word.continuity(results$network, words)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
display.learning.curves <- function(results){
for(i in 1:n.hidden){
layout(matrix(1:4, nrow=2))
plot(results$history$learning.curve[,i], main=paste("Node",i))
plot(results$history$bias.tracker[,i])
image(matrix(results$network$input.hidden.weights[,i], nrow = 40))
}
}
display.output.bias.tracker <- function(results){
for(i in 1:n.output){
plot(results$history$output.bias.tracker[,i], main=paste('Node', i))
}
}
test.word.continuity <- function(network, words){
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
temp.layer.activations(network, input.matrix)
}
temp.layer.activations <- function(network, input.matrix){
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
image(storing.activations)
print(storing.activations)
}
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
storing.activations <- matrix(0, nrow=nrow(input.matrix), ncol=n.output)
for(i in 1:nrow(input.matrix)){
act.results <- forward.pass(input.matrix[i,], network$input.hidden.weights, network$hidden.bias.weights, network$hidden.output.weights, network$output.bias.weights)
storing.activations[i,] <- act.results$output
}
image(storing.activations)
print(storing.activations)
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
temp.layer.activations(network, input.matrix)
network <- list(
input.hidden.weights = matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden), #initialiize weights at random values between 0 and 0.05
hidden.bias.weights = matrix(0, nrow=n.hidden, ncol=1),
hidden.output.weights = matrix(runif(n.hidden*n.output, min=0, max=0.05), nrow=n.hidden, ncol=n.output),
output.bias.weights = matrix(0, nrow=n.output, ncol=1),
trace.hidden = rep(0, times = n.hidden),
trace.output = rep(0, times = n.output)
)
history <- list(
learning.curve = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
bias.tracker = matrix(0, nrow = n.epochs/100, ncol = n.hidden), #initializes learning data matrix
output.bias.tracker = matrix(0, nrow = n.epochs/100, ncol= n.output),
hidden.win.tracker = matrix(0, nrow=n.epochs, ncol= n.hidden)
)
n.letters <- 0
for(i in 1:length(words)){
n.letters <- n.letters + ncol(words[[i]])
}
input.matrix <- matrix(0, ncol=n.input, nrow=n.letters)
r <- 1
for(i in 1:length(words)){
for(j in 1:ncol(words[[i]])){
input.matrix[r,] <- words[[i]][,j]
r <- r + 1
}
}
temp.layer.activations(network, input.matrix)
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
n.input <- 1600
n.hidden <- 26
n.output <- 20
learning.rate <- 0.05
n.epochs <- 10000
trace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account
trace.param.output <- 0.2
hidden.bias.param.minus <- 1
hidden.bias.param.plus <- 0.05
output.bias.param.minus <- 1
output.bias.param.plus <- 0.05
source('Load Letters.R')
source('Visualize Output.R')
source('multi-layer-network.R')
## RUN ##
results <- batch(n.epochs) #run training batches
display.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs
display.output.bias.tracker(results)
