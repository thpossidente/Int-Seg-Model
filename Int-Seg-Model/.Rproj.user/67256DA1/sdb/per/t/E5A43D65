{
    "collab_server" : "",
    "contents" : "n.input <- 1600\nn.hidden <- 26\nn.output <- 50\nlearning.rate <- 0.05\nn.epochs <- 5000\nn.test <- 26\ntrace.param.hidden <- 1 # value of 1 indicates pure hebbian learning. Closer to zero, more of 'history' of node activation is taken into account\ntrace.param.output <- 0.2\nhidden.bias.param.minus <- 1\nhidden.bias.param.plus <- 0.05\noutput.bias.param.minus <- 1\noutput.bias.param.plus <- 0.05\n\n#install.packages('png')\nlibrary('png')\n#install.packages('abind')\nlibrary('abind')\n\nalphabet <- list(\n  a <- as.vector(t(1-adrop(readPNG('AlphabetPNG/A.png')[,,1,drop=F], drop=3))),\n  b <- as.vector(t(1-adrop(readPNG('AlphabetPNG/B.png')[,,1,drop=F], drop=3))),\n  c <- as.vector(t(1-adrop(readPNG('AlphabetPNG/C.png')[,,1,drop=F], drop=3))),\n  d <- as.vector(t(1-adrop(readPNG('AlphabetPNG/D.png')[,,1,drop=F], drop=3))),\n  e <- as.vector(t(1-adrop(readPNG('AlphabetPNG/E.png')[,,1,drop=F], drop=3))),\n  f <- as.vector(t(1-adrop(readPNG('AlphabetPNG/F.png')[,,1,drop=F], drop=3))),\n  g <- as.vector(t(1-adrop(readPNG('AlphabetPNG/G.png')[,,1,drop=F], drop=3))),\n  h <- as.vector(t(1-adrop(readPNG('AlphabetPNG/H.png')[,,1,drop=F], drop=3))),\n  i <- as.vector(t(1-adrop(readPNG('AlphabetPNG/I.png')[,,1,drop=F], drop=3))),\n  j <- as.vector(t(1-adrop(readPNG('AlphabetPNG/J.png')[,,1,drop=F], drop=3))),\n  k <- as.vector(t(1-adrop(readPNG('AlphabetPNG/K.png')[,,1,drop=F], drop=3))),\n  l <- as.vector(t(1-adrop(readPNG('AlphabetPNG/L.png')[,,1,drop=F], drop=3))),\n  m <- as.vector(t(1-adrop(readPNG('AlphabetPNG/M.png')[,,1,drop=F], drop=3))),\n  n <- as.vector(t(1-adrop(readPNG('AlphabetPNG/N.png')[,,1,drop=F], drop=3))),\n  o <- as.vector(t(1-adrop(readPNG('AlphabetPNG/O.png')[,,1,drop=F], drop=3))),\n  p <- as.vector(t(1-adrop(readPNG('AlphabetPNG/P.png')[,,1,drop=F], drop=3))),\n  q <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Q.png')[,,1,drop=F], drop=3))),\n  r <- as.vector(t(1-adrop(readPNG('AlphabetPNG/R.png')[,,1,drop=F], drop=3))),\n  s <- as.vector(t(1-adrop(readPNG('AlphabetPNG/S.png')[,,1,drop=F], drop=3))),\n  t <- as.vector(t(1-adrop(readPNG('AlphabetPNG/T.png')[,,1,drop=F], drop=3))),\n  u <- as.vector(t(1-adrop(readPNG('AlphabetPNG/U.png')[,,1,drop=F], drop=3))),\n  v <- as.vector(t(1-adrop(readPNG('AlphabetPNG/V.png')[,,1,drop=F], drop=3))),\n  w <- as.vector(t(1-adrop(readPNG('AlphabetPNG/W.png')[,,1,drop=F], drop=3))),\n  x <- as.vector(t(1-adrop(readPNG('AlphabetPNG/X.png')[,,1,drop=F], drop=3))),\n  y <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Y.png')[,,1,drop=F], drop=3))),\n  z <- as.vector(t(1-adrop(readPNG('AlphabetPNG/Z.png')[,,1,drop=F], drop=3)))\n)\n\nwords <- list(\n  cat <- cbind(c,a,t), bow <- cbind(b,o,w),\n  sip <- cbind(s,i,p), rad <- cbind(r,a,d),\n  zen <- cbind(z,e,n), two <- cbind(t,w,o),\n  rub <- cbind(r,u,b), vex <- cbind(v,e,x),\n  fox <- cbind(f,o,x), wry <- cbind(w,r,y),\n  vow <- cbind(v,o,w), zag <- cbind(z,a,g),\n  quo <- cbind(q,u,o), fry <- cbind(f,r,y),\n  the <- cbind(t,h,e), pew <- cbind(p,e,w),\n  dug <- cbind(d,u,g), keg <- cbind(k,e,w),\n  yak <- cbind(y,a,k), tax <- cbind(t,a,x),\n  jaw <- cbind(j,a,w), who <- cbind(w,h,o),\n  lax <- cbind(l,a,x), til <- cbind(t,i,l),\n  sin <- cbind(s,i,n), mud <- cbind(m,u,d),\n  yap <- cbind(y,a,p), orb <- cbind(o,r,b),\n  ply <- cbind(p,l,y), cry <- cbind(c,r,y),\n  tom <- cbind(t,o,m), coy <- cbind(c,o,y),\n  any <- cbind(a,n,y), jot <- cbind(j,o,t),\n  she <- cbind(s,h,e), gig <- cbind(g,i,g),\n  axe <- cbind(a,x,e), icy <- cbind(i,c,y),\n  elm <- cbind(e,l,m), owl <- cbind(o,w,l),\n  gag <- cbind(g,a,g), nun <- cbind(n,u,n),\n  jay <- cbind(j,a,y), rye <- cbind(r,y,e),\n  apt <- cbind(a,p,t), sty <- cbind(s,t,y),\n  lit <- cbind(l,i,t), why <- cbind(w,h,y),\n  hue <- cbind(h,u,e), use <- cbind(u,s,e)\n)\n\nsigmoid.activation <- function(x){\n  #return(1 / (1+exp(-x)))\n  return(x)\n}\n\n\nforward.pass <- function(input, input.hidden.weights, hidden.bias.weights){ #calculate output activations with \"winner-takes-all\" method\n  \n  hidden <- numeric(n.hidden)\n  for(i in 1:n.hidden){\n    hidden[i] <- sigmoid.activation(sum(input * input.hidden.weights[,i]) + hidden.bias.weights[i,1])\n  }\n  hidden[hidden != max(hidden)] <- 0\n  hidden[which.max(hidden)] <- 1\n  return(hidden)\n}\n\nforward.pass.2 <- function(input, input.hidden.weights, hidden.bias.weights, hidden.output.weights, output.bias.weights){ #calculate output activations with \"winner-takes-all\" method\n  \n  hidden <- numeric(n.hidden)\n  for(i in 1:n.hidden){\n    hidden[i] <- sigmoid.activation(sum(input * input.hidden.weights[,i]) + hidden.bias.weights[i,1])\n  }\n  hidden[hidden != max(hidden)] <- 0\n  hidden[which.max(hidden)] <- 1\n  \n  output <- numeric(n.output)\n  for(b in 1:n.output){\n    output[b] <- sigmoid.activation(sum(hidden * hidden.output.weights[,b] +  output.bias.weights[b,1]))\n  }\n  output[output != max(output)] <- 0\n  output[which.max(output)] <- 1\n  return(list(hidden=hidden, output=output))\n}\n\n\ntrace.update <- function(input, input.hidden.weights, trace.hidden, hidden.bias.weights){\n  \n  hidden <- forward.pass(input, input.hidden.weights, hidden.bias.weights)\n  \n  for(h in 1:n.hidden){\n    if(hidden[h] == 1){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] - hidden.bias.param.minus\n    }\n    if(hidden[h] == 0){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] + hidden.bias.param.plus\n    }\n    if(hidden.bias.weights[h,1] < 0){\n      hidden.bias.weights[h,1] <- 0\n    }\n  }\n  \n  for(i in 1:n.hidden){\n    trace.hidden[i] <- (1 - trace.param.hidden) * trace.hidden[i] + trace.param.hidden * hidden[i] \n    input.hidden.weights[,i] <- input.hidden.weights[,i] + learning.rate * trace.hidden[i] * (input - input.hidden.weights[,i])\n  }\n  \n  return(list(\n    trace.hidden = trace.hidden,\n    hidden = hidden,\n    input.hidden.weights = input.hidden.weights, \n    hidden.bias.weights = hidden.bias.weights\n  ))\n}\n\nlearning.measure <- function(input.hidden.weights){\n  all.letters.compared <- numeric(26)\n  best.fit <- numeric(n.hidden)\n  for(i in 1:n.hidden){\n    for(h in 1:26){\n      all.letters.compared[h] <- sum(abs(input.hidden.weights[,i] - alphabet[[h]]))\n    }\n    best.fit[i] <- min(all.letters.compared)\n  }\n  return(best.fit)\n}\n\ndisplay.learning.curves <- function(results){\n  for(i in 1:n.hidden){\n    layout(matrix(1:4, nrow=2))\n    plot(results$learning.curve[,i], main=paste(\"Node\",i))\n    plot(results$bias.tracker[,i])\n    image(matrix(results$input.hidden.weights[,i], nrow = 40))\n  }\n}\n\ntrace.update.2 <- function(input, input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights){\n  \n  forward.pass.results <- forward.pass.2(input, input.hidden.weights, hidden.bias.weights, hidden.output.weights, output.bias.weights)\n  hidden <- forward.pass.results$hidden\n  output <- forward.pass.results$output\n  \n  for(h in 1:n.hidden){\n    if(hidden[h] == 1){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] - hidden.bias.param.minus\n    }\n    if(hidden[h] == 0){\n      hidden.bias.weights[h,1] <- hidden.bias.weights[h,1] + hidden.bias.param.plus\n    }\n    if(hidden.bias.weights[h,1] < 0){\n      hidden.bias.weights[h,1] <- 0\n    }\n  }\n  \n  for(i in 1:n.hidden){\n    trace.hidden[i] <- (1 - trace.param.hidden) * trace.hidden[i] + trace.param.hidden * hidden[i] \n    input.hidden.weights[,i] <- input.hidden.weights[,i] + learning.rate * trace.hidden[i] * (input - input.hidden.weights[,i])\n  }\n  \n  for(j in 1:n.output){\n    if(output[j] == 1){\n      output.bias.weights[j,1] <- output.bias.weights[j,1] - output.bias.param.minus\n    }\n    if(output[j] == 0){\n      output.bias.weights[j,1] <- output.bias.weights[j,1] + output.bias.param.plus\n    }\n    if(output.bias.weights[j,1] < 0){\n      output.bias.weights[j,1] <- 0\n    }\n  }\n  \n  for(b in 1:n.output){\n    trace.output[b] <- (1 - trace.param.output) * trace.output[b] + trace.param.output * output[b]\n    hidden.output.weights[,b] <- hidden.output.weights[,b] + learning.rate * trace.output[b] * (hidden - hidden.output.weights[,b])\n  }\n  \n  return(list(\n    trace.hidden=trace.hidden, \n    hidden = hidden,\n    input.hidden.weights=input.hidden.weights, \n    hidden.bias.weights=hidden.bias.weights,\n    trace.output=trace.output,\n    output=output,\n    hidden.output.weights=hidden.output.weights,\n    output.bias.weights=output.bias.weights))\n}\n\n\nlearning.measure <- function(input.hidden.weights){\n  all.letters.compared <- numeric(26)\n  best.fit <- numeric(n.hidden)\n  for(i in 1:n.hidden){\n    for(h in 1:26){\n      all.letters.compared[h] <- sum(abs(input.hidden.weights[,i] - alphabet[[h]]))\n    }\n    best.fit[i] <- min(all.letters.compared)\n  }\n  return(best.fit)\n}\n\n\ndisplay.learning.curves <- function(results){\n  for(i in 1:n.hidden){\n    layout(matrix(1:4, nrow=2))\n    plot(results$learning.curve[,i], main=paste(\"Node\",i))\n    plot(results$bias.tracker[,i])\n    image(matrix(results$input.hidden.weights[,i], nrow = 40))\n  }\n}\n\n\nbatch <- function(n.epochs){\n  \n  # network properties #\n  input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden) #initialiize weights at random values between 0 and 0.05\n  hidden.bias.weights <- matrix(0, nrow=n.hidden, ncol=1)\n  \n  # tracking learning #\n  learning.curve <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix\n  bias.tracker <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix\n  hidden.win.tracker <- matrix(0, nrow=n.epochs, ncol= n.hidden)\n  \n  pb <- txtProgressBar(min=1, max=n.epochs,style=3)\n  for(i in 1:n.epochs){\n    letter <- alphabet[[sample(1:26,1, replace = T)]]\n    results <- trace.update(letter, input.hidden.weights, trace.hidden, hidden.bias.weights)\n    input.hidden.weights <- results$input.hidden.weights\n    trace.hidden <- results$trace.hidden\n    hidden.bias.weights <- results$hidden.bias.weights\n    hidden.win.tracker[i,] <- results$hidden\n    if(i %% 100 == 0){\n      learning.curve[i / 100,] <- learning.measure(input.hidden.weights)\n      bias.tracker[i / 100,] <- as.vector(hidden.bias.weights)\n    }\n    setTxtProgressBar(pb, i)\n  }\n  return(list(\n    input.hidden.weights=input.hidden.weights, \n    learning.curve=learning.curve, \n    bias.tracker=bias.tracker,\n    hidden.bias.weights=hidden.bias.weights,\n    hidden.win.tracker = hidden.win.tracker\n  ))\n}\n\n\nbatch.2 <- function(n.epochs){ \n  # network properties #\n  input.hidden.weights <- matrix(runif(n.input*n.hidden, min=0, max=0.05), nrow=n.input, ncol=n.hidden) #initialiize weights at random values between 0 and 0.05\n  hidden.bias.weights <- matrix(0, nrow=n.hidden, ncol=1)\n  hidden.output.weights <- matrix(runif(n.hidden*n.output, min=0, max=0.05), nrow=n.hidden, ncol=n.output)\n  output.bias.weights <- matrix(0, nrow=n.output, ncol=1)\n  trace.hidden <- rep(0, times = n.hidden)\n  trace.output <- rep(0, times = n.output)\n  \n  # tracking learning #\n  learning.curve <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix\n  bias.tracker <- matrix(0, nrow = n.epochs/100, ncol = n.hidden) #initializes learning data matrix\n  hidden.win.tracker <- matrix(0, nrow=n.epochs, ncol= n.hidden)\n  \n  pb <- txtProgressBar(min=1, max=n.epochs,style=3)\n  for(i in 1:n.epochs){\n    word <- words[[sample(1:50,1, replace = T)]]\n    for(b in 1:(length(word)/n.input)){\n      letter <- word[,b]\n      results <- trace.update.2(letter, input.hidden.weights, trace.hidden, hidden.bias.weights, hidden.output.weights, trace.output, output.bias.weights)\n      input.hidden.weights <- results$input.hidden.weights\n      hidden.output.weights <- results$hidden.output.weights\n      trace.hidden <- results$trace.hidden\n      hidden.bias.weights <- results$hidden.bias.weights\n      hidden.output.weights <- results$hidden.output.weights\n      trace.output <- results$trace.output\n      output.bias.weights <- results$output.bias.weights\n      setTxtProgressBar(pb, i)\n    }\n  }\n  return(list(\n    input.hidden.weights=input.hidden.weights,\n    hidden.output.weights=hidden.output.weights\n  ))\n}\n\n\n\n\nresults <- batch.2(n.epochs) #run training batches\ndisplay.learning.curves(results) #visualize learning by plotting weight similarity to alphabet input every 100 epochs\n\n\n\n\n## output storage func. and weight image generation ##\n\noutput.storage <- function(){ #stores outputs \n  hidden.outputs <- matrix(0, nrow = n.test, ncol = n.hidden)\n  for(i in 1:26){\n    one.hidden <- forward.pass(alphabet[[i]])\n    hidden.outputs[i,] <- one.hidden\n  }\n  return(hidden.outputs)\n}\n\nweight.images <- function(){\n  return(\n    for(i in 1:26){\n      image(matrix(input.hidden.weights[,i], nrow = 40))\n    })\n}\n\nimage(results$hidden.win.tracker)",
    "created" : 1490473606062.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1358165303",
    "id" : "E5A43D65",
    "lastKnownWriteTime" : 1490642012,
    "last_content_update" : 1490642012132,
    "path" : "~/GitHub/Int-Seg-Model/Spatial Pooling.R",
    "project_path" : "Spatial Pooling.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}