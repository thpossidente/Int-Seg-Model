
n.inputs <- 25
n.outputs <- 10
learning.rate <- 0.2
training.data.input <- 1000
test.trials <- 50
trace.param <- 0.2
trace <- rep(0, times = n.outputs)
n.epochs <- 2000



trainingData <- matrix(0, nrow = training.data.input, ncol = n.inputs) #blank trainingData template to run before trainingDataGenerator

trainingDataGenerator <- function(){ #generates 1000 inputs of 25 length each. inputs are zeroes with about 5 ones randomly placed
  for(b in 1:training.data.input){
    trainingData[b,sample(n.inputs, n.inputs*0.2, replace = F)] <<- 1
  }
  return(trainingData)
}


data <- trainingDataGenerator()  #fills trainingData 

groups <- function(){  #function for grouping inputs into groups of 100 inputs each

  groups <- list(data[1:100,],
                 data[101:200,],
                 data[201:300,],
                 data[301:400,],
                 data[401:500,],
                 data[501:600,],
                 data[601:700,],
                 data[701:800,],
                 data[801:900,],
                 data[901:1000,])
  return(groups)
}


select.input <- function(){ #randomly selects one group from 10 groups of 100 inputs each
  groups <- groups()
  input <- sample(groups,1, replace=T)
  input <- matrix(unlist(input), ncol=n.inputs, byrow = T)
  return(input)
}



weights <- matrix(runif(n.inputs*n.outputs, min=0, max=1), nrow=n.inputs, ncol=n.outputs) #initialiize weights at random values between 1 and 0


sigmoid.activation <- function(x){
  return(1 / (1+exp(-x)))
}


forward.pass = function(input){ #calculate output activations with "winner-takes-all" method
  outputs <- numeric(n.outputs)
  for(i in 1:n.outputs){
    outputs[i] <- sigmoid.activation(sum(input * weights[,i]))
  }
  outputs[which.max(outputs)] <- 1
  outputs[outputs != max(outputs)] <- 0
  return(outputs)
}


trace.update <- function(input){ #changes weights based on temporal learning algorithm
  output <- forward.pass(input)
  
  for(i in 1:n.outputs){
    trace[i] <<- (1 - trace.param) * trace[i] + trace.param * output[i]
    weights[,i] <<- weights[,i] + learning.rate * trace[i] * (input - weights[,i])
    
  }
}



batch <- function(n.epochs){ #runs many groups(n.epochs) through trace update to train network
  pb <- txtProgressBar(min=0, max=n.epochs, style=3)
  for(i in 1:n.epochs){
    g <- select.input()
    if(i %% 100 == 0){
      print(entropy.measure())
    }
    for(o in 1:nrow(g)){
      trace.update(g[o,])
    }
    setTxtProgressBar(pb, i)
  }
}

batch(n.epochs)  #run training batches and calc. entropy every 100 epochs



## entropy testing functions ##

output.storage <- function(){ #stores ouputs from running every group of inputs once
  outputs <- matrix(0, nrow = training.data.input, ncol = 10)
  groups <- groups()
  for(i in 1:10){
    testinput <- groups[[i]]
    for(b in 1:100){
      one.output <- forward.pass(testinput[b,])
      outputs[(100 * (i-1) + b),] <- one.output
    }
  }
  return(outputs)
}


entropy.calc <- function(v){ #function to pass in matrix and get entropy
  v <- v / sum(v)
  e.sum <- 0
  for(i in 1:length(v)){
    if(v[i] != 0){
      e.sum <- e.sum + -v[i] * log2(v[i])
    }
  }
  return(e.sum)
}



entropy.measure <- function(){ #calculate average entropy of output activations for each group
  output.storage <- output.storage()
  entropy <- numeric(10)
  for(i in 1:10){
    stability.one <- colSums(output.storage[((i-1) * 100 + 1):(i * 100),])
    entropy[i] <- entropy.calc(stability.one)
  }
  return(mean(entropy))
}